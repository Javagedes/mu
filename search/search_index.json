{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Project Mu \u00b6 Project Mu is a modular adaptation of TianoCore's edk2 tuned for building modern devices using a scalable, maintainable, and reusable pattern. Mu is built around the idea that shipping and maintaining a UEFI product is an ongoing collaboration between numerous partners. For too long, the industry has built products using a \"forking\" model combined with copy/paste/rename and with each new product, the maintenance burden grows to such a level that updates are near impossible due to cost and risk. Project Mu also tries to address the complex business relationships and legal challenges facing partners today. To build most products, it often requires both closed-source, proprietary assets as well as open-source and industry-standard code. The distributed build system and multi-repository design allow product teams to keep code separate and connected to their original source while respecting legal and business boundaries. Project Mu originated from building modern Windows PCs but its patterns and design allow it to be scaled down or up for whatever the final product's intent. IoT, Server, PC, or any other form factor should be able to leverage the content. Primary Goals \u00b6 Initially, this project will focus on two central goals. Share our active code tree to both solicit feedback and entice partners to collaborate \u00b6 Project Mu is an active project. This is not a side project, mirror, clone, or example. This is the same code used today on many of Microsoft's 1 st party devices and it will be kept current because it must be to continue to enable shipping products. Promote, evangelize, and support an industry shift to a more collaborative environment so we all can build and maintain products with lower costs and higher quality \u00b6 Today's open source projects, although extremely valuable, are very resource-intensive to interact with. This friction leads to major industry players avoiding public interaction, thus diminishing the overall community\u2019s value. The modern era of open source projects has incorporated new tools and procedures to lower this friction and it is our goal to leverage those tools. GitHub provides issue tracking, Pull Requests, Gated builds, tracked/required web-based code reviews, and CI/CD (Continuous integration and delivery). It is our belief that by leveraging and extending this automation and workflow, we can lower the friction and foster a safe place for all contributors to work. Guiding Principles \u00b6 Less is More * Be open to change / flexible - Keep learning. If it was easy this would have been solved before Design for code reuse Leverage tools / invest in automation Navigation \u00b6 Have a look around this site to see what is Project Mu. Start by reviewing the details of the community and our process. See how to interact and get involved, why it's different, how to work within or extend it, as well as where everything is located. Finally, explore the Developer Docs if you want to review more in-depth details. Having trouble? \u00b6 Skim the FAQ Roadmap \u00b6 After the first few months of Mu, our initial roadmap is largely complete. Any remaining items have been moved to the GitHub Issues and will continue to be tracked there. We hope to use GitHub Issues to track new roadmap items going forwards. Project Mu GitHub Issues Join Us \u00b6 Contact info and additional methods to collaborate coming soon. Code of conduct \u00b6 This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Reporting Issues \u00b6 Short answer: Open a github issue. More details: Contributing Contributing \u00b6 Short answer: Open a pull request. More details: Contributing License \u00b6 Refer to License Documentation Build Information Version: 0.9.1 Build Time: 2020-08-13 22:52","title":"Home"},{"location":"#welcome-to-project-mu","text":"Project Mu is a modular adaptation of TianoCore's edk2 tuned for building modern devices using a scalable, maintainable, and reusable pattern. Mu is built around the idea that shipping and maintaining a UEFI product is an ongoing collaboration between numerous partners. For too long, the industry has built products using a \"forking\" model combined with copy/paste/rename and with each new product, the maintenance burden grows to such a level that updates are near impossible due to cost and risk. Project Mu also tries to address the complex business relationships and legal challenges facing partners today. To build most products, it often requires both closed-source, proprietary assets as well as open-source and industry-standard code. The distributed build system and multi-repository design allow product teams to keep code separate and connected to their original source while respecting legal and business boundaries. Project Mu originated from building modern Windows PCs but its patterns and design allow it to be scaled down or up for whatever the final product's intent. IoT, Server, PC, or any other form factor should be able to leverage the content.","title":"Welcome to Project Mu"},{"location":"#primary-goals","text":"Initially, this project will focus on two central goals.","title":"Primary Goals"},{"location":"#share-our-active-code-tree-to-both-solicit-feedback-and-entice-partners-to-collaborate","text":"Project Mu is an active project. This is not a side project, mirror, clone, or example. This is the same code used today on many of Microsoft's 1 st party devices and it will be kept current because it must be to continue to enable shipping products.","title":"Share our active code tree to both solicit feedback and entice partners to collaborate"},{"location":"#promote-evangelize-and-support-an-industry-shift-to-a-more-collaborative-environment-so-we-all-can-build-and-maintain-products-with-lower-costs-and-higher-quality","text":"Today's open source projects, although extremely valuable, are very resource-intensive to interact with. This friction leads to major industry players avoiding public interaction, thus diminishing the overall community\u2019s value. The modern era of open source projects has incorporated new tools and procedures to lower this friction and it is our goal to leverage those tools. GitHub provides issue tracking, Pull Requests, Gated builds, tracked/required web-based code reviews, and CI/CD (Continuous integration and delivery). It is our belief that by leveraging and extending this automation and workflow, we can lower the friction and foster a safe place for all contributors to work.","title":"Promote, evangelize, and support an industry shift to a more collaborative environment so we all can build and maintain products with lower costs and higher quality"},{"location":"#guiding-principles","text":"Less is More * Be open to change / flexible - Keep learning. If it was easy this would have been solved before Design for code reuse Leverage tools / invest in automation","title":"Guiding Principles"},{"location":"#navigation","text":"Have a look around this site to see what is Project Mu. Start by reviewing the details of the community and our process. See how to interact and get involved, why it's different, how to work within or extend it, as well as where everything is located. Finally, explore the Developer Docs if you want to review more in-depth details.","title":"Navigation"},{"location":"#having-trouble","text":"Skim the FAQ","title":"Having trouble?"},{"location":"#roadmap","text":"After the first few months of Mu, our initial roadmap is largely complete. Any remaining items have been moved to the GitHub Issues and will continue to be tracked there. We hope to use GitHub Issues to track new roadmap items going forwards. Project Mu GitHub Issues","title":"Roadmap"},{"location":"#join-us","text":"Contact info and additional methods to collaborate coming soon.","title":"Join Us"},{"location":"#code-of-conduct","text":"This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Code of conduct"},{"location":"#reporting-issues","text":"Short answer: Open a github issue. More details: Contributing","title":"Reporting Issues"},{"location":"#contributing","text":"Short answer: Open a pull request. More details: Contributing","title":"Contributing"},{"location":"#license","text":"Refer to License Documentation Build Information Version: 0.9.1 Build Time: 2020-08-13 22:52","title":"License"},{"location":"faq/","text":"FAQ \u00b6 Purpose/Goals \u00b6 How is this related to TianoCore? \u00b6 As you can probably tell, Project Mu is based on TianoCore . It represents a variant of TianoCore that was customized within Microsoft for scaling and maintainability. It's not exactly a staging branch for TianoCore, as there are some changes that may not have application within or meet the explicit goals of that project, but it is a place where features and changes can be publicly featured and discussed. So, is this a fork? \u00b6 Not entirely. It is our goal to continue to treat TianoCore as a true upstream. Our release branches will always be based on the latest stable TianoCore release, and we will always try to PR viable fixes and features into the TianoCore project. What is it? Where is it going? \u00b6 Project Mu is a product of the Microsoft Core UEFI team and is the basis for the system firmware within a number of Microsoft products. It will continue to be maintained to reflect the FW practices and features leveraged for the best experience with Windows and other Microsoft products. A secondary purpose is to engage with the community, both in TianoCore and the industry at large. We hope that Project Mu serves as a concrete example for discussing different approaches to managing the challenges faced by the UEFI ecosystem. Content/Structure \u00b6 Is this really following \"Less is More\"? \u00b6 Yes. The idea is lowering the entanglement of code, lowering the coupling, and allowing the product to pick and choose the code it needs. This means when building any given product, you don't need all the Project Mu code. Why are there so many repos? \u00b6 Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. For details, see \"Repo Philosophy\" in What and Why .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#purposegoals","text":"","title":"Purpose/Goals"},{"location":"faq/#how-is-this-related-to-tianocore","text":"As you can probably tell, Project Mu is based on TianoCore . It represents a variant of TianoCore that was customized within Microsoft for scaling and maintainability. It's not exactly a staging branch for TianoCore, as there are some changes that may not have application within or meet the explicit goals of that project, but it is a place where features and changes can be publicly featured and discussed.","title":"How is this related to TianoCore?"},{"location":"faq/#so-is-this-a-fork","text":"Not entirely. It is our goal to continue to treat TianoCore as a true upstream. Our release branches will always be based on the latest stable TianoCore release, and we will always try to PR viable fixes and features into the TianoCore project.","title":"So, is this a fork?"},{"location":"faq/#what-is-it-where-is-it-going","text":"Project Mu is a product of the Microsoft Core UEFI team and is the basis for the system firmware within a number of Microsoft products. It will continue to be maintained to reflect the FW practices and features leveraged for the best experience with Windows and other Microsoft products. A secondary purpose is to engage with the community, both in TianoCore and the industry at large. We hope that Project Mu serves as a concrete example for discussing different approaches to managing the challenges faced by the UEFI ecosystem.","title":"What is it? Where is it going?"},{"location":"faq/#contentstructure","text":"","title":"Content/Structure"},{"location":"faq/#is-this-really-following-less-is-more","text":"Yes. The idea is lowering the entanglement of code, lowering the coupling, and allowing the product to pick and choose the code it needs. This means when building any given product, you don't need all the Project Mu code.","title":"Is this really following \"Less is More\"?"},{"location":"faq/#why-are-there-so-many-repos","text":"Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. For details, see \"Repo Philosophy\" in What and Why .","title":"Why are there so many repos?"},{"location":"license/","text":"Licensing for Project Mu \u00b6 Project Mu has numerous repositories. Each of these can have different licenses depending on the content and partner but in general we want OSS friendly licenses. For this documentation we use the following license. License \u00b6 BSD 2-Clause License Copyright \u00a9 Microsoft All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"license/#licensing-for-project-mu","text":"Project Mu has numerous repositories. Each of these can have different licenses depending on the content and partner but in general we want OSS friendly licenses. For this documentation we use the following license.","title":"Licensing for Project Mu"},{"location":"license/#license","text":"BSD 2-Clause License Copyright \u00a9 Microsoft All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"CodeDevelopment/compile/","text":"How to Build/Compile \u00b6 Info MAJOR UPDATE -- The Project Mu Python packages for UEFI support and build tools have migrated to Tianocore and as a result things have changed. These docs are now updated to leverage building with \"edk2-pytools\". The repository/product/project should describe the unique steps required to build and test. The build tools are now a set of unique single purpose built tools called \"stuart\". These tools together support building, updating binary dependencies, updating git dependencies, and other functions. Project Mu has two main patterns for building. Those will be described here to encourage pattern/code reuse and limit the required repository specific documentation. More details for pytools can be found here: https://github.com/tianocore/edk2-pytool-extensions/tree/master/docs https://github.com/tianocore/edk2-pytool-library/tree/master/docs CI multi-package Building and Testing aka stuart_ci_build \u00b6 stuart_ci_build is a framework for running a battery of tests against a single Mu repository (and its dependencies). A plugin model is used for adding additional tests. Today one such plugin is a basic compile test. Another plugin compiles host based unit tests and runs them. A third plugin checks for misspellings. Check out the repository for details on the tests. Additional test plugins are usually found in .pytool/Plugin It is often desirable to compile test code and at times there might not be a product to test with. This is also how the Pull Requests gates are implemented and enforced. The Process \u00b6 Open cmd prompt at workspace root Activate your python virtual environment Install or update Python dependencies using pip pip install --upgrade -r pip_requirements.txt Run stuart_setup to download required submodules. stuart_setup -c <PyTool Config File> Run stuart_ci_setup to download CI only dependencies stuart_ci_setup -c <PyTool Config File> Run stuart_update to download or update binary dependencies stuart_update -c <PyTool Config File> Run stuart_ci_build to build and test the packages stuart_ci_build -c <PyTool Config File> Open TestResults.xml in the build output for results (usually in workspace/Build) Open log files to debug any errors Info In Project Mu repos the config file is generally at .pytool/CISettings.py Project Mu runs on Windows 10 using the following tags: VS2017 and VS2019 Project Mu runs on Ubuntu 18.04 using the tags: GCC5 Each of the stuart commands can take in additional parameters. To see customized help run <stuart cmd> -c .pytool/CISettings.py -h Some common optional parameters that might allow the stuart operation to optimize for expected usage. For example if only building for X64 ARCH then the ARM compilers might not be downloaded. Or if using the VS2019 toolchain then GCC specific assets aren't needed. If you only want to run CI against the MdePkg and MdeModulePkg then you can do that with -p . -a <arch csv> - list of architectures to run for -p <packages csv> - list of packages to run against -t <targets csv> - list of targets to run for TOOL_CHAIN_TAG=<tag> - set toolchain for operation Project Build aka PlatformBuild aka stuart_build \u00b6 When you actually want to compile for a platform that will create a firmware binary which can be flashed and execute on a platform the process is generally as follows. Again the platform repository should have details but this is generally the process. The Process \u00b6 Open cmd prompt at workspace root Activate your python virtual environment Install or update Python dependencies using pip pip install --upgrade -r <pip_requirements.txt file> Run stuart_setup to download required submodules. stuart_setup -c <platform Config File> Run stuart_update to download or update binary dependencies stuart_update -c <platform Config File> Run stuart_build to build and test the packages stuart_build -c <platform Config File> Open the build output for log files to debug any errors (usually in workspace/Build) Info In Project Mu repos the platform config file is generally in the platform package. Toolchains and host OS support is defined by the platform documentation. Each of the stuart commands can take in additional parameters. To see customized help run <stuart cmd> -c <platform config file> -h Other features \u00b6 stuart_build leverages a common UefiBuild python component. This component provides a common set of features. The UefiBuild component documentation is published from the edk2-pytool-extensions repository but here are a few of the common features developers find useful. Control the target of the build. Pass Target=RELEASE Build a single module: BuildModule=MdePkg/ModuleToBuild.inf Build with reporting: Single report type BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD\" Change report file BUILDREPORT_FILE=filename.txt default is BUILD_REPORT.TXT All report types. BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\" Clean build: --clean Clean only (no compile): --cleanonly Skip some of the build steps: Skip the Edk2 build step: --skipbuild Skip pre or post build steps: --skipprebuild or --skippostbuild Change a Build variable that is used in Edk2 build process: BLD_*_DEBUG_OUTPUT_LEVEL=0x80000004 will be passed to DSC/FDF as DEBUG_OUTPUT_LEVEL . These variable names and behavior are platform defined. BLD_*_<var name> is used for builds of any target type unless there is a more specific version for the given target type. BLD_DEBUG_<var name> is used for debug builds only BLD_RELEASE_<var name> is used for release builds only Using a config file. To simplify calling of PlatformBuild.py if there is a BuildConfig.conf in the root of your UEFI workspace those parameters will be used as well. The command line overrides anything from the conf file. Example BuildConfig.conf \u00b6 # Turn on full build reports BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\"","title":"Compiling"},{"location":"CodeDevelopment/compile/#how-to-buildcompile","text":"Info MAJOR UPDATE -- The Project Mu Python packages for UEFI support and build tools have migrated to Tianocore and as a result things have changed. These docs are now updated to leverage building with \"edk2-pytools\". The repository/product/project should describe the unique steps required to build and test. The build tools are now a set of unique single purpose built tools called \"stuart\". These tools together support building, updating binary dependencies, updating git dependencies, and other functions. Project Mu has two main patterns for building. Those will be described here to encourage pattern/code reuse and limit the required repository specific documentation. More details for pytools can be found here: https://github.com/tianocore/edk2-pytool-extensions/tree/master/docs https://github.com/tianocore/edk2-pytool-library/tree/master/docs","title":"How to Build/Compile"},{"location":"CodeDevelopment/compile/#ci-multi-package-building-and-testing-aka-stuart_ci_build","text":"stuart_ci_build is a framework for running a battery of tests against a single Mu repository (and its dependencies). A plugin model is used for adding additional tests. Today one such plugin is a basic compile test. Another plugin compiles host based unit tests and runs them. A third plugin checks for misspellings. Check out the repository for details on the tests. Additional test plugins are usually found in .pytool/Plugin It is often desirable to compile test code and at times there might not be a product to test with. This is also how the Pull Requests gates are implemented and enforced.","title":"CI multi-package Building and Testing aka stuart_ci_build"},{"location":"CodeDevelopment/compile/#the-process","text":"Open cmd prompt at workspace root Activate your python virtual environment Install or update Python dependencies using pip pip install --upgrade -r pip_requirements.txt Run stuart_setup to download required submodules. stuart_setup -c <PyTool Config File> Run stuart_ci_setup to download CI only dependencies stuart_ci_setup -c <PyTool Config File> Run stuart_update to download or update binary dependencies stuart_update -c <PyTool Config File> Run stuart_ci_build to build and test the packages stuart_ci_build -c <PyTool Config File> Open TestResults.xml in the build output for results (usually in workspace/Build) Open log files to debug any errors Info In Project Mu repos the config file is generally at .pytool/CISettings.py Project Mu runs on Windows 10 using the following tags: VS2017 and VS2019 Project Mu runs on Ubuntu 18.04 using the tags: GCC5 Each of the stuart commands can take in additional parameters. To see customized help run <stuart cmd> -c .pytool/CISettings.py -h Some common optional parameters that might allow the stuart operation to optimize for expected usage. For example if only building for X64 ARCH then the ARM compilers might not be downloaded. Or if using the VS2019 toolchain then GCC specific assets aren't needed. If you only want to run CI against the MdePkg and MdeModulePkg then you can do that with -p . -a <arch csv> - list of architectures to run for -p <packages csv> - list of packages to run against -t <targets csv> - list of targets to run for TOOL_CHAIN_TAG=<tag> - set toolchain for operation","title":"The Process"},{"location":"CodeDevelopment/compile/#project-build-aka-platformbuild-aka-stuart_build","text":"When you actually want to compile for a platform that will create a firmware binary which can be flashed and execute on a platform the process is generally as follows. Again the platform repository should have details but this is generally the process.","title":"Project Build aka PlatformBuild aka stuart_build"},{"location":"CodeDevelopment/compile/#the-process_1","text":"Open cmd prompt at workspace root Activate your python virtual environment Install or update Python dependencies using pip pip install --upgrade -r <pip_requirements.txt file> Run stuart_setup to download required submodules. stuart_setup -c <platform Config File> Run stuart_update to download or update binary dependencies stuart_update -c <platform Config File> Run stuart_build to build and test the packages stuart_build -c <platform Config File> Open the build output for log files to debug any errors (usually in workspace/Build) Info In Project Mu repos the platform config file is generally in the platform package. Toolchains and host OS support is defined by the platform documentation. Each of the stuart commands can take in additional parameters. To see customized help run <stuart cmd> -c <platform config file> -h","title":"The Process"},{"location":"CodeDevelopment/compile/#other-features","text":"stuart_build leverages a common UefiBuild python component. This component provides a common set of features. The UefiBuild component documentation is published from the edk2-pytool-extensions repository but here are a few of the common features developers find useful. Control the target of the build. Pass Target=RELEASE Build a single module: BuildModule=MdePkg/ModuleToBuild.inf Build with reporting: Single report type BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD\" Change report file BUILDREPORT_FILE=filename.txt default is BUILD_REPORT.TXT All report types. BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\" Clean build: --clean Clean only (no compile): --cleanonly Skip some of the build steps: Skip the Edk2 build step: --skipbuild Skip pre or post build steps: --skipprebuild or --skippostbuild Change a Build variable that is used in Edk2 build process: BLD_*_DEBUG_OUTPUT_LEVEL=0x80000004 will be passed to DSC/FDF as DEBUG_OUTPUT_LEVEL . These variable names and behavior are platform defined. BLD_*_<var name> is used for builds of any target type unless there is a more specific version for the given target type. BLD_DEBUG_<var name> is used for debug builds only BLD_RELEASE_<var name> is used for release builds only Using a config file. To simplify calling of PlatformBuild.py if there is a BuildConfig.conf in the root of your UEFI workspace those parameters will be used as well. The command line overrides anything from the conf file.","title":"Other features"},{"location":"CodeDevelopment/compile/#example-buildconfigconf","text":"# Turn on full build reports BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\"","title":"Example BuildConfig.conf"},{"location":"CodeDevelopment/overview/","text":"Code Development Overview \u00b6 Tools \u00b6 First you will need to setup your UEFI development environment. Project Mu leverages most of the tools from TianoCore EDK2 . We have streamlined the process for the tool chains and systems we use but our project's goals are to support various tool chains and development environments. For the best experience or for those new to UEFI and Project Mu we have provided guidance in our prerequisites page. Code \u00b6 Next you will need to clone a repository or set of repositories to work on. For core work (Project Mu Repos) you can clone the desired repo, make your changes, run CI builds, run your tests, and submit a PR. For platform work (outside of Project Mu) you will need to clone the platform repository and then follow the platform setup process. See details on the compile page for more information about CI builds and how to compile a package or platform. Code should follow best practices. We are working to add some best practices on the requirements page. We also attempt to enforce these best practices thru our CI build process. Tests \u00b6 One area of focus for Project Mu is on testing. Firmware testing has traditionally been hard and very manual. With Project Mu and our recent contributions to Tianocore, we now have a unit test framework available to developers. Host based tests written in this framework will be automatically run during CI passes and are required for new contributions. Check out the testing page for more details.","title":"Overview"},{"location":"CodeDevelopment/overview/#code-development-overview","text":"","title":"Code Development Overview"},{"location":"CodeDevelopment/overview/#tools","text":"First you will need to setup your UEFI development environment. Project Mu leverages most of the tools from TianoCore EDK2 . We have streamlined the process for the tool chains and systems we use but our project's goals are to support various tool chains and development environments. For the best experience or for those new to UEFI and Project Mu we have provided guidance in our prerequisites page.","title":"Tools"},{"location":"CodeDevelopment/overview/#code","text":"Next you will need to clone a repository or set of repositories to work on. For core work (Project Mu Repos) you can clone the desired repo, make your changes, run CI builds, run your tests, and submit a PR. For platform work (outside of Project Mu) you will need to clone the platform repository and then follow the platform setup process. See details on the compile page for more information about CI builds and how to compile a package or platform. Code should follow best practices. We are working to add some best practices on the requirements page. We also attempt to enforce these best practices thru our CI build process.","title":"Code"},{"location":"CodeDevelopment/overview/#tests","text":"One area of focus for Project Mu is on testing. Firmware testing has traditionally been hard and very manual. With Project Mu and our recent contributions to Tianocore, we now have a unit test framework available to developers. Host based tests written in this framework will be automatically run during CI passes and are required for new contributions. Check out the testing page for more details.","title":"Tests"},{"location":"CodeDevelopment/prerequisites/","text":"Prerequisites for building Code \u00b6 Generally there are a set of tools required on the platform. Project Mu tries to minimize the number of global tools but there are a few. There could be more depending on the repository/product/platform you are building but this should get you started. If the repo requires other tools those should be documented within the repo. The tools also vary by Operating System and Compiler choice. Project Mu will document what is currently supported but the expectation is that between Project Mu and TianoCore Edk2 you could use any of those tool sets. Windows 10 x64 \u00b6 Python \u00b6 Download latest Python from https://www.python.org/downloads https://www.python.org/ftp/python/3.8.2/python-3.8.2-amd64.exe It is recommended you use the following options when installing python: include pip support include test support include venv virtual environment support Git \u00b6 Download latest Git For Windows from https://git-scm.com/download/win https://github.com/git-for-windows/git/releases/download/v2.25.1.windows.1/Git-2.25.1-64-bit.exe It is recommended you use the following options: Checkout as is, commit as is. Native Channel support (this will help in corp environments) Check the box to \"Enable Git Credential Manager\" Visual Studio 2019 preferred \u00b6 Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/16/release/vs_buildtools.exe Install from cmd line with required features (this set will change over time). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2019 Visual Studio 2017 \u00b6 Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/15/release/vs_buildtools.exe Install from cmd line with required features (this set will change over time). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2017 Optional - Windows Driver Kit \u00b6 Provides Inf2Cat.exe, needed to prepare Windows firmware update packages for signing . Download the WDK installer https://go.microsoft.com/fwlink/?linkid=2085767 Install from cmd line with required features (this set will change over time). wdksetup.exe /features OptionId.WindowsDriverKitComplete /q Optional - Create an Omnicache \u00b6 An Omnicache is a Project Mu tool that leverages git features to speed up git update operations. This helps speed up git operations if you have multiple workspaces by using the git \"--reference\" feature. Omnicache is documented in the Mu Pip Environment section of this site. Windows Subsystem For Linux (WSL) and Linux \u00b6 Basic directions here. https://github.com/tianocore/edk2-pytool-extensions/blob/master/docs/usability/using_linux.md Pre-built Linux containers \u00b6 Coming soon All Operating Systems - Python Virtual Environment and PyTools \u00b6 In all Operating Systems environments the PyTools python modules are needed. Python virtual environments are strongly suggested especially when doing development in multiple workspaces. Each workspace should have its own virtual environment as to not modify the global system state. Since Project Mu uses Pip modules this allows each workspace to keep the versions in sync with the workspace requirements. More info on Python Virtual Environments: https://docs.python.org/3/library/venv.html Workspace Virtual Environment Setup Process \u00b6 A sample directory layout of workspaces and Python Virtual Environments \u00b6 /Workspace1Root (basic platform) |-- src_of_project1 |-- venv <-- Virtual environment for Project in workspace root 1 | /Workspace2Root (basic + local pytool dev support) | -- src_of_project2 | -- venv <-- Virtual environment for Project in workspace root 2 pip requirements | -- venv_dev <-- Virtual environment configured to use local python modules | -- edk2-pytool-library <-- local clone of python modules in library | -- edk2-pytool-extensions <-- local clone of python modules in extensions Virtual environments only need to be created once per workspace. They must be activated in each new cmd shell. Open Cmd Prompt in the directory where you want to store your virtual environment. A directory adjacent to workspace directories is convenient. run python cmd python -m venv <your virtual env name> Activate it for your session. Activate Virtual Environment \u00b6 Do this each time you open a new command window to build your workspace. Open Cmd Prompt run activate script - for windows cmd prompt (cmd.exe) do this <your virtual env name>\\Script\\activate cd into your workspace directory Update/Install your python pip requirements. This is generally at the workspace root. pip install --upgrade -r requirements.txt Do dev work and run your builds!","title":"Tools and Prerequisite"},{"location":"CodeDevelopment/prerequisites/#prerequisites-for-building-code","text":"Generally there are a set of tools required on the platform. Project Mu tries to minimize the number of global tools but there are a few. There could be more depending on the repository/product/platform you are building but this should get you started. If the repo requires other tools those should be documented within the repo. The tools also vary by Operating System and Compiler choice. Project Mu will document what is currently supported but the expectation is that between Project Mu and TianoCore Edk2 you could use any of those tool sets.","title":"Prerequisites for building Code"},{"location":"CodeDevelopment/prerequisites/#windows-10-x64","text":"","title":"Windows 10 x64"},{"location":"CodeDevelopment/prerequisites/#python","text":"Download latest Python from https://www.python.org/downloads https://www.python.org/ftp/python/3.8.2/python-3.8.2-amd64.exe It is recommended you use the following options when installing python: include pip support include test support include venv virtual environment support","title":"Python"},{"location":"CodeDevelopment/prerequisites/#git","text":"Download latest Git For Windows from https://git-scm.com/download/win https://github.com/git-for-windows/git/releases/download/v2.25.1.windows.1/Git-2.25.1-64-bit.exe It is recommended you use the following options: Checkout as is, commit as is. Native Channel support (this will help in corp environments) Check the box to \"Enable Git Credential Manager\"","title":"Git"},{"location":"CodeDevelopment/prerequisites/#visual-studio-2019-preferred","text":"Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/16/release/vs_buildtools.exe Install from cmd line with required features (this set will change over time). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2019","title":"Visual Studio 2019 preferred"},{"location":"CodeDevelopment/prerequisites/#visual-studio-2017","text":"Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/15/release/vs_buildtools.exe Install from cmd line with required features (this set will change over time). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2017","title":"Visual Studio 2017"},{"location":"CodeDevelopment/prerequisites/#optional-windows-driver-kit","text":"Provides Inf2Cat.exe, needed to prepare Windows firmware update packages for signing . Download the WDK installer https://go.microsoft.com/fwlink/?linkid=2085767 Install from cmd line with required features (this set will change over time). wdksetup.exe /features OptionId.WindowsDriverKitComplete /q","title":"Optional - Windows Driver Kit"},{"location":"CodeDevelopment/prerequisites/#optional-create-an-omnicache","text":"An Omnicache is a Project Mu tool that leverages git features to speed up git update operations. This helps speed up git operations if you have multiple workspaces by using the git \"--reference\" feature. Omnicache is documented in the Mu Pip Environment section of this site.","title":"Optional - Create an Omnicache"},{"location":"CodeDevelopment/prerequisites/#windows-subsystem-for-linux-wsl-and-linux","text":"Basic directions here. https://github.com/tianocore/edk2-pytool-extensions/blob/master/docs/usability/using_linux.md","title":"Windows Subsystem For Linux (WSL) and Linux"},{"location":"CodeDevelopment/prerequisites/#pre-built-linux-containers","text":"Coming soon","title":"Pre-built Linux containers"},{"location":"CodeDevelopment/prerequisites/#all-operating-systems-python-virtual-environment-and-pytools","text":"In all Operating Systems environments the PyTools python modules are needed. Python virtual environments are strongly suggested especially when doing development in multiple workspaces. Each workspace should have its own virtual environment as to not modify the global system state. Since Project Mu uses Pip modules this allows each workspace to keep the versions in sync with the workspace requirements. More info on Python Virtual Environments: https://docs.python.org/3/library/venv.html","title":"All Operating Systems - Python Virtual Environment and PyTools"},{"location":"CodeDevelopment/prerequisites/#workspace-virtual-environment-setup-process","text":"","title":"Workspace Virtual Environment Setup Process"},{"location":"CodeDevelopment/prerequisites/#a-sample-directory-layout-of-workspaces-and-python-virtual-environments","text":"/Workspace1Root (basic platform) |-- src_of_project1 |-- venv <-- Virtual environment for Project in workspace root 1 | /Workspace2Root (basic + local pytool dev support) | -- src_of_project2 | -- venv <-- Virtual environment for Project in workspace root 2 pip requirements | -- venv_dev <-- Virtual environment configured to use local python modules | -- edk2-pytool-library <-- local clone of python modules in library | -- edk2-pytool-extensions <-- local clone of python modules in extensions Virtual environments only need to be created once per workspace. They must be activated in each new cmd shell. Open Cmd Prompt in the directory where you want to store your virtual environment. A directory adjacent to workspace directories is convenient. run python cmd python -m venv <your virtual env name> Activate it for your session.","title":"A sample directory layout of workspaces and Python Virtual Environments"},{"location":"CodeDevelopment/prerequisites/#activate-virtual-environment","text":"Do this each time you open a new command window to build your workspace. Open Cmd Prompt run activate script - for windows cmd prompt (cmd.exe) do this <your virtual env name>\\Script\\activate cd into your workspace directory Update/Install your python pip requirements. This is generally at the workspace root. pip install --upgrade -r requirements.txt Do dev work and run your builds!","title":"Activate Virtual Environment"},{"location":"CodeDevelopment/requirements/","text":"Requirements for contributing Source Code \u00b6 Basics \u00b6 Make sure it follows the package, repo, and codebase rules Make sure it builds Write a unit test for it. Test positive cases as well as negative cases. Make sure it has docs. Even a minimal readme.md will get collected and added to the docs. Make sure it has only valid characters encoded (often copy paste from Microsoft Word docs or the internet will lead to invalid characters) If it is a small change/tweak to existing code that originates outside of Project Mu please mark it with //MU_CHANGE Uefi Package \u00b6 UEFI Components \u00b6 All new modules must be listed in their containing package DSC in the components section All modules must follow the dependency rules of their containing package All modules within common layers should avoid silicon or architecture dependencies. Use existing libraries and functionality when possible Build out minimal required abstraction to allow other silicon or architectures to leverage common capabilities Public Header files \u00b6 Don't include other header files Don't mix public and private information in the same header file Implementation details should be contained to the instance Use \"doxygen\" style function header comments to clearly specify parameters and return results. Use a guidgen tool to define any guids For libraries: Library class should be listed in Package DEC file A NULL instance must be created that allows compiling and linking with minimal dependencies. Library Instance \u00b6 The supported module types in the INFs must be accurate. LIBRARY_CLASS: <Library Class Name>|<Module types supported by this instance> Use STATIC on each non-public function and non-public global to avoid conflicts with other modules. Use EFIAPI on all public library class functions. More info \u00b6 For general Edk2 and UEFI development additional information can be found at the TianoCore.org website.","title":"Code Requirements"},{"location":"CodeDevelopment/requirements/#requirements-for-contributing-source-code","text":"","title":"Requirements for contributing Source Code"},{"location":"CodeDevelopment/requirements/#basics","text":"Make sure it follows the package, repo, and codebase rules Make sure it builds Write a unit test for it. Test positive cases as well as negative cases. Make sure it has docs. Even a minimal readme.md will get collected and added to the docs. Make sure it has only valid characters encoded (often copy paste from Microsoft Word docs or the internet will lead to invalid characters) If it is a small change/tweak to existing code that originates outside of Project Mu please mark it with //MU_CHANGE","title":"Basics"},{"location":"CodeDevelopment/requirements/#uefi-package","text":"","title":"Uefi Package"},{"location":"CodeDevelopment/requirements/#uefi-components","text":"All new modules must be listed in their containing package DSC in the components section All modules must follow the dependency rules of their containing package All modules within common layers should avoid silicon or architecture dependencies. Use existing libraries and functionality when possible Build out minimal required abstraction to allow other silicon or architectures to leverage common capabilities","title":"UEFI Components"},{"location":"CodeDevelopment/requirements/#public-header-files","text":"Don't include other header files Don't mix public and private information in the same header file Implementation details should be contained to the instance Use \"doxygen\" style function header comments to clearly specify parameters and return results. Use a guidgen tool to define any guids For libraries: Library class should be listed in Package DEC file A NULL instance must be created that allows compiling and linking with minimal dependencies.","title":"Public Header files"},{"location":"CodeDevelopment/requirements/#library-instance","text":"The supported module types in the INFs must be accurate. LIBRARY_CLASS: <Library Class Name>|<Module types supported by this instance> Use STATIC on each non-public function and non-public global to avoid conflicts with other modules. Use EFIAPI on all public library class functions.","title":"Library Instance"},{"location":"CodeDevelopment/requirements/#more-info","text":"For general Edk2 and UEFI development additional information can be found at the TianoCore.org website.","title":"More info"},{"location":"CodeDevelopment/test/","text":"Tests \u00b6 Testing firmware is critical and should be done so much more than it is today. So please, start writing tests. A lot of work has been done to make it easier. Static Code Tests (analysis) \u00b6 stuart_ci_build provides a framework for running static tests on the code base. More details of the ever changing tests can be found here. https://github.com/microsoft/mu_basecore/tree/release/202002/.pytool/Plugin UEFI Unit Tests - C code \u00b6 It now exists!! There is a framework available in Tianocore and Project Mu basecore. Simple API here. https://github.com/microsoft/mu_basecore/blob/release/202002/MdePkg/Include/Library/UnitTestLib.h Implementation details here. https://github.com/microsoft/mu_basecore/tree/release/202002/UnitTestFrameworkPkg Host Based - console app \u00b6 Host based allow you to run your tests on the same machine in which you are compiling your code. These tests will run as applications within the operating system host environment. This is the preferred route when possible as these tests will automatically roll into the CI process and are much faster and easier to run. Obviously this means you will need to write your code and tests with limited UEFI dependencies. Any dependency your code has will need to be mocked or faked for the unit test scenario. The host test does leverage cmocka so lightweight mocking is possible. Target Based (UEFI Firmware on a device) \u00b6 Some testing just doesn't make sense to run as a host test. Tests that rely on system hardware and system state might only work as target tests. The unit test framework supports this and the implementation works for both. This can also include features that require reboots and saving state before the reboot so that tests can resume upon continued execution. UEFI Shell Based Functional Tests \u00b6 These can also leverage the UEFI target based tests. UEFI Shell Based Audit Tests \u00b6 These tests are often one off UEFI shell applications that collect system data and then compare that data against known good values for a system. This is because these types of tests have no right or wrong answer. Often we have a python script/component to the test to compare expected result to actual result. If the actual result doesn't match then this type of test should fail. Testing Automation for physical hardware \u00b6 The Project Mu team has done a lot of work with the open source project \"robot framework\". This framework provides a great logging and execution environment but at this time it is out of scope for Project Mu docs. If you want to know more, contact us as we are definitely willing to partner/share/engage. Testing Python \u00b6 Create pytest and/or python unit-test compatible tests. Make sure the python code passes the flake8 \"linter\"","title":"Testing"},{"location":"CodeDevelopment/test/#tests","text":"Testing firmware is critical and should be done so much more than it is today. So please, start writing tests. A lot of work has been done to make it easier.","title":"Tests"},{"location":"CodeDevelopment/test/#static-code-tests-analysis","text":"stuart_ci_build provides a framework for running static tests on the code base. More details of the ever changing tests can be found here. https://github.com/microsoft/mu_basecore/tree/release/202002/.pytool/Plugin","title":"Static Code Tests (analysis)"},{"location":"CodeDevelopment/test/#uefi-unit-tests-c-code","text":"It now exists!! There is a framework available in Tianocore and Project Mu basecore. Simple API here. https://github.com/microsoft/mu_basecore/blob/release/202002/MdePkg/Include/Library/UnitTestLib.h Implementation details here. https://github.com/microsoft/mu_basecore/tree/release/202002/UnitTestFrameworkPkg","title":"UEFI Unit Tests - C code"},{"location":"CodeDevelopment/test/#host-based-console-app","text":"Host based allow you to run your tests on the same machine in which you are compiling your code. These tests will run as applications within the operating system host environment. This is the preferred route when possible as these tests will automatically roll into the CI process and are much faster and easier to run. Obviously this means you will need to write your code and tests with limited UEFI dependencies. Any dependency your code has will need to be mocked or faked for the unit test scenario. The host test does leverage cmocka so lightweight mocking is possible.","title":"Host Based - console app"},{"location":"CodeDevelopment/test/#target-based-uefi-firmware-on-a-device","text":"Some testing just doesn't make sense to run as a host test. Tests that rely on system hardware and system state might only work as target tests. The unit test framework supports this and the implementation works for both. This can also include features that require reboots and saving state before the reboot so that tests can resume upon continued execution.","title":"Target Based (UEFI Firmware on a device)"},{"location":"CodeDevelopment/test/#uefi-shell-based-functional-tests","text":"These can also leverage the UEFI target based tests.","title":"UEFI Shell Based Functional Tests"},{"location":"CodeDevelopment/test/#uefi-shell-based-audit-tests","text":"These tests are often one off UEFI shell applications that collect system data and then compare that data against known good values for a system. This is because these types of tests have no right or wrong answer. Often we have a python script/component to the test to compare expected result to actual result. If the actual result doesn't match then this type of test should fail.","title":"UEFI Shell Based Audit Tests"},{"location":"CodeDevelopment/test/#testing-automation-for-physical-hardware","text":"The Project Mu team has done a lot of work with the open source project \"robot framework\". This framework provides a great logging and execution environment but at this time it is out of scope for Project Mu docs. If you want to know more, contact us as we are definitely willing to partner/share/engage.","title":"Testing Automation for physical hardware"},{"location":"CodeDevelopment/test/#testing-python","text":"Create pytest and/or python unit-test compatible tests. Make sure the python code passes the flake8 \"linter\"","title":"Testing Python"},{"location":"DeveloperDocs/attribution/","text":"Documentation framework attribution \u00b6 A special thank you to the people and projects that helped make Project Mu Documentation possible. Projects \u00b6 Mkdocs \u00b6 https://www.mkdocs.org/ MkDocs License (BSD) Copyright \u00a9 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. mkdocs macros plugin \u00b6 https://github.com/fralau/mkdocs_macros_plugin MIT License Copyright (C) 2018 Laurent Franceschetti Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Material for MkDocs \u00b6 https://squidfunk.github.io/mkdocs-material/ License MIT License Copyright \u00a9 2016 - 2017 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. PyMdown Extensions \u00b6 https://facelessuser.github.io/pymdown-extensions/ PyMdown Extensions The MIT License (MIT) (Except where stated below) Copyright \u00a9 2014 - 2018 Isaac Muse Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Doc Framework Attribution"},{"location":"DeveloperDocs/attribution/#documentation-framework-attribution","text":"A special thank you to the people and projects that helped make Project Mu Documentation possible.","title":"Documentation framework attribution"},{"location":"DeveloperDocs/attribution/#projects","text":"","title":"Projects"},{"location":"DeveloperDocs/attribution/#mkdocs","text":"https://www.mkdocs.org/ MkDocs License (BSD) Copyright \u00a9 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Mkdocs"},{"location":"DeveloperDocs/attribution/#mkdocs-macros-plugin","text":"https://github.com/fralau/mkdocs_macros_plugin MIT License Copyright (C) 2018 Laurent Franceschetti Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"mkdocs macros plugin"},{"location":"DeveloperDocs/attribution/#material-for-mkdocs","text":"https://squidfunk.github.io/mkdocs-material/ License MIT License Copyright \u00a9 2016 - 2017 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Material for MkDocs"},{"location":"DeveloperDocs/attribution/#pymdown-extensions","text":"https://facelessuser.github.io/pymdown-extensions/ PyMdown Extensions The MIT License (MIT) (Except where stated below) Copyright \u00a9 2014 - 2018 Isaac Muse Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"PyMdown Extensions"},{"location":"DeveloperDocs/build_community_docs/","text":"Building Community Docs \u00b6 Info Today this process has been validated for use on Windows 10. This setup process is expected to roughly the same on other operating systems and none of the actual documentation source or tools should have any OS dependency. Get the docs repository \u00b6 First, you need to clone the project mu docs repository. git clone https://github.com/Microsoft/mu.git Install required tools \u00b6 Install python (Current suggested version is 3.7.x). Current min requirement is python 3.4+. Checkout python.org for directions. Install pip. Generally, this is done when installing python but can also be done as its own process. Details here https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip Update pip. python -m pip install --upgrade pip Install dependencies. pip install --upgrade -r requirements.txt if wanting to use spell check Install nodejs from https://nodejs.org/en/ Install cspell npm install -g cspell Install Git on your path (Required for generating dynamic repo based content during preprocess) General Suggested documentation workflow \u00b6 open two command windows at the root of docs repository Window 1: Use to serve files locally Use mkdocs serve Any changes from the DocBuild process will be picked up and served Window 2: Use to preprocess the source repo files Run the DocBuild.py command from this window Make changes to the docs in source repos or this repo and then re-run the DocBuild.py build command Pre-process with dynamic content from source repo(s) \u00b6 Create \"repos\" folder (somewhere outside of workspace) Clone all repositories for dynamic content here Set each repo to the branch/commit that you want to document run the DocBuild.py command supplying the parameters DocBuild.py --clean --build --OutputDir docs --yml mkdocs_base.yml --RootDir ..\\repos Pre-process with no source repo(s) content \u00b6 run the DocBuild.py command supplying minimal parameters DocBuild.py --clean --build --yml mkdocs_base.yml Clean / Remove all pre-processed content \u00b6 use DocBuild.py command DocBuild.py --clean --yml <path to yml base file> --OutputDir <docs folder> Check for character encoding issues \u00b6 navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run Utf8Test python script cmd prompt Utf8Test.py --RootDir docs should complete with no errors Note Note you can also run it on any dynamic content by using a different RootDir parameter. Use -h for usage to get more detailed information of any failures Use mkdocs to build the docs \u00b6 navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run mkdocs build from cmd prompt at root mkdocs build -s -v should complete with no errors Spell check the docs \u00b6 navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run command to spell check cspell docs/**/*.md should complete with no errors False Spelling Errors If the spelling error is a false positive there are two solutions: If it is a valid word or commonly understood term then add the word to the cspell.json config file words section Update the cspell.json file ignorePaths element to ignore the entire file. Locally serve the docs \u00b6 One great feature of mkdocs is how easy it is to locally serve the docs to validate your changes. Use mkdocs to serve your local copy mkdocs serve navigate to 127.0.0.1:8000 in web browser Important If you get an error like Config file 'mkdocs.yml' does not exist you must run the preprocess step. Advanced doc features \u00b6 We do turn on a few advanced/extension features. Please use these carefully as they may break compatibility if the publishing engine is changed. Checkout the sample syntax / test page for syntax and information.","title":"How To Build"},{"location":"DeveloperDocs/build_community_docs/#building-community-docs","text":"Info Today this process has been validated for use on Windows 10. This setup process is expected to roughly the same on other operating systems and none of the actual documentation source or tools should have any OS dependency.","title":"Building Community Docs"},{"location":"DeveloperDocs/build_community_docs/#get-the-docs-repository","text":"First, you need to clone the project mu docs repository. git clone https://github.com/Microsoft/mu.git","title":"Get the docs repository"},{"location":"DeveloperDocs/build_community_docs/#install-required-tools","text":"Install python (Current suggested version is 3.7.x). Current min requirement is python 3.4+. Checkout python.org for directions. Install pip. Generally, this is done when installing python but can also be done as its own process. Details here https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip Update pip. python -m pip install --upgrade pip Install dependencies. pip install --upgrade -r requirements.txt if wanting to use spell check Install nodejs from https://nodejs.org/en/ Install cspell npm install -g cspell Install Git on your path (Required for generating dynamic repo based content during preprocess)","title":"Install required tools"},{"location":"DeveloperDocs/build_community_docs/#general-suggested-documentation-workflow","text":"open two command windows at the root of docs repository Window 1: Use to serve files locally Use mkdocs serve Any changes from the DocBuild process will be picked up and served Window 2: Use to preprocess the source repo files Run the DocBuild.py command from this window Make changes to the docs in source repos or this repo and then re-run the DocBuild.py build command","title":"General Suggested documentation workflow"},{"location":"DeveloperDocs/build_community_docs/#pre-process-with-dynamic-content-from-source-repos","text":"Create \"repos\" folder (somewhere outside of workspace) Clone all repositories for dynamic content here Set each repo to the branch/commit that you want to document run the DocBuild.py command supplying the parameters DocBuild.py --clean --build --OutputDir docs --yml mkdocs_base.yml --RootDir ..\\repos","title":"Pre-process with dynamic content from source repo(s)"},{"location":"DeveloperDocs/build_community_docs/#pre-process-with-no-source-repos-content","text":"run the DocBuild.py command supplying minimal parameters DocBuild.py --clean --build --yml mkdocs_base.yml","title":"Pre-process with no source repo(s) content"},{"location":"DeveloperDocs/build_community_docs/#clean-remove-all-pre-processed-content","text":"use DocBuild.py command DocBuild.py --clean --yml <path to yml base file> --OutputDir <docs folder>","title":"Clean / Remove all pre-processed content"},{"location":"DeveloperDocs/build_community_docs/#check-for-character-encoding-issues","text":"navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run Utf8Test python script cmd prompt Utf8Test.py --RootDir docs should complete with no errors Note Note you can also run it on any dynamic content by using a different RootDir parameter. Use -h for usage to get more detailed information of any failures","title":"Check for character encoding issues"},{"location":"DeveloperDocs/build_community_docs/#use-mkdocs-to-build-the-docs","text":"navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run mkdocs build from cmd prompt at root mkdocs build -s -v should complete with no errors","title":"Use mkdocs to build the docs"},{"location":"DeveloperDocs/build_community_docs/#spell-check-the-docs","text":"navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run command to spell check cspell docs/**/*.md should complete with no errors False Spelling Errors If the spelling error is a false positive there are two solutions: If it is a valid word or commonly understood term then add the word to the cspell.json config file words section Update the cspell.json file ignorePaths element to ignore the entire file.","title":"Spell check the docs"},{"location":"DeveloperDocs/build_community_docs/#locally-serve-the-docs","text":"One great feature of mkdocs is how easy it is to locally serve the docs to validate your changes. Use mkdocs to serve your local copy mkdocs serve navigate to 127.0.0.1:8000 in web browser Important If you get an error like Config file 'mkdocs.yml' does not exist you must run the preprocess step.","title":"Locally serve the docs"},{"location":"DeveloperDocs/build_community_docs/#advanced-doc-features","text":"We do turn on a few advanced/extension features. Please use these carefully as they may break compatibility if the publishing engine is changed. Checkout the sample syntax / test page for syntax and information.","title":"Advanced doc features"},{"location":"DeveloperDocs/developer_docs/","text":"Developer Docs \u00b6 Philosophy \u00b6 Documentation is critical. There is a steep learning curve in UEFI and no amount of documentation will change that, but at a minimum quick, clear, and easy documentation can help everyone adopt features faster and with higher confidence. Our documentation system will focus on making this an easy, low friction, and collaborative process. The pull request process will eventually compel developers to submit documentation whenever they submit new components and refactoring. Documentation will be done in markdown as this has the benefit of being easily readable in both plain text as well as transformed into a richer experience. It also is quick to learn and to write. Currently, we leverage mkdocs as our publishing engine but since all content is in markdown it could be transitioned to another engine without significant reinvestment. Community documentation \u00b6 This content is documented in static markdown files within the Project Mu repository. We leverage mkdocs to generate web-hosted content on every change and host these using github.io. These static files focus on how the project and community interact. We strongly encourage contribution and follow the standard PR model for all changes, big and small. Developer documentation \u00b6 This content is documented in a couple of ways. There are static markdown files in the Project Mu repository. This contains details about high level concepts, howto articles, and features of the project and all repos within Project Mu. Examples: Code layout, git usage, tools, building, packaging, etc. There is repo and package level documentation for features. These are also static markdown files but these are contained within the repo that contains the feature. A \u201cdocs\u201d folder for each repo and each package will host this content. Changes will also follow the standard PR model for the containing repo. Next, there is feature and instance documentation. This should inform a developer interested in the implementation specifics of what this module is and what additional requirements it has including code dependencies and limitations. This should be documented in markdown files located with the component. These should be updated whenever the component is updated and should be part of a code PR. Finally, for API and traditional functional documentation, our current stance is this is required in code (public APIs) but the published documentation (doxygen html, pdf, etc) is not necessary. Code tools like vscode already provide a lower friction method to index, find def, and search that uses this content directly embedded in the code.","title":"Overview"},{"location":"DeveloperDocs/developer_docs/#developer-docs","text":"","title":"Developer Docs"},{"location":"DeveloperDocs/developer_docs/#philosophy","text":"Documentation is critical. There is a steep learning curve in UEFI and no amount of documentation will change that, but at a minimum quick, clear, and easy documentation can help everyone adopt features faster and with higher confidence. Our documentation system will focus on making this an easy, low friction, and collaborative process. The pull request process will eventually compel developers to submit documentation whenever they submit new components and refactoring. Documentation will be done in markdown as this has the benefit of being easily readable in both plain text as well as transformed into a richer experience. It also is quick to learn and to write. Currently, we leverage mkdocs as our publishing engine but since all content is in markdown it could be transitioned to another engine without significant reinvestment.","title":"Philosophy"},{"location":"DeveloperDocs/developer_docs/#community-documentation","text":"This content is documented in static markdown files within the Project Mu repository. We leverage mkdocs to generate web-hosted content on every change and host these using github.io. These static files focus on how the project and community interact. We strongly encourage contribution and follow the standard PR model for all changes, big and small.","title":"Community documentation"},{"location":"DeveloperDocs/developer_docs/#developer-documentation","text":"This content is documented in a couple of ways. There are static markdown files in the Project Mu repository. This contains details about high level concepts, howto articles, and features of the project and all repos within Project Mu. Examples: Code layout, git usage, tools, building, packaging, etc. There is repo and package level documentation for features. These are also static markdown files but these are contained within the repo that contains the feature. A \u201cdocs\u201d folder for each repo and each package will host this content. Changes will also follow the standard PR model for the containing repo. Next, there is feature and instance documentation. This should inform a developer interested in the implementation specifics of what this module is and what additional requirements it has including code dependencies and limitations. This should be documented in markdown files located with the component. These should be updated whenever the component is updated and should be part of a code PR. Finally, for API and traditional functional documentation, our current stance is this is required in code (public APIs) but the published documentation (doxygen html, pdf, etc) is not necessary. Code tools like vscode already provide a lower friction method to index, find def, and search that uses this content directly embedded in the code.","title":"Developer documentation"},{"location":"DeveloperDocs/doc_sample_test/","text":"Documentation Sample / Test file / Advanced doc features \u00b6 mkdocs macros plugin \u00b6 This plugin allows providing some variables in mkdocs.yml file and then reference those variables using jinja2 syntax in md files. Most of these variables are populated and created during the DocBuild step and inserted into the yml file. Docs: https://mkdocs-macros-plugin.readthedocs.io/en/latest/ Src Project: https://github.com/fralau/mkdocs_macros_plugin Material theme \u00b6 This theme provides the skin for the site. This also provides capabilities thru plugins. https://squidfunk.github.io/mkdocs-material/ Markdown Extensions \u00b6 The Material theme supports markdown extensions. Check the yml file for what extensions are currently on. Below is more specific info. https://squidfunk.github.io/mkdocs-material/extensions/permalinks/ https://squidfunk.github.io/mkdocs-material/extensions/pymdown/ Admonition plugin \u00b6 This plugin in combo with the material theme provides great looking ways for doc developers to highlight parts of their message. Please check out: https://squidfunk.github.io/mkdocs-material/extensions/admonition/ for the capabilities and syntax. One example: Note Sample note here. emoji support \u00b6 Who doesn't love using emojis. Icon usage has shown to help communicate directions and cross language barriers. https://facelessuser.github.io/pymdown-extensions/extensions/emoji/ Twitter, github, and emojione tags available. Tab support \u00b6 You can now use tabs to organize your content. Basic Example Tab1 Some content in tab1 Example Tab2 More content in tab2 Link to More details https://facelessuser.github.io/pymdown-extensions/extensions/tabbed/ Mermaid Charts \u00b6 A picture is worth 1000 words so lets make a few. Mermaid supports a rich, easily integrated set. https://mermaid-js.github.io/mermaid/ This is done with custom superfences configuration. See here: https://github.com/squidfunk/mkdocs-material/issues/693#issuecomment-591933381 To support this the Mermaid format is supported. Chart Result graph TD; A-->B; A-->C; B-->D; C-->D; Markdown Syntax ```mermaid graph TD; A-->B; A-->C; B-->D; C-->D; ``` Others \u00b6 Check out the mkdocs.yml file for other extensions and details can be found in the links above.","title":"Sample Syntax"},{"location":"DeveloperDocs/doc_sample_test/#documentation-sample-test-file-advanced-doc-features","text":"","title":"Documentation Sample / Test file / Advanced doc features"},{"location":"DeveloperDocs/doc_sample_test/#mkdocs-macros-plugin","text":"This plugin allows providing some variables in mkdocs.yml file and then reference those variables using jinja2 syntax in md files. Most of these variables are populated and created during the DocBuild step and inserted into the yml file. Docs: https://mkdocs-macros-plugin.readthedocs.io/en/latest/ Src Project: https://github.com/fralau/mkdocs_macros_plugin","title":"mkdocs macros plugin"},{"location":"DeveloperDocs/doc_sample_test/#material-theme","text":"This theme provides the skin for the site. This also provides capabilities thru plugins. https://squidfunk.github.io/mkdocs-material/","title":"Material theme"},{"location":"DeveloperDocs/doc_sample_test/#markdown-extensions","text":"The Material theme supports markdown extensions. Check the yml file for what extensions are currently on. Below is more specific info. https://squidfunk.github.io/mkdocs-material/extensions/permalinks/ https://squidfunk.github.io/mkdocs-material/extensions/pymdown/","title":"Markdown Extensions"},{"location":"DeveloperDocs/doc_sample_test/#admonition-plugin","text":"This plugin in combo with the material theme provides great looking ways for doc developers to highlight parts of their message. Please check out: https://squidfunk.github.io/mkdocs-material/extensions/admonition/ for the capabilities and syntax. One example: Note Sample note here.","title":"Admonition plugin"},{"location":"DeveloperDocs/doc_sample_test/#emoji-support","text":"Who doesn't love using emojis. Icon usage has shown to help communicate directions and cross language barriers. https://facelessuser.github.io/pymdown-extensions/extensions/emoji/ Twitter, github, and emojione tags available.","title":"emoji support"},{"location":"DeveloperDocs/doc_sample_test/#tab-support","text":"You can now use tabs to organize your content. Basic Example Tab1 Some content in tab1 Example Tab2 More content in tab2 Link to More details https://facelessuser.github.io/pymdown-extensions/extensions/tabbed/","title":"Tab support"},{"location":"DeveloperDocs/doc_sample_test/#mermaid-charts","text":"A picture is worth 1000 words so lets make a few. Mermaid supports a rich, easily integrated set. https://mermaid-js.github.io/mermaid/ This is done with custom superfences configuration. See here: https://github.com/squidfunk/mkdocs-material/issues/693#issuecomment-591933381 To support this the Mermaid format is supported. Chart Result graph TD; A-->B; A-->C; B-->D; C-->D; Markdown Syntax ```mermaid graph TD; A-->B; A-->C; B-->D; C-->D; ```","title":"Mermaid Charts"},{"location":"DeveloperDocs/doc_sample_test/#others","text":"Check out the mkdocs.yml file for other extensions and details can be found in the links above.","title":"Others"},{"location":"DeveloperDocs/requirements/","text":"Requirements for contributing documentation \u00b6 Conventions and lessons learned \u00b6 Please update this list as you learn more. filenames should all be lowercase. filenames should use \"_\" to separate words and should not have spaces. all links to pages are case sensitive (when published to GitHub the server is case sensitive) use a code editor like vscode for markdown. It has linting support and will identify issues prior to build. If you markdown has images: Awesome. Images help make docs more informative and easier to understand Path in markdown to image must be relative Suggested to put in same directory as md file image filename must end with _mu. extension . Example my_image_name_mu.png Supported image extensions are gif, jpg, png","title":"Documentation Requirements"},{"location":"DeveloperDocs/requirements/#requirements-for-contributing-documentation","text":"","title":"Requirements for contributing documentation"},{"location":"DeveloperDocs/requirements/#conventions-and-lessons-learned","text":"Please update this list as you learn more. filenames should all be lowercase. filenames should use \"_\" to separate words and should not have spaces. all links to pages are case sensitive (when published to GitHub the server is case sensitive) use a code editor like vscode for markdown. It has linting support and will identify issues prior to build. If you markdown has images: Awesome. Images help make docs more informative and easier to understand Path in markdown to image must be relative Suggested to put in same directory as md file image filename must end with _mu. extension . Example my_image_name_mu.png Supported image extensions are gif, jpg, png","title":"Conventions and lessons learned"},{"location":"How/contributing/","text":"How to contribute \u00b6 There are three common ways to contribute. Participate in discussions using GitHub issues. Contribute documentation by opening a GitHub Pull Request. Contribute code by opening a GitHub Pull Request Issue Tracker Usage \u00b6 https://github.com/Microsoft/mu/issues General feedback and discussions \u00b6 Please start a discussion on the issue tracker. Bugs and feature requests \u00b6 For non-security related bugs please log a new issue on the Project Mu repo issue tracker . The best way to get your bug fixed is to be as detailed as you can be about the problem. Providing a code snippet or sample driver that exposes the issue with steps to reproduce the problem is ideal. Reporting security issues and bugs \u00b6 Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter . Contributions of Documentation and/or Code \u00b6 Pull Requests \u00b6 If you don't know what a pull request is read this article: https://help.github.com/articles/about-pull-requests . Make sure the repository can build and all tests pass. Familiarize yourself with the project workflow and our coding conventions. General workflow \u00b6 Fork Repository in GitHub Make desired changes. Build it, test it, document it Submit a Pull Request back to the development branch you would like to target. You will be asked to digitally sign a CLA The server will run some builds and tests and report status Community and reviewers will provide feedback in the Pull Request Make changes / adjust based on feedback and discussion Keep your PR branch in-sync with the branch you are targeting and resolve any merge conflicts Once the the PR status is all passing it can be squashed and merged (just press the button in the PR). If the PR is ready the maintainers may complete it for you. That is it. Thanks for contributing. More details on : Code Development Tests Development Documentation Development Contributor License Agreement (CLA) \u00b6 This project welcomes contributions and suggestions. Most (code and documentation) contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.","title":"Contributing"},{"location":"How/contributing/#how-to-contribute","text":"There are three common ways to contribute. Participate in discussions using GitHub issues. Contribute documentation by opening a GitHub Pull Request. Contribute code by opening a GitHub Pull Request","title":"How to contribute"},{"location":"How/contributing/#issue-tracker-usage","text":"https://github.com/Microsoft/mu/issues","title":"Issue Tracker Usage"},{"location":"How/contributing/#general-feedback-and-discussions","text":"Please start a discussion on the issue tracker.","title":"General feedback and discussions"},{"location":"How/contributing/#bugs-and-feature-requests","text":"For non-security related bugs please log a new issue on the Project Mu repo issue tracker . The best way to get your bug fixed is to be as detailed as you can be about the problem. Providing a code snippet or sample driver that exposes the issue with steps to reproduce the problem is ideal.","title":"Bugs and feature requests"},{"location":"How/contributing/#reporting-security-issues-and-bugs","text":"Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter .","title":"Reporting security issues and bugs"},{"location":"How/contributing/#contributions-of-documentation-andor-code","text":"","title":"Contributions of Documentation and/or Code"},{"location":"How/contributing/#pull-requests","text":"If you don't know what a pull request is read this article: https://help.github.com/articles/about-pull-requests . Make sure the repository can build and all tests pass. Familiarize yourself with the project workflow and our coding conventions.","title":"Pull Requests"},{"location":"How/contributing/#general-workflow","text":"Fork Repository in GitHub Make desired changes. Build it, test it, document it Submit a Pull Request back to the development branch you would like to target. You will be asked to digitally sign a CLA The server will run some builds and tests and report status Community and reviewers will provide feedback in the Pull Request Make changes / adjust based on feedback and discussion Keep your PR branch in-sync with the branch you are targeting and resolve any merge conflicts Once the the PR status is all passing it can be squashed and merged (just press the button in the PR). If the PR is ready the maintainers may complete it for you. That is it. Thanks for contributing. More details on : Code Development Tests Development Documentation Development","title":"General workflow"},{"location":"How/contributing/#contributor-license-agreement-cla","text":"This project welcomes contributions and suggestions. Most (code and documentation) contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.","title":"Contributor License Agreement (CLA)"},{"location":"How/release_process/","text":"Overview \u00b6 Contents and Process Under Active Development The basics of this process are identical to those followed by the Project Mu firmware integration and release process internal to Microsoft, but the formal documentation, branch naming, and tagging process is a work in progress. While this is how we expect things to work, there may be changes within the the first few releases driven by feedback within the team and any external consumers/contributors. In the interest of maintaining a close, well-defined relationship with the upstream project, TianoCore, the active release branch of Project Mu is periodically deprecated and all Mu-related changes are rebased onto a selected commit of TianoCore. This keeps Project Mu up to date with TianoCore while highlighting all Project Mu differences in the most recent commits and encouraging the reverse integration of all changes/fixes back into TianoCore In general, the life-cycle of active code follows the following path: All active work in Project Mu is performed on a release/* branch, named sequentially according to the date of TianoCore commit that it's based on (e.g. release/201808 is based on the edk2-stable201808 branch in TianoCore). Work proceeds on that branch until a new TianoCore integration is targeted, at which point a new branch is created and all existing changes are rebased onto the new branch and the new branch is used for all active development going forward. At this point, the previous branch enters a stabilization period where further tests are performed and only bug fixes are allowed to be committed. After stabilization, the branch is labeled as stable and will only receive critical bug fixes either directly to the branch or backported from a more recent release. release/* branches will be maintained in LTS (Long-Term Support) for at least the next two releases. The below diagram illustrates the life-cycle of a single branch and indicates the critical points in its lifetime. These critical points will be applied as tags for reference and documentation. The tags are given a name relative to the target branch and consist of: Upstream base, Rebase complete, Rebase builds, Rebase boots, RCn, and Stable. These tags are discussed in more detail below. Important Due to the impacts of the rebase process on the history of Mu release branches, any downstream consumers will have to follow a similar integration process when upgrading to a new release. Any custom changes made within the Project Mu repos will have to be rebased from one release to the next. This is why we strongly discourage forking Project Mu for direct modification (ie. consumption, not contribution). Instead, leverage the distributed repo management system and override management system to integrate proprietary code/modules. Current Branch Status \u00b6 While the earliest release branches may not be included in this process, starting with release/201903 and going forwards the status of each branch will be recorded in the README.rst file at the root of the branch. In general, the README found in Basecore will contain information that is common to all of the Mu submodules, but each submodule will also have its own README for each release branch that contains notes specific to the development that occurs in that submodule during a release cycle. The README will also contain a summary of the branch status at a given time. For example, here is a sample status for Basecore release/201903 as of the time of this writing: Current Phase: Development Entered Current Phase: 2019/03/25 Planned Exit Date: May 2019 Upstream Integration Phase \u00b6 At this time, we are targeting upstream integrations for roughly once a quarter, attempting to align 1:1 with the TianoCore stable release cadence. Prior to an integration, the status dashboard (not yet created) will be updated with the target date of completion and the target TianoCore commit and/or release. For example, a plan was made to transition off of release/20180529 when TianoCore announced the edk2-stable201808 release. Once a commit is selected, a set of rebase commits will be chosen from the active (previous) release/* branch. Ideally, these commits would include everything from the previous rebase through the most recent *_RC tag. For example, when moving from the release/201808 branch, the commits will be selected from 1808_Upstream (not inclusive) tag to 1808_RC1 . After selection, this list of commits will be evaluated to determine whether any changes are no longer needed in the Mu history. The most likely causes of this action are: A change was submitted to TianoCore and has been accepted since the last rebase. Therefore, the change is no longer needed in Mu history. A change was reverted or modified more recently in Mu history, and the history of this change was squashed to maintain simplicity when comparing with upstream (TianoCore). Once all evaluation is completed, the rebase will be performed in the new release/* branch. This branch will then be built for a reference platform (to be selected by internal team) and booted, at which point it will be considered the active development branch. Integration Milestone Tags \u00b6 During integration, multiple tags are applied to the branch to serve as milestones. They also serve as reference point for changelog documentation that is produced during the integration process. These tags are described below: *_Upstream This tag is placed on the exact TianoCore commit that a given release branch started from. This is used as a reference point between branches and relative to the rebase operation. The documentation produced for this tag contains the differences in TianoCore between this branch and the previous branch. For branches that originated from TianoCore releases, this changelog should be identical to the TianoCore changelog. *_Rebase This tag is placed on the commit at the branch HEAD once the rebase is completed. The only changes to the commits from the last branch should be merge conflict resolutions and any history simplification as described above. The documentation produced for this tag contains a record of these resolutions and simplifications. *_RefBuild This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully build. The documentation produced for this tag contains any changes required to get the reference platform building. It includes a list of changes outside the Mu project that are recommended for any consuming platform. *_RefBoot This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully boot. The documentation produced for this tag contains any changes required to get the reference platform booting. It includes a list of changes outside the Mu project that are recommended for any consuming platform. In each of these cases, the * will be replaced with a corresponding branch name. For example, the tags associated with release/201808 will be prefixed with 1808 (e.g. 1808_Rebase , 1808_RC1 , etc.). Active Development Phase \u00b6 During the active development phase, the release branch is open for comment and contribution both internally and publicly. All work contributed by the Project Mu team will be publicly available after an internal PR review. This commits will automatically be mirrored to the public repos. Similarly, all completed public PRs are mirrored in internal review repos (with preference being given to the public PR in event of a conflict). While this means that there will be times where Project Mu team will make contributions without going through a full public PR review, all code is open to comment and contribution, up to and including a full revert of the internal Mu team contribution. Public Contribution/Commentary \u00b6 For information on the contribution policies and steps, see the How to Contribute document. Upstream Cherry-Picks \u00b6 In the event that a critical change is made in the TianoCore upstream during the Active Development phase, the Project Mu team (with any suggestions or comment from downstream contributors) will evaluate the change for a mid-release cherry pick. If warranted, the commit(s) will be cherry-picked directly from TianoCore and prefixed with a \"CHERRY-PICK\" tag in the commit message so they can be cleaned up in the next rebase. Stabilization Phase \u00b6 When warranted, active development on the active release/* branch will be halted so that it may enter a period of rigorous testing and stabilization. Upon entering the Stabilization phase, the branch will be tagged with a *_RC1 tag and only bug fixes will be accepted from then on. Any defects or regressions found during stabilization will be fixed and documented. Once confidence is built in the stability of the code, the branch will be tagged as *_Stable and it will enter LTS. It is Project Mu's goal that this cadence be aligned with the TianoCore release cadence, with the previous branch stabilizing at the same time a new TianoCore release is available. In this way, development can seamlessly move to the next release/* branch without lapse in availability. Note It is possible that the *_RC1 tag be applied to the same commit as *_Stable if there are no defects found in the branch. (Because that happens all the time.) It is also possible that multiple *_RCn tags may be useful to distinguish between milestones of a particularly protracted Stabilization phase. Transition Branches \u00b6 In the event that it becomes necessary to stabilize a release/* branch prior to the availability of a suitable TianoCore commit for rebasing, all active development will move to a dev/* branch that will branch from the previous *_RC1 tag. If bugs are discovered in the Stabilization phase for the release/* branch, they will also be fixed in the dev/* branch and all changes made in the dev/* branch will be rebased as part of the next release/* branch when it is ready. Long-Term Support (LTS) \u00b6 It is Project Mu's goal that all release/* branches continue to be maintained with active bug fixes -- as necessary -- for at least two full releases after the branch becomes stable. The Project Mu team will serve as the primary deciding body for whether a bug fix to the current release/* branch merits porting back to the prior two branches, but community input or suggestions are always welcome. All release branches that make it to the Stabilization phase will be hosted and kept in the repository in perpetuity. If any change was required to this policy (perhaps for server considerations), the branches will remain archived for posterity and should be available by request. Lifetime of a Single Integration \u00b6 TBD","title":"Release Process"},{"location":"How/release_process/#overview","text":"Contents and Process Under Active Development The basics of this process are identical to those followed by the Project Mu firmware integration and release process internal to Microsoft, but the formal documentation, branch naming, and tagging process is a work in progress. While this is how we expect things to work, there may be changes within the the first few releases driven by feedback within the team and any external consumers/contributors. In the interest of maintaining a close, well-defined relationship with the upstream project, TianoCore, the active release branch of Project Mu is periodically deprecated and all Mu-related changes are rebased onto a selected commit of TianoCore. This keeps Project Mu up to date with TianoCore while highlighting all Project Mu differences in the most recent commits and encouraging the reverse integration of all changes/fixes back into TianoCore In general, the life-cycle of active code follows the following path: All active work in Project Mu is performed on a release/* branch, named sequentially according to the date of TianoCore commit that it's based on (e.g. release/201808 is based on the edk2-stable201808 branch in TianoCore). Work proceeds on that branch until a new TianoCore integration is targeted, at which point a new branch is created and all existing changes are rebased onto the new branch and the new branch is used for all active development going forward. At this point, the previous branch enters a stabilization period where further tests are performed and only bug fixes are allowed to be committed. After stabilization, the branch is labeled as stable and will only receive critical bug fixes either directly to the branch or backported from a more recent release. release/* branches will be maintained in LTS (Long-Term Support) for at least the next two releases. The below diagram illustrates the life-cycle of a single branch and indicates the critical points in its lifetime. These critical points will be applied as tags for reference and documentation. The tags are given a name relative to the target branch and consist of: Upstream base, Rebase complete, Rebase builds, Rebase boots, RCn, and Stable. These tags are discussed in more detail below. Important Due to the impacts of the rebase process on the history of Mu release branches, any downstream consumers will have to follow a similar integration process when upgrading to a new release. Any custom changes made within the Project Mu repos will have to be rebased from one release to the next. This is why we strongly discourage forking Project Mu for direct modification (ie. consumption, not contribution). Instead, leverage the distributed repo management system and override management system to integrate proprietary code/modules.","title":"Overview"},{"location":"How/release_process/#current-branch-status","text":"While the earliest release branches may not be included in this process, starting with release/201903 and going forwards the status of each branch will be recorded in the README.rst file at the root of the branch. In general, the README found in Basecore will contain information that is common to all of the Mu submodules, but each submodule will also have its own README for each release branch that contains notes specific to the development that occurs in that submodule during a release cycle. The README will also contain a summary of the branch status at a given time. For example, here is a sample status for Basecore release/201903 as of the time of this writing: Current Phase: Development Entered Current Phase: 2019/03/25 Planned Exit Date: May 2019","title":"Current Branch Status"},{"location":"How/release_process/#upstream-integration-phase","text":"At this time, we are targeting upstream integrations for roughly once a quarter, attempting to align 1:1 with the TianoCore stable release cadence. Prior to an integration, the status dashboard (not yet created) will be updated with the target date of completion and the target TianoCore commit and/or release. For example, a plan was made to transition off of release/20180529 when TianoCore announced the edk2-stable201808 release. Once a commit is selected, a set of rebase commits will be chosen from the active (previous) release/* branch. Ideally, these commits would include everything from the previous rebase through the most recent *_RC tag. For example, when moving from the release/201808 branch, the commits will be selected from 1808_Upstream (not inclusive) tag to 1808_RC1 . After selection, this list of commits will be evaluated to determine whether any changes are no longer needed in the Mu history. The most likely causes of this action are: A change was submitted to TianoCore and has been accepted since the last rebase. Therefore, the change is no longer needed in Mu history. A change was reverted or modified more recently in Mu history, and the history of this change was squashed to maintain simplicity when comparing with upstream (TianoCore). Once all evaluation is completed, the rebase will be performed in the new release/* branch. This branch will then be built for a reference platform (to be selected by internal team) and booted, at which point it will be considered the active development branch.","title":"Upstream Integration Phase"},{"location":"How/release_process/#integration-milestone-tags","text":"During integration, multiple tags are applied to the branch to serve as milestones. They also serve as reference point for changelog documentation that is produced during the integration process. These tags are described below: *_Upstream This tag is placed on the exact TianoCore commit that a given release branch started from. This is used as a reference point between branches and relative to the rebase operation. The documentation produced for this tag contains the differences in TianoCore between this branch and the previous branch. For branches that originated from TianoCore releases, this changelog should be identical to the TianoCore changelog. *_Rebase This tag is placed on the commit at the branch HEAD once the rebase is completed. The only changes to the commits from the last branch should be merge conflict resolutions and any history simplification as described above. The documentation produced for this tag contains a record of these resolutions and simplifications. *_RefBuild This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully build. The documentation produced for this tag contains any changes required to get the reference platform building. It includes a list of changes outside the Mu project that are recommended for any consuming platform. *_RefBoot This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully boot. The documentation produced for this tag contains any changes required to get the reference platform booting. It includes a list of changes outside the Mu project that are recommended for any consuming platform. In each of these cases, the * will be replaced with a corresponding branch name. For example, the tags associated with release/201808 will be prefixed with 1808 (e.g. 1808_Rebase , 1808_RC1 , etc.).","title":"Integration Milestone Tags"},{"location":"How/release_process/#active-development-phase","text":"During the active development phase, the release branch is open for comment and contribution both internally and publicly. All work contributed by the Project Mu team will be publicly available after an internal PR review. This commits will automatically be mirrored to the public repos. Similarly, all completed public PRs are mirrored in internal review repos (with preference being given to the public PR in event of a conflict). While this means that there will be times where Project Mu team will make contributions without going through a full public PR review, all code is open to comment and contribution, up to and including a full revert of the internal Mu team contribution.","title":"Active Development Phase"},{"location":"How/release_process/#public-contributioncommentary","text":"For information on the contribution policies and steps, see the How to Contribute document.","title":"Public Contribution/Commentary"},{"location":"How/release_process/#upstream-cherry-picks","text":"In the event that a critical change is made in the TianoCore upstream during the Active Development phase, the Project Mu team (with any suggestions or comment from downstream contributors) will evaluate the change for a mid-release cherry pick. If warranted, the commit(s) will be cherry-picked directly from TianoCore and prefixed with a \"CHERRY-PICK\" tag in the commit message so they can be cleaned up in the next rebase.","title":"Upstream Cherry-Picks"},{"location":"How/release_process/#stabilization-phase","text":"When warranted, active development on the active release/* branch will be halted so that it may enter a period of rigorous testing and stabilization. Upon entering the Stabilization phase, the branch will be tagged with a *_RC1 tag and only bug fixes will be accepted from then on. Any defects or regressions found during stabilization will be fixed and documented. Once confidence is built in the stability of the code, the branch will be tagged as *_Stable and it will enter LTS. It is Project Mu's goal that this cadence be aligned with the TianoCore release cadence, with the previous branch stabilizing at the same time a new TianoCore release is available. In this way, development can seamlessly move to the next release/* branch without lapse in availability. Note It is possible that the *_RC1 tag be applied to the same commit as *_Stable if there are no defects found in the branch. (Because that happens all the time.) It is also possible that multiple *_RCn tags may be useful to distinguish between milestones of a particularly protracted Stabilization phase.","title":"Stabilization Phase"},{"location":"How/release_process/#transition-branches","text":"In the event that it becomes necessary to stabilize a release/* branch prior to the availability of a suitable TianoCore commit for rebasing, all active development will move to a dev/* branch that will branch from the previous *_RC1 tag. If bugs are discovered in the Stabilization phase for the release/* branch, they will also be fixed in the dev/* branch and all changes made in the dev/* branch will be rebased as part of the next release/* branch when it is ready.","title":"Transition Branches"},{"location":"How/release_process/#long-term-support-lts","text":"It is Project Mu's goal that all release/* branches continue to be maintained with active bug fixes -- as necessary -- for at least two full releases after the branch becomes stable. The Project Mu team will serve as the primary deciding body for whether a bug fix to the current release/* branch merits porting back to the prior two branches, but community input or suggestions are always welcome. All release branches that make it to the Stabilization phase will be hosted and kept in the repository in perpetuity. If any change was required to this policy (perhaps for server considerations), the branches will remain archived for posterity and should be available by request.","title":"Long-Term Support (LTS)"},{"location":"How/release_process/#lifetime-of-a-single-integration","text":"TBD","title":"Lifetime of a Single Integration"},{"location":"How/using_project_mu/","text":"How to setup a new Repo for a Platform that will use Project MU? \u00b6 This document will describe the base guidelines for setting up a Project MU repo. You will need to install the prerequisites tools Determine how to layout your project and the content Look at layout to understand our recommended repository layout. You can also look at ms-iot iMX8 for a real platform implementation. Check out our docs in our Project Mu Teams channel as we have presentations on OVMF and Intel OpenKBL platforms. Nomenclature \u00b6 I will use the term workspace root to reference the base folder for your code tree. Ordinarily, we use the Platform Repository as the outer-most layer. This means that the outermost git repository is where we store Platform specific files and libraries. In this case, our Platform Repo is also our workspace root . If you choose to have a different repository layout, it will be important to note what your workspace root is, as it should still be the base folder of your code tree. Submodules are full git repos on their own. What we do with these repos is add them as sub-repos to the workspace root . Git will create a .gitmodules file that contains links to the repo and default branches. There are git submodule commands that you can use to work with your submodules, such as: git submodule add <url> <path> # url to submodule, path to submodule installation git submodule update --init --recursive # Recursively initializes and updates all submodules. git submodule foreach git status # git submodule foreach can be used to run a command in each submodule. git status is just an example. For more information available here . Create Git Repo \u00b6 Make new directory. mkdir NewPlatformRepo cd NewPlatformRepo git init This will serve as our Platform Repository as well as our Workspace Root. For more information on creating a Git repo, here are command line instructions and here are web instructions . Add pertinent submodules \u00b6 Project MU is separated into submodules. For each submodule that you need for your project, run the \"git submodule add\" command to add it to your base Repository. The path after the URL is the path we typically use to group the submodules. You can change it if you'd like, just remember your environment will diverge from the one in these instructions. MU_BASECORE \u00b6 This is the core section of TianoCore. Contains the guts of UEFI, forked from TianoCore, as well as the BaseTools needed to build. You will need this to continue. git submodule add https://github.com/Microsoft/mu_basecore.git MU_BASECORE MU_PLUS \u00b6 Additional, optional libraries and tools we've added to make MU great! git submodule add https://github.com/Microsoft/mu_plus.git Common/MU MU_TIANO_PLUS \u00b6 Additional, optional libraries and tools forked from TianoCore. git submodule add https://github.com/Microsoft/mu_tiano_plus.git Common/TIANO MU_OEM_SAMPLE \u00b6 This module is a sample implementation of a FrontPage and several BDS support libraries. This module is intended to be forked and customized. git submodule add https://github.com/Microsoft/mu_oem_sample.git Common/MU_OEM_SAMPLE MU_SILICON_ARM_TIANO \u00b6 Silicon code from TianoCore has been broken out into individual submodules. This is the ARM specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_arm_tiano.git Silicon/ARM/TIANO MU_SILICON_INTEL_TIANO \u00b6 Silicon code from TianoCore has been broken out into individual submodules. This is the Intel specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_intel_tiano.git Silicon/INTEL/TIANO You can run git submodule --update --init to make sure all the submodules are set up. Adding your platform contents \u00b6 New_Platform_Repo/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # MU_PLUS, MU_OEM_SAMPLE, MU_TIANO_PLUS are generally created by the \"git submodule ...\" commands shown above \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 PlatformGroup/ \u2502 \u2514\u2500\u2500 PlatformName/ \u2502 \u2514\u2500\u2500 PlatformBuild.py # Python script to provide information to the build process. \u2502 \u2514\u2500\u2500 Platform.dsc # List of UEFI libraries and drivers to compile, as well as platform settings. \u2502 \u2514\u2500\u2500 Platform.fdf # List of UEFI Drivers to put into Firmware Volumes. \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ # You may want to create a separate git repo for Silicon code to enable development with partners. \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules You will need to create PlatformBuild.py, Platform.dsc, and Platform.fdf. These files will go inside the platform folder, which will be New_Platform_Repo/PlatformGroup/PlatformName . The ms-iot iMX8 repo can help you get started as a layout reference and can demonstrate the PlatformBuild file. More information about PlatformBuild can be found here .","title":"Using"},{"location":"How/using_project_mu/#how-to-setup-a-new-repo-for-a-platform-that-will-use-project-mu","text":"This document will describe the base guidelines for setting up a Project MU repo. You will need to install the prerequisites tools Determine how to layout your project and the content Look at layout to understand our recommended repository layout. You can also look at ms-iot iMX8 for a real platform implementation. Check out our docs in our Project Mu Teams channel as we have presentations on OVMF and Intel OpenKBL platforms.","title":"How to setup a new Repo for a Platform that will use Project MU?"},{"location":"How/using_project_mu/#nomenclature","text":"I will use the term workspace root to reference the base folder for your code tree. Ordinarily, we use the Platform Repository as the outer-most layer. This means that the outermost git repository is where we store Platform specific files and libraries. In this case, our Platform Repo is also our workspace root . If you choose to have a different repository layout, it will be important to note what your workspace root is, as it should still be the base folder of your code tree. Submodules are full git repos on their own. What we do with these repos is add them as sub-repos to the workspace root . Git will create a .gitmodules file that contains links to the repo and default branches. There are git submodule commands that you can use to work with your submodules, such as: git submodule add <url> <path> # url to submodule, path to submodule installation git submodule update --init --recursive # Recursively initializes and updates all submodules. git submodule foreach git status # git submodule foreach can be used to run a command in each submodule. git status is just an example. For more information available here .","title":"Nomenclature"},{"location":"How/using_project_mu/#create-git-repo","text":"Make new directory. mkdir NewPlatformRepo cd NewPlatformRepo git init This will serve as our Platform Repository as well as our Workspace Root. For more information on creating a Git repo, here are command line instructions and here are web instructions .","title":"Create Git Repo"},{"location":"How/using_project_mu/#add-pertinent-submodules","text":"Project MU is separated into submodules. For each submodule that you need for your project, run the \"git submodule add\" command to add it to your base Repository. The path after the URL is the path we typically use to group the submodules. You can change it if you'd like, just remember your environment will diverge from the one in these instructions.","title":"Add pertinent submodules"},{"location":"How/using_project_mu/#mu_basecore","text":"This is the core section of TianoCore. Contains the guts of UEFI, forked from TianoCore, as well as the BaseTools needed to build. You will need this to continue. git submodule add https://github.com/Microsoft/mu_basecore.git MU_BASECORE","title":"MU_BASECORE"},{"location":"How/using_project_mu/#mu_plus","text":"Additional, optional libraries and tools we've added to make MU great! git submodule add https://github.com/Microsoft/mu_plus.git Common/MU","title":"MU_PLUS"},{"location":"How/using_project_mu/#mu_tiano_plus","text":"Additional, optional libraries and tools forked from TianoCore. git submodule add https://github.com/Microsoft/mu_tiano_plus.git Common/TIANO","title":"MU_TIANO_PLUS"},{"location":"How/using_project_mu/#mu_oem_sample","text":"This module is a sample implementation of a FrontPage and several BDS support libraries. This module is intended to be forked and customized. git submodule add https://github.com/Microsoft/mu_oem_sample.git Common/MU_OEM_SAMPLE","title":"MU_OEM_SAMPLE"},{"location":"How/using_project_mu/#mu_silicon_arm_tiano","text":"Silicon code from TianoCore has been broken out into individual submodules. This is the ARM specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_arm_tiano.git Silicon/ARM/TIANO","title":"MU_SILICON_ARM_TIANO"},{"location":"How/using_project_mu/#mu_silicon_intel_tiano","text":"Silicon code from TianoCore has been broken out into individual submodules. This is the Intel specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_intel_tiano.git Silicon/INTEL/TIANO You can run git submodule --update --init to make sure all the submodules are set up.","title":"MU_SILICON_INTEL_TIANO"},{"location":"How/using_project_mu/#adding-your-platform-contents","text":"New_Platform_Repo/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # MU_PLUS, MU_OEM_SAMPLE, MU_TIANO_PLUS are generally created by the \"git submodule ...\" commands shown above \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 PlatformGroup/ \u2502 \u2514\u2500\u2500 PlatformName/ \u2502 \u2514\u2500\u2500 PlatformBuild.py # Python script to provide information to the build process. \u2502 \u2514\u2500\u2500 Platform.dsc # List of UEFI libraries and drivers to compile, as well as platform settings. \u2502 \u2514\u2500\u2500 Platform.fdf # List of UEFI Drivers to put into Firmware Volumes. \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ # You may want to create a separate git repo for Silicon code to enable development with partners. \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules You will need to create PlatformBuild.py, Platform.dsc, and Platform.fdf. These files will go inside the platform folder, which will be New_Platform_Repo/PlatformGroup/PlatformName . The ms-iot iMX8 repo can help you get started as a layout reference and can demonstrate the PlatformBuild file. More information about PlatformBuild can be found here .","title":"Adding your platform contents"},{"location":"WhatAndWhy/features/","text":"Features \u00b6 Summary \u00b6 Project Mu features will generally be found in a \"MU\" sub-module, for example, \"Common/MU\" or \"Silicon/Intel/MU\". What major features does Project Mu bring to the table above/beyond EDK2? Feature List \u00b6 Pluggable, cross-device, performance-optimized BDS Device Firmware Configuration Interface (DFCI) - enables practical MDM management PBKDF2-based BIOS password example Support for EKU-based trust anchors during signature validation Microsoft unit test framework Audit, function, & performance tests for platform features Scalable Python build environment Build plug in: override tracking tool Build plug in: flash descriptor analysis Binary package management via NuGet Capsule signing via signtool.exe Up-to-date Visual Studio compiler support Base64 encode for binary objects XML Support Package Features Coming Soon \u00b6 Modern BIOS menu example (Surface inspired) On screen keyboard (OSK) with mouse, touch support Graphical end-to-end boot performance analysis library and tool Infineon TPM firmware update via Capsule On screen notifications: color bars to inform users that a device is not in a production configuration Features integrated into Tiano \u00b6 Safe Integer library Heap Guard ESRT DXE driver Scalable device FMP framework Progress bar for Capsule Updates TCG FV pre hashing optimization NVME shutdown","title":"Features"},{"location":"WhatAndWhy/features/#features","text":"","title":"Features"},{"location":"WhatAndWhy/features/#summary","text":"Project Mu features will generally be found in a \"MU\" sub-module, for example, \"Common/MU\" or \"Silicon/Intel/MU\". What major features does Project Mu bring to the table above/beyond EDK2?","title":"Summary"},{"location":"WhatAndWhy/features/#feature-list","text":"Pluggable, cross-device, performance-optimized BDS Device Firmware Configuration Interface (DFCI) - enables practical MDM management PBKDF2-based BIOS password example Support for EKU-based trust anchors during signature validation Microsoft unit test framework Audit, function, & performance tests for platform features Scalable Python build environment Build plug in: override tracking tool Build plug in: flash descriptor analysis Binary package management via NuGet Capsule signing via signtool.exe Up-to-date Visual Studio compiler support Base64 encode for binary objects XML Support Package","title":"Feature List"},{"location":"WhatAndWhy/features/#features-coming-soon","text":"Modern BIOS menu example (Surface inspired) On screen keyboard (OSK) with mouse, touch support Graphical end-to-end boot performance analysis library and tool Infineon TPM firmware update via Capsule On screen notifications: color bars to inform users that a device is not in a production configuration","title":"Features Coming Soon"},{"location":"WhatAndWhy/features/#features-integrated-into-tiano","text":"Safe Integer library Heap Guard ESRT DXE driver Scalable device FMP framework Progress bar for Capsule Updates TCG FV pre hashing optimization NVME shutdown","title":"Features integrated into Tiano"},{"location":"WhatAndWhy/layout/","text":"Dependencies and Layout \u00b6 Conceptual Layers \u00b6 A modern, full-featured, product-ready UEFI firmware codebase combines code from a multitude of sources: TianoCore EDK2 UEFI standard-based code Value-add code from TianoCore Silicon vendor hardware initialization code Silicon vendor value-add code Independent BIOS Vendor code ODM/OEM customization code OS firmware support code Legacy BIOS compatibility code Board-specific code etc. Some of the above components come from closed-source projects (silicon vendors, IBVs, OEMs), others are open source. Each component is supported at its own schedule with new features and bug fixes, creating a problem of stale code if not synced up regularly. Compound the version and source problem with the sheer size: a common UEFI codebase is typically well above 1 million LOC and only goes up from there. What is a dependency \u00b6 To understand the layering you must first understand the terminology. There are two types of code assets. A definition of something. Generally, this is defined in an accessible header file. This is the API provided by some asset. This API can be \"depended\" upon to provide some capability. An implementation of something. Example of a dependency: DxeCore in the Basecore layer includes a TimerLib interface. TimerLib interface is defined in the same Basecore layer as DxeCore, so in this case a Basecore module is depending on a Basecore interface. This is allowed. Another example: Silicon-layer module implements a TimerLib interface defined in Basecore. Here, a Silicon layer module depends on a Basecore interface. This is allowed. Architecture \u00b6 Project Mu is an attempt to create a rigid layering scheme that defines the hierarchy of dependencies. Architectural goal kept in mind when designing this layering scheme is a controlled, limited scope, and allowed dependencies for each module within a given layer. It is important to know, when implementing a module, what the module is allowed to depend on. When creating an interface, it is important to identify the correct layer for it such that all the consuming modules are located in the layers below. Motivation and goals of the layering scheme: Easy component integration Code reuse Only carry relevant code Dependency Block Diagram \u00b6 File Layout \u00b6 To best preserve and delineate these concepts of componentization and unidirectional dependency, we have chosen to lay out our repository files in a structure that reinforces the same mentality. The underlying logic of this layout is to clearly distinguish each layer from the rest. As such, the Basecore -- which is considered foundational -- is broken out on its own, followed by the Common repos, followed by the Silicon, followed by the Platform. As mentioned elsewhere, Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the firmware ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. One of the goals of Project Mu is to make this seemingly complicated layout easier to work with. Min Platform Example \u00b6 A simple tree might look like this... project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # Common code optional, but probably not required \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u2514\u2500\u2500 Sample/ \u2502 \u2514\u2500\u2500 MyMinPlatform # Platform-specific build files and code \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Note that this file structure is likely located in a Git repository, and every \"ALL CAPS\" directory in this example is a Git submodule/nested repository. Surface Laptop Example \u00b6 For a real-world example, this is a tree that could build the Surface Laptop product, including both open- and closed-source repositories: project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u251c\u2500\u2500 MSCORE_INTERNAL/ # Proprietary code and code not yet approved for public distribution \u2502 \u251c\u2500\u2500 MU/ \u2502 \u251c\u2500\u2500 MU_TIANO/ \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common features like FrontPage \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u251c\u2500\u2500 Surface/ \u2502 \u2502 \u251c\u2500\u2500 SurfKbl/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 Laptop/ # Surface Laptop-Specific Platform Code \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Others/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 Silicon/ \u2502 \u251c\u2500\u2500 Intel/ \u2502 \u2502 \u251c\u2500\u2500 KBL/ # Intel KBL Reference Code \u2502 \u2502 \u251c\u2500\u2500 MU/ # Project Mu Intel Common Code \u2502 \u2502 \u251c\u2500\u2500 MU_TIANO/ # Project Mu Intel Code from TianoCore \u2502 \u2502 \u2514\u2500\u2500 SURF_KBL/ # Surface Customizations/Overrides for KBL Ref Code \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common HW like ECs \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Once again, the \"ALL CAPS\" directories are submodules.","title":"Dependencies and Layout"},{"location":"WhatAndWhy/layout/#dependencies-and-layout","text":"","title":"Dependencies and Layout"},{"location":"WhatAndWhy/layout/#conceptual-layers","text":"A modern, full-featured, product-ready UEFI firmware codebase combines code from a multitude of sources: TianoCore EDK2 UEFI standard-based code Value-add code from TianoCore Silicon vendor hardware initialization code Silicon vendor value-add code Independent BIOS Vendor code ODM/OEM customization code OS firmware support code Legacy BIOS compatibility code Board-specific code etc. Some of the above components come from closed-source projects (silicon vendors, IBVs, OEMs), others are open source. Each component is supported at its own schedule with new features and bug fixes, creating a problem of stale code if not synced up regularly. Compound the version and source problem with the sheer size: a common UEFI codebase is typically well above 1 million LOC and only goes up from there.","title":"Conceptual Layers"},{"location":"WhatAndWhy/layout/#what-is-a-dependency","text":"To understand the layering you must first understand the terminology. There are two types of code assets. A definition of something. Generally, this is defined in an accessible header file. This is the API provided by some asset. This API can be \"depended\" upon to provide some capability. An implementation of something. Example of a dependency: DxeCore in the Basecore layer includes a TimerLib interface. TimerLib interface is defined in the same Basecore layer as DxeCore, so in this case a Basecore module is depending on a Basecore interface. This is allowed. Another example: Silicon-layer module implements a TimerLib interface defined in Basecore. Here, a Silicon layer module depends on a Basecore interface. This is allowed.","title":"What is a dependency"},{"location":"WhatAndWhy/layout/#architecture","text":"Project Mu is an attempt to create a rigid layering scheme that defines the hierarchy of dependencies. Architectural goal kept in mind when designing this layering scheme is a controlled, limited scope, and allowed dependencies for each module within a given layer. It is important to know, when implementing a module, what the module is allowed to depend on. When creating an interface, it is important to identify the correct layer for it such that all the consuming modules are located in the layers below. Motivation and goals of the layering scheme: Easy component integration Code reuse Only carry relevant code","title":"Architecture"},{"location":"WhatAndWhy/layout/#dependency-block-diagram","text":"","title":"Dependency Block Diagram"},{"location":"WhatAndWhy/layout/#file-layout","text":"To best preserve and delineate these concepts of componentization and unidirectional dependency, we have chosen to lay out our repository files in a structure that reinforces the same mentality. The underlying logic of this layout is to clearly distinguish each layer from the rest. As such, the Basecore -- which is considered foundational -- is broken out on its own, followed by the Common repos, followed by the Silicon, followed by the Platform. As mentioned elsewhere, Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the firmware ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. One of the goals of Project Mu is to make this seemingly complicated layout easier to work with.","title":"File Layout"},{"location":"WhatAndWhy/layout/#min-platform-example","text":"A simple tree might look like this... project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # Common code optional, but probably not required \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u2514\u2500\u2500 Sample/ \u2502 \u2514\u2500\u2500 MyMinPlatform # Platform-specific build files and code \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Note that this file structure is likely located in a Git repository, and every \"ALL CAPS\" directory in this example is a Git submodule/nested repository.","title":"Min Platform Example"},{"location":"WhatAndWhy/layout/#surface-laptop-example","text":"For a real-world example, this is a tree that could build the Surface Laptop product, including both open- and closed-source repositories: project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u251c\u2500\u2500 MSCORE_INTERNAL/ # Proprietary code and code not yet approved for public distribution \u2502 \u251c\u2500\u2500 MU/ \u2502 \u251c\u2500\u2500 MU_TIANO/ \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common features like FrontPage \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u251c\u2500\u2500 Surface/ \u2502 \u2502 \u251c\u2500\u2500 SurfKbl/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 Laptop/ # Surface Laptop-Specific Platform Code \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Others/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 Silicon/ \u2502 \u251c\u2500\u2500 Intel/ \u2502 \u2502 \u251c\u2500\u2500 KBL/ # Intel KBL Reference Code \u2502 \u2502 \u251c\u2500\u2500 MU/ # Project Mu Intel Common Code \u2502 \u2502 \u251c\u2500\u2500 MU_TIANO/ # Project Mu Intel Code from TianoCore \u2502 \u2502 \u2514\u2500\u2500 SURF_KBL/ # Surface Customizations/Overrides for KBL Ref Code \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common HW like ECs \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Once again, the \"ALL CAPS\" directories are submodules.","title":"Surface Laptop Example"},{"location":"WhatAndWhy/overview/","text":"Overview \u00b6 Project Organization \u00b6 This documentation is hosted in the main repository for Project Mu, which is used as a central collection point for community interaction and documentation. The build system and firmware code for the project is hosted in a number of other repositories, grouped/divided by function, partner, license, and dependencies. Several of these repositories are brought together by the build system to create a FW project, but we'll get into those details later. ;) For now, an overview of the repositories and what code you'll find there... Mu Basecore \u00b6 This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this code should be one or more of the following: Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services Mu Common Plus \u00b6 The packages found in this repository are contributed entirely by Project Mu. They should be common to all silicon architectures and only depend on Mu Basecore. These packages provide features and functionality that are entirely optional, but may be recommended for PC platform FW. Mu Tiano Plus \u00b6 This repository contains only modules that were originally sourced from TianoCore. They are not essential for any particular platform, but are likely useful to many platforms. The versions contained in this repo are modified and/or improved to work with the rest of Project Mu. Repo Philosophy \u00b6 Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. Examples of this are: A downstream contributor wants to add a generic feature with a silicon-specific implementation. This feature would be leveraged by Common code. If all code were in one repository, no barriers would be in place to prevent the contributor from directly calling from Common code into the Silicon implementation. By forcing the API/interface to be published in a separate repository, we can ensure that the unidirectional dependency relationship is maintained. Module A and Module B both provide optional functionality. However, Module A is far more likely to be consume by a wide audience than Module B. To achieve \"Less is More\", Module A may be placed in a different repos to enable downstream consumers to carry as little \"unused\" code as possible, since it's likely they would not need Module B in their code tree. A downstream consumer is producing a product in conjunction with a vendor/partner. While most of the enabling code for the vendor component is open-source, a portion of it is only released under NDA. By having multiple repositories comprise a single workspace, the downstream consumer is able to maximize their open-source consumption (which minimizes forking) while maintaining the legal requirements of closed-source/proprietary partitioning.","title":"Overview"},{"location":"WhatAndWhy/overview/#overview","text":"","title":"Overview"},{"location":"WhatAndWhy/overview/#project-organization","text":"This documentation is hosted in the main repository for Project Mu, which is used as a central collection point for community interaction and documentation. The build system and firmware code for the project is hosted in a number of other repositories, grouped/divided by function, partner, license, and dependencies. Several of these repositories are brought together by the build system to create a FW project, but we'll get into those details later. ;) For now, an overview of the repositories and what code you'll find there...","title":"Project Organization"},{"location":"WhatAndWhy/overview/#mu-basecore","text":"This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this code should be one or more of the following: Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services","title":"Mu Basecore"},{"location":"WhatAndWhy/overview/#mu-common-plus","text":"The packages found in this repository are contributed entirely by Project Mu. They should be common to all silicon architectures and only depend on Mu Basecore. These packages provide features and functionality that are entirely optional, but may be recommended for PC platform FW.","title":"Mu Common Plus"},{"location":"WhatAndWhy/overview/#mu-tiano-plus","text":"This repository contains only modules that were originally sourced from TianoCore. They are not essential for any particular platform, but are likely useful to many platforms. The versions contained in this repo are modified and/or improved to work with the rest of Project Mu.","title":"Mu Tiano Plus"},{"location":"WhatAndWhy/overview/#repo-philosophy","text":"Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. Examples of this are: A downstream contributor wants to add a generic feature with a silicon-specific implementation. This feature would be leveraged by Common code. If all code were in one repository, no barriers would be in place to prevent the contributor from directly calling from Common code into the Silicon implementation. By forcing the API/interface to be published in a separate repository, we can ensure that the unidirectional dependency relationship is maintained. Module A and Module B both provide optional functionality. However, Module A is far more likely to be consume by a wide audience than Module B. To achieve \"Less is More\", Module A may be placed in a different repos to enable downstream consumers to carry as little \"unused\" code as possible, since it's likely they would not need Module B in their code tree. A downstream consumer is producing a product in conjunction with a vendor/partner. While most of the enabling code for the vendor component is open-source, a portion of it is only released under NDA. By having multiple repositories comprise a single workspace, the downstream consumer is able to maximize their open-source consumption (which minimizes forking) while maintaining the legal requirements of closed-source/proprietary partitioning.","title":"Repo Philosophy"},{"location":"Where/external_resources/","text":"External Resources \u00b6 UEFI Industry Organization \u00b6 UEFI is the industry standards body that develops and distributes the UEFI, PI, and ACPI specifications. These specifications govern the firmware interfaces between OS, OEM/Device Manufacturer, and Silicon partner. This is a great site to download the industry specifications and if you are a member you can join working groups for future specifications. TianoCore Project \u00b6 Tianocore is an existing open source project. Their EDK2 repository is the basis for many/most UEFI implementations used on products today. It provides UEFI spec compliant code modules, supports industry standard hardware, and a multi-platform build environment. This is a great site to download specifications for the different file types and build process. It also has links to repositories that Project Mu tracks as \"upstreams\". MkDocs \u00b6 Great tool for creating documentation websites based on markdown. In fact it was used to generate this documentation. Markdown Help \u00b6 Quick link for common markdown support.","title":"External Resources"},{"location":"Where/external_resources/#external-resources","text":"","title":"External Resources"},{"location":"Where/external_resources/#uefi-industry-organization","text":"UEFI is the industry standards body that develops and distributes the UEFI, PI, and ACPI specifications. These specifications govern the firmware interfaces between OS, OEM/Device Manufacturer, and Silicon partner. This is a great site to download the industry specifications and if you are a member you can join working groups for future specifications.","title":"UEFI Industry Organization"},{"location":"Where/external_resources/#tianocore-project","text":"Tianocore is an existing open source project. Their EDK2 repository is the basis for many/most UEFI implementations used on products today. It provides UEFI spec compliant code modules, supports industry standard hardware, and a multi-platform build environment. This is a great site to download specifications for the different file types and build process. It also has links to repositories that Project Mu tracks as \"upstreams\".","title":"TianoCore Project"},{"location":"Where/external_resources/#mkdocs","text":"Great tool for creating documentation websites based on markdown. In fact it was used to generate this documentation.","title":"MkDocs"},{"location":"Where/external_resources/#markdown-help","text":"Quick link for common markdown support.","title":"Markdown Help"},{"location":"Where/project_resources/","text":"Project Resources \u00b6 Public Source Code Repositories \u00b6 Listed here: GitHub Project Mu Repo List Issue/Bug/Feature Tracking \u00b6 https://github.com/Microsoft/mu/issues Builds \u00b6 https://dev.azure.com/projectmu/mu/_build Docs \u00b6 https://microsoft.github.io/mu/ Collaborate \u00b6 Send an email request to join the discussion on our Teams channels. Help \u00b6 For one-off questions, feel free to open an Issue against the Mu repo with the \"question\" tag https://github.com/Microsoft/mu/issues For deeper discussion & faster communication, join our Microsoft Teams channels. To join send an email request .","title":"Project Resources"},{"location":"Where/project_resources/#project-resources","text":"","title":"Project Resources"},{"location":"Where/project_resources/#public-source-code-repositories","text":"Listed here: GitHub Project Mu Repo List","title":"Public Source Code Repositories"},{"location":"Where/project_resources/#issuebugfeature-tracking","text":"https://github.com/Microsoft/mu/issues","title":"Issue/Bug/Feature Tracking"},{"location":"Where/project_resources/#builds","text":"https://dev.azure.com/projectmu/mu/_build","title":"Builds"},{"location":"Where/project_resources/#docs","text":"https://microsoft.github.io/mu/","title":"Docs"},{"location":"Where/project_resources/#collaborate","text":"Send an email request to join the discussion on our Teams channels.","title":"Collaborate"},{"location":"Where/project_resources/#help","text":"For one-off questions, feel free to open an Issue against the Mu repo with the \"question\" tag https://github.com/Microsoft/mu/issues For deeper discussion & faster communication, join our Microsoft Teams channels. To join send an email request .","title":"Help"},{"location":"dyn/mu_basecore/RepoDetails/","text":"Project Mu Basecore Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_basecore.git Branch: release/202005 Commit: 2960583444950ef459ccc2d679f6aea8302275f2 Commit Date: 2020-08-06 21:54:26 +0000 This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this any code within this repository should be one or more of the following Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services More Info \u00b6 Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements Builds \u00b6 pip install mu_build mu_build -c corebuild.mu.json More info Copyright & License \u00b6 Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_basecore/RepoDetails/#project-mu-basecore-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_basecore.git Branch: release/202005 Commit: 2960583444950ef459ccc2d679f6aea8302275f2 Commit Date: 2020-08-06 21:54:26 +0000 This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this any code within this repository should be one or more of the following Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services","title":"Project Mu Basecore Repository"},{"location":"dyn/mu_basecore/RepoDetails/#more-info","text":"Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_basecore/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_basecore/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_basecore/RepoDetails/#builds","text":"pip install mu_build mu_build -c corebuild.mu.json More info","title":"Builds"},{"location":"dyn/mu_basecore/RepoDetails/#copyright-license","text":"Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_basecore/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/","text":"Mu BaseTools Notes \u00b6 This is a set of compiled tools for Edk2 development on x86 for Windows and Linux. This set has both the standard Edk2 tools as well as additional tools created for Project Mu. Where \u00b6 Information about the TianoCore Edk2 Basetools can be found here: * https://tianocore.org * https://github.com/tianocore/edk2 * https://github.com/tianocore/edk2-BaseTools-win32 Information about Project Mu can be found here: * https://microsoft.github.io/mu/ * https://github.com/Microsoft/mu * https://github.com/microsoft/mu_basecore What \u00b6 TianoCore/Project Mu Edk2 Build tools Version \u00b6 BaseTools binaries are versioned based on the Release branch they are associated with (e.g. release/201808, release/201811, etc.). The version format is YYYY.MM.XX where: YYYY is the 4-digit year MM is the 2-digit month XX is a point-release in case fixes are required Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0 Process to publish new version of tool \u00b6 Download desired version from Unzip Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder Run the < TOOL > -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Basetools.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Mu-Basetools"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#mu-basetools-notes","text":"This is a set of compiled tools for Edk2 development on x86 for Windows and Linux. This set has both the standard Edk2 tools as well as additional tools created for Project Mu.","title":"Mu BaseTools Notes"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#where","text":"Information about the TianoCore Edk2 Basetools can be found here: * https://tianocore.org * https://github.com/tianocore/edk2 * https://github.com/tianocore/edk2-BaseTools-win32 Information about Project Mu can be found here: * https://microsoft.github.io/mu/ * https://github.com/Microsoft/mu * https://github.com/microsoft/mu_basecore","title":"Where"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#what","text":"TianoCore/Project Mu Edk2 Build tools","title":"What"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#version","text":"BaseTools binaries are versioned based on the Release branch they are associated with (e.g. release/201808, release/201811, etc.). The version format is YYYY.MM.XX where: YYYY is the 4-digit year MM is the 2-digit month XX is a point-release in case fixes are required Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0","title":"Version"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#process-to-publish-new-version-of-tool","text":"Download desired version from Unzip Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder Run the < TOOL > -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Basetools.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Process to publish new version of tool"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/","text":"Mu-Nasm Notes \u00b6 This tool is the open source NASM assembler. More information can be found at https://nasm.us/ Where \u00b6 Go to https://nasm.us and find the desired download. What \u00b6 nasm.exe is the assembler. Version \u00b6 nasm.exe -v Nuget version is AA.BB.CC The version command generally outputs a version in AA.BB.CC format. Process to publish new version of tool \u00b6 Download desired version from nasm.us (Windows .exe and Linux .rpm) Unzip (unzipping RPM requires 7z) Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder (in this case just nasm and ndisasm) Run the nasm.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Nasm.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Mu-Nasm"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#mu-nasm-notes","text":"This tool is the open source NASM assembler. More information can be found at https://nasm.us/","title":"Mu-Nasm Notes"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#where","text":"Go to https://nasm.us and find the desired download.","title":"Where"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#what","text":"nasm.exe is the assembler.","title":"What"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#version","text":"nasm.exe -v Nuget version is AA.BB.CC The version command generally outputs a version in AA.BB.CC format.","title":"Version"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#process-to-publish-new-version-of-tool","text":"Download desired version from nasm.us (Windows .exe and Linux .rpm) Unzip (unzipping RPM requires 7z) Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder (in this case just nasm and ndisasm) Run the nasm.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Nasm.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Process to publish new version of tool"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/","text":"NugetPublishing \u00b6 Tool to help create and publish nuget packages for Project Mu resources Usage \u00b6 See NugetPublishing.py -h OPTIONAL: host_specific folders \u00b6 The possible different setups for the host are: OS: Linux, Windows, Java Architecture: x86 or ARM Highest Order Bit: 32 or 64 Before the path to the NuGet package contents is published, the Python environment can look inside at several subfolders and decide which one to use based on the Host OS, highest order bit available, and the architecture of the processor. To do so, add \"separated\" to your flags like so: \"flags\": [\"host_specific\"], If this flag is present, the environment will make a list possible subfolders that would be acceptable for the host machine. For this example, a 64 bit Windows machine with an x86 processor was used: Windows-x86-64 Windows-x86 Windows-64 x86-64 Windows x86 64 The environment will look for these folders, following this order, and select the first one it finds. If none are found, the flag will be ignored. Authentication \u00b6 For publishing most service providers require authentication. The --ApiKey parameter allows the caller to supply a unique key for authorization. There are numerous ways to authenticate. For example * Azure Dev Ops: * VSTS credential manager. In an interactive session a dialog will popup for the user to login * Tokens can also be used as the API key. Go to your account page to generate a token that can push packages * NuGet.org * Must use an API key. Go to your account page and generate a key. Example: Creating new config file for first use \u00b6 This will create the config files and place them in the current directory: NugetPublishing.py --Operation New --Name iasl --Author ProjectMu --ConfigFileFolderPath . --Description \"Description of item.\" --FeedUrl https://api.nuget.org/v3/index.json --ProjectUrl http://aka.ms/projectmu --LicenseType BSD2 For help run: NugetPublishing.py --Operation New --help Example: Publishing new version of tool \u00b6 Using an existing config file publish a new iasl.exe. See the example file iasl.config.json 1. Download version from acpica.org 2. Unzip 3. Make a new folder (for my example I will call it \"new\") 4. Copy the assets to publish into this new folder (in this case just iasl.exe) 5. Run the iasl.exe -v command to see the version. 6. Open cmd prompt in the NugetPublishing dir 7. Pack and push (here is my example command. ) NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version 20180209.0.0 --InputFolderPath \"C:\\temp\\iasl-win-20180209\\new\" --ApiKey <your key here>","title":"Read Me"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#nugetpublishing","text":"Tool to help create and publish nuget packages for Project Mu resources","title":"NugetPublishing"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#usage","text":"See NugetPublishing.py -h","title":"Usage"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#optional-host_specific-folders","text":"The possible different setups for the host are: OS: Linux, Windows, Java Architecture: x86 or ARM Highest Order Bit: 32 or 64 Before the path to the NuGet package contents is published, the Python environment can look inside at several subfolders and decide which one to use based on the Host OS, highest order bit available, and the architecture of the processor. To do so, add \"separated\" to your flags like so: \"flags\": [\"host_specific\"], If this flag is present, the environment will make a list possible subfolders that would be acceptable for the host machine. For this example, a 64 bit Windows machine with an x86 processor was used: Windows-x86-64 Windows-x86 Windows-64 x86-64 Windows x86 64 The environment will look for these folders, following this order, and select the first one it finds. If none are found, the flag will be ignored.","title":"OPTIONAL: host_specific folders"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#authentication","text":"For publishing most service providers require authentication. The --ApiKey parameter allows the caller to supply a unique key for authorization. There are numerous ways to authenticate. For example * Azure Dev Ops: * VSTS credential manager. In an interactive session a dialog will popup for the user to login * Tokens can also be used as the API key. Go to your account page to generate a token that can push packages * NuGet.org * Must use an API key. Go to your account page and generate a key.","title":"Authentication"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#example-creating-new-config-file-for-first-use","text":"This will create the config files and place them in the current directory: NugetPublishing.py --Operation New --Name iasl --Author ProjectMu --ConfigFileFolderPath . --Description \"Description of item.\" --FeedUrl https://api.nuget.org/v3/index.json --ProjectUrl http://aka.ms/projectmu --LicenseType BSD2 For help run: NugetPublishing.py --Operation New --help","title":"Example: Creating new config file for first use"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#example-publishing-new-version-of-tool","text":"Using an existing config file publish a new iasl.exe. See the example file iasl.config.json 1. Download version from acpica.org 2. Unzip 3. Make a new folder (for my example I will call it \"new\") 4. Copy the assets to publish into this new folder (in this case just iasl.exe) 5. Run the iasl.exe -v command to see the version. 6. Open cmd prompt in the NugetPublishing dir 7. Pack and push (here is my example command. ) NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version 20180209.0.0 --InputFolderPath \"C:\\temp\\iasl-win-20180209\\new\" --ApiKey <your key here>","title":"Example: Publishing new version of tool"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/","text":"IASL Notes \u00b6 This tool is the open source ACPI compiler. More information can be found at https://acpica.org/ Where \u00b6 For Windows Binary tools: https://acpica.org/downloads/binary-tools What \u00b6 iasl.exe is the compiler. Version \u00b6 iasl.exe -v Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0 Process to publish new version of tool \u00b6 Download desired version from acpica.org Unzip Make a new folder (for my example I will call it \"new\") Copy the assets to publish into this new folder (in this case just iasl.exe) Run the iasl.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"iasl"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#iasl-notes","text":"This tool is the open source ACPI compiler. More information can be found at https://acpica.org/","title":"IASL Notes"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#where","text":"For Windows Binary tools: https://acpica.org/downloads/binary-tools","title":"Where"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#what","text":"iasl.exe is the compiler.","title":"What"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#version","text":"iasl.exe -v Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0","title":"Version"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#process-to-publish-new-version-of-tool","text":"Download desired version from acpica.org Unzip Make a new folder (for my example I will call it \"new\") Copy the assets to publish into this new folder (in this case just iasl.exe) Run the iasl.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Process to publish new version of tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/","text":"Flash Descriptor Size Report Generator Plugin and Command Line Tool \u00b6 Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 FdSizeReportGenerator is a UEFI Build Plugin and Command Line Tool used to parse EDK2 build reports and FDF files and then produce an HTML report of the module sizes and fd sizes. The HTML report then allows deeper analysis of the Flash Usage, the Module Sizes, and overall breakdown of usage. UEFI Build Plugin \u00b6 When used in the plugin capacity this plugin will do its work in the do_post_build function. This plugin uses the following variables from the build environment: BUILDREPORTING - [REQUIRED] - must be True otherwise plugin will not run FLASH_DEFINITION - [REQUIRED] - must point to the platform FDF file BUILDREPORT_FILE - [REQUIRED] - must point to the build report file FDSIZEREPORT_FILE - [OPTIONAL] - should be path for output HTML report. If not set default path will be set based on BUILD_OUTPUT_BASE variable PRODUCT_NAME - [OPTIONAL] - should give friendly product name BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version Command Line Tool \u00b6 When used as a command line tool check the required parameters by using the -h option.","title":"Fd Size Report"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#flash-descriptor-size-report-generator-plugin-and-command-line-tool","text":"","title":"Flash Descriptor Size Report Generator Plugin and Command Line Tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#copyright","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#about","text":"FdSizeReportGenerator is a UEFI Build Plugin and Command Line Tool used to parse EDK2 build reports and FDF files and then produce an HTML report of the module sizes and fd sizes. The HTML report then allows deeper analysis of the Flash Usage, the Module Sizes, and overall breakdown of usage.","title":"About"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#uefi-build-plugin","text":"When used in the plugin capacity this plugin will do its work in the do_post_build function. This plugin uses the following variables from the build environment: BUILDREPORTING - [REQUIRED] - must be True otherwise plugin will not run FLASH_DEFINITION - [REQUIRED] - must point to the platform FDF file BUILDREPORT_FILE - [REQUIRED] - must point to the build report file FDSIZEREPORT_FILE - [OPTIONAL] - should be path for output HTML report. If not set default path will be set based on BUILD_OUTPUT_BASE variable PRODUCT_NAME - [OPTIONAL] - should give friendly product name BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version","title":"UEFI Build Plugin"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#command-line-tool","text":"When used as a command line tool check the required parameters by using the -h option.","title":"Command Line Tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/","text":"Override Validation Plugin \u00b6 Module Level Override Validation Plugin and Linkage Creation Command Line Tool About \u00b6 OverrideValidation is a UEFI Build Plugin and Command Line Tool used to create linkage between overriding and overridden modules and parse INF files referenced in platform DSC files during build process and then produce a TXT report of the module overriding status. The TXT report then allows deeper analysis of the Overriding Hierarchy, the Override Linkage Validity, the Override Linkage Ages, and overall breakdown of usage. UEFI Build Plugin \u00b6 When used in the plugin capacity this plugin will do its override linkage validation work in the do_pre_build function. This plugin uses the following variables from the build environment: ACTIVE_PLATFORM - [REQUIRED] - must be workspace relative or package path relative pointing to the target platform dsc file, otherwise this validation will not run BUILD_OUTPUT_BASE - [REQUIRED] - must be an absolute path specified to store override log at $(BUILD_OUTPUT_BASE)/OVERRIDELOG.TXT, otherwise no report will be generated BUILDSHA - [OPTIONAL] - should have valid commit sha value for report purpose, if not provided, 'None' will be used for the corresponding field PRODUCT_NAME - [OPTIONAL] - should give friendly product name, if not provided, 'None' will be used for the corresponding field BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version, if not provided, 'None' will be used for the corresponding field Command Line Tool \u00b6 When used as a command line tool, this tool takes the absolute path of workspace (the root directory of Devices repo) as well as the absolute path of overridden module's inf file and then generate a screen-print line for users to include in overriding modules in order to create override linkage. Check the required parameters by using the -h option for command line argument details. The override can also be used on the Active Platform DSC or the Flash Definition FDF defined by the DSC. Example \u00b6 Command to generate an override record: OverrideValidation.py -w C:\\Repo -m C:\\Repo\\SM_UDK\\MdePkg\\Library\\BaseMemoryLib\\BaseMemoryLib.inf Override record to be included in overriding module's inf: #Override : 00000001 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 Override log generated during pre-build process: Platform: PlatformName Version: 123.456.7890 Date: 2018-05-11T17-56-27 Commit: _SHA_2c9def7a4ce84ef26ed6597afcc60cee4e5c92c0 State: 3/4 Overrides ---------------------------------------------------------------- OVERRIDER: MdePkg/Library/BaseMemoryLibOptDxe/BaseMemoryLibOptDxe.inf ORIGINALS: + MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | SUCCESS | 2 days OVERRIDER: PlatformNamePkg/Library/NvmConfigLib/NvmConfigLib.inf ORIGINALS: + MdeModulePkg/Bus/Pci/NvmExpressDxe/NvmExpressDxe.inf | MISMATCH | 35 days | Current State: 62929532257365b261080b7e7b1c4e7a | Last Fingerprint: dc9f5e3af1efbac6cf5485b672291903 + MdePkg/Library/BaseMemoryLibOptDxe/BaseMemoryLibOptDxe.inf | SUCCESS | 0 days + MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | SUCCESS | 2 days Versions \u00b6 There are two versions of the override format. Version 1 \u00b6 #Override : 00000001 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 Version 2 \u00b6 #Override : 00000002 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 | 575096df6a Version 2 includes a second hash at the end, which is the git commit that the upstream was last updated. This allows to tools to do a git diff between what you currently have and what is in the tree. It currently only diffs the overridden file (the INF or DSC) and the overriding file. Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Override Validation"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#override-validation-plugin","text":"Module Level Override Validation Plugin and Linkage Creation Command Line Tool","title":"Override Validation Plugin"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#about","text":"OverrideValidation is a UEFI Build Plugin and Command Line Tool used to create linkage between overriding and overridden modules and parse INF files referenced in platform DSC files during build process and then produce a TXT report of the module overriding status. The TXT report then allows deeper analysis of the Overriding Hierarchy, the Override Linkage Validity, the Override Linkage Ages, and overall breakdown of usage.","title":"About"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#uefi-build-plugin","text":"When used in the plugin capacity this plugin will do its override linkage validation work in the do_pre_build function. This plugin uses the following variables from the build environment: ACTIVE_PLATFORM - [REQUIRED] - must be workspace relative or package path relative pointing to the target platform dsc file, otherwise this validation will not run BUILD_OUTPUT_BASE - [REQUIRED] - must be an absolute path specified to store override log at $(BUILD_OUTPUT_BASE)/OVERRIDELOG.TXT, otherwise no report will be generated BUILDSHA - [OPTIONAL] - should have valid commit sha value for report purpose, if not provided, 'None' will be used for the corresponding field PRODUCT_NAME - [OPTIONAL] - should give friendly product name, if not provided, 'None' will be used for the corresponding field BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version, if not provided, 'None' will be used for the corresponding field","title":"UEFI Build Plugin"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#command-line-tool","text":"When used as a command line tool, this tool takes the absolute path of workspace (the root directory of Devices repo) as well as the absolute path of overridden module's inf file and then generate a screen-print line for users to include in overriding modules in order to create override linkage. Check the required parameters by using the -h option for command line argument details. The override can also be used on the Active Platform DSC or the Flash Definition FDF defined by the DSC.","title":"Command Line Tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#example","text":"Command to generate an override record: OverrideValidation.py -w C:\\Repo -m C:\\Repo\\SM_UDK\\MdePkg\\Library\\BaseMemoryLib\\BaseMemoryLib.inf Override record to be included in overriding module's inf: #Override : 00000001 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 Override log generated during pre-build process: Platform: PlatformName Version: 123.456.7890 Date: 2018-05-11T17-56-27 Commit: _SHA_2c9def7a4ce84ef26ed6597afcc60cee4e5c92c0 State: 3/4 Overrides ---------------------------------------------------------------- OVERRIDER: MdePkg/Library/BaseMemoryLibOptDxe/BaseMemoryLibOptDxe.inf ORIGINALS: + MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | SUCCESS | 2 days OVERRIDER: PlatformNamePkg/Library/NvmConfigLib/NvmConfigLib.inf ORIGINALS: + MdeModulePkg/Bus/Pci/NvmExpressDxe/NvmExpressDxe.inf | MISMATCH | 35 days | Current State: 62929532257365b261080b7e7b1c4e7a | Last Fingerprint: dc9f5e3af1efbac6cf5485b672291903 + MdePkg/Library/BaseMemoryLibOptDxe/BaseMemoryLibOptDxe.inf | SUCCESS | 0 days + MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | SUCCESS | 2 days","title":"Example"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#versions","text":"There are two versions of the override format.","title":"Versions"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#version-1","text":"#Override : 00000001 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17","title":"Version 1"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#version-2","text":"#Override : 00000002 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 | 575096df6a Version 2 includes a second hash at the end, which is the git commit that the upstream was last updated. This allows to tools to do a git diff between what you currently have and what is in the tree. It currently only diffs the overridden file (the INF or DSC) and the overriding file.","title":"Version 2"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_basecore/BaseTools/Scripts/PackageDocumentTools/Readme/","text":"Prerequisite Tools: 1. Install Python 2.7.3 from https://www.python.org/download/releases/2.7.3/ 2. Install wxPython 2.8.12.1 from https://sourceforge.net/projects/wxpython/files/wxPython/2.8.12.1/ generally the libraries will be installed at python's subfolder, for example in windows: c:\\python27\\Lib\\site-packages\\ 3. Install DoxyGen 1.8.6 from https://sourceforge.net/projects/doxygen/files/rel-1.8.6/ 4. (Windows only) Install Htmlhelp tool from https://msdn.microsoft.com/en-us/library/windows/desktop/ms669985(v=vs.85).aspx Limitation: 1. Current tool doesn't work on latest wxPython and DoxyGen tool. Please use the sepecific version in above. Run the Tool: a) Run with GUI: 1. Enter src folder, double click \"packagedocapp.pyw\" or run command \"python packagedocapp.pyw\" to open the GUI. 2. Make sure all the information in blank are correct. 3. Click \"Generate Package Document!\" b) Run with command line: 1. Open command line window 2. Enter src folder, for example: \"cd C:\\PackageDocumentTools\\src\" 3. Run \"python packagedoc_cli.py --help\" for detail command.","title":"Package Document Tools"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/","text":"Introduction \u00b6 Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli Build instructions \u00b6 Vcpkg \u00b6 You can download and install brotli using the vcpkg dependency manager: git clone https://github.com/Microsoft/vcpkg.git cd vcpkg ./bootstrap-vcpkg.sh ./vcpkg integrate install vcpkg install brotli The brotli port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please create an issue or pull request on the vcpkg repository. Autotools-style CMake \u00b6 configure-cmake is an autotools-style configure script for CMake-based projects (not supported on Windows). The basic commands to build, test and install brotli are: $ mkdir out && cd out $ ../configure-cmake $ make $ make test $ make install By default, debug binaries are built. To generate \"release\" Makefile specify --disable-debug option to configure-cmake . Bazel \u00b6 See Bazel CMake \u00b6 The basic commands to build and install brotli are: $ mkdir out && cd out $ cmake -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = ./installed .. $ cmake --build . --config Release --target install You can use other CMake configuration. Premake5 \u00b6 See Premake5 Python \u00b6 To install the latest release of the Python module, run the following: $ pip install brotli To install the tip-of-the-tree version, run: $ pip install --upgrade git+https://github.com/google/brotli See the Python readme for more details on installing from source, development, and testing. Benchmarks \u00b6 Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark Related projects \u00b6 Disclaimer: Brotli authors take no responsibility for the third party projects mentioned in this section. Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli Hand ported decoder / encoder in haxe by Dominik Homberger. Output source code: JavaScript, PHP, Python, Java and C# 7Zip plugin Dart native bindings","title":"README"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#introduction","text":"Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli","title":"Introduction"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#build-instructions","text":"","title":"Build instructions"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#vcpkg","text":"You can download and install brotli using the vcpkg dependency manager: git clone https://github.com/Microsoft/vcpkg.git cd vcpkg ./bootstrap-vcpkg.sh ./vcpkg integrate install vcpkg install brotli The brotli port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please create an issue or pull request on the vcpkg repository.","title":"Vcpkg"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#autotools-style-cmake","text":"configure-cmake is an autotools-style configure script for CMake-based projects (not supported on Windows). The basic commands to build, test and install brotli are: $ mkdir out && cd out $ ../configure-cmake $ make $ make test $ make install By default, debug binaries are built. To generate \"release\" Makefile specify --disable-debug option to configure-cmake .","title":"Autotools-style CMake"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#bazel","text":"See Bazel","title":"Bazel"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#cmake","text":"The basic commands to build and install brotli are: $ mkdir out && cd out $ cmake -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = ./installed .. $ cmake --build . --config Release --target install You can use other CMake configuration.","title":"CMake"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#premake5","text":"See Premake5","title":"Premake5"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#python","text":"To install the latest release of the Python module, run the following: $ pip install brotli To install the tip-of-the-tree version, run: $ pip install --upgrade git+https://github.com/google/brotli See the Python readme for more details on installing from source, development, and testing.","title":"Python"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#benchmarks","text":"Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark","title":"Benchmarks"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/#related-projects","text":"Disclaimer: Brotli authors take no responsibility for the third party projects mentioned in this section. Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli Hand ported decoder / encoder in haxe by Dominik Homberger. Output source code: JavaScript, PHP, Python, Java and C# 7Zip plugin Dart native bindings","title":"Related projects"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/CONTRIBUTING/","text":"Want to contribute? Great! First, read this page (including the small print at the end). Before you contribute \u00b6 Before we can use your code, you must sign the [Google Individual Contributor License Agreement] ( https://cla.developers.google.com/about/google-individual ) (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first through the issue tracker with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on. Code reviews \u00b6 All submissions, including submissions by project members, require review. We use Github pull requests for this purpose. The small print \u00b6 Contributions made by corporations are covered by a different agreement than the one above, the [Software Grant and Corporate Contributor License Agreement] ( https://cla.developers.google.com/about/google-corporate ).","title":"CONTRIBUTING"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/CONTRIBUTING/#before-you-contribute","text":"Before we can use your code, you must sign the [Google Individual Contributor License Agreement] ( https://cla.developers.google.com/about/google-individual ) (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first through the issue tracker with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on.","title":"Before you contribute"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/CONTRIBUTING/#code-reviews","text":"All submissions, including submissions by project members, require review. We use Github pull requests for this purpose.","title":"Code reviews"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/CONTRIBUTING/#the-small-print","text":"Contributions made by corporations are covered by a different agreement than the one above, the [Software Grant and Corporate Contributor License Agreement] ( https://cla.developers.google.com/about/google-corporate ).","title":"The small print"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/","text":"brotli(1) -- brotli, unbrotli - compress or decompress files \u00b6 SYNOPSIS \u00b6 brotli [ OPTION|FILE ]... unbrotli is equivalent to brotli --decompress DESCRIPTION \u00b6 brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2-nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. brotli command line syntax similar to gzip (1) and zstd (1) . Unlike gzip (1) , source files are preserved by default. It is possible to remove them after processing by using the --rm option . Arguments that look like \" --name \" or \" --name=value \" are options . Every option has a short form \" -x \" or \" -x value \". Multiple short form options could be coalesced: \" --decompress --stdout --suffix=.b \" works the same as \" -d -s -S .b \" and \" -dsS .b \" brotli has 3 operation modes: default mode is compression; --decompress option activates decompression mode; --test option switches to integrity test mode; this option is equivalent to \" --decompress --stdout \" except that the decompressed data is discarded instead of being written to standard output. Every non-option argument is a file entry. If no files are given or file is \" - \", brotli reads from standard input. All arguments after \" -- \" are file entries. Unless --stdout or --output is specified, files are written to a new file whose name is derived from the source file name: when compressing, a suffix is appended to the source filename to get the target filename when decompressing, a suffix is removed from the source filename to get the target filename Default suffix is .br , but it could be specified with --suffix option. Conflicting or duplicate options are not allowed. OPTIONS \u00b6 -# : compression level (0-9); bigger values cause denser, but slower compression -c , --stdout : write on standard output -d , --decompress : decompress mode -f , --force : force output file overwrite -h , --help : display this help and exit -j , --rm : remove source file(s); gzip (1) -like behaviour -k , --keep : keep source file(s); zstd (1) -like behaviour -n , --no-copy-stat : do not copy source file(s) attributes -o FILE , --output=FILE output file; valid only if there is a single input entry -q NUM , --quality=NUM : compression level (0-11); bigger values cause denser, but slower compression -t , --test : test file integrity mode -v , --verbose : increase output verbosity -w NUM , --lgwin=NUM : set LZ77 window size (0, 10-24) (default: 22); window size is (2**NUM - 16) ; 0 lets compressor decide over the optimal value; bigger windows size improve density; decoder might require up to window size memory to operate -S SUF , --suffix=SUF : output file suffix (default: .br ) -V , --version : display version and exit -Z , --best : use best compression level (default); same as \" -q 11 \" SEE ALSO \u00b6 brotli file format is defined in RFC 7932 . brotli is open-sourced under the MIT License . Mailing list: https://groups.google.com/forum/#!forum/brotli BUGS \u00b6 Report bugs at: https://github.com/google/brotli/issues","title":"tools"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/#brotli1-brotli-unbrotli-compress-or-decompress-files","text":"","title":"brotli(1) -- brotli, unbrotli - compress or decompress files"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/#synopsis","text":"brotli [ OPTION|FILE ]... unbrotli is equivalent to brotli --decompress","title":"SYNOPSIS"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/#description","text":"brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2-nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. brotli command line syntax similar to gzip (1) and zstd (1) . Unlike gzip (1) , source files are preserved by default. It is possible to remove them after processing by using the --rm option . Arguments that look like \" --name \" or \" --name=value \" are options . Every option has a short form \" -x \" or \" -x value \". Multiple short form options could be coalesced: \" --decompress --stdout --suffix=.b \" works the same as \" -d -s -S .b \" and \" -dsS .b \" brotli has 3 operation modes: default mode is compression; --decompress option activates decompression mode; --test option switches to integrity test mode; this option is equivalent to \" --decompress --stdout \" except that the decompressed data is discarded instead of being written to standard output. Every non-option argument is a file entry. If no files are given or file is \" - \", brotli reads from standard input. All arguments after \" -- \" are file entries. Unless --stdout or --output is specified, files are written to a new file whose name is derived from the source file name: when compressing, a suffix is appended to the source filename to get the target filename when decompressing, a suffix is removed from the source filename to get the target filename Default suffix is .br , but it could be specified with --suffix option. Conflicting or duplicate options are not allowed.","title":"DESCRIPTION"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/#options","text":"-# : compression level (0-9); bigger values cause denser, but slower compression -c , --stdout : write on standard output -d , --decompress : decompress mode -f , --force : force output file overwrite -h , --help : display this help and exit -j , --rm : remove source file(s); gzip (1) -like behaviour -k , --keep : keep source file(s); zstd (1) -like behaviour -n , --no-copy-stat : do not copy source file(s) attributes -o FILE , --output=FILE output file; valid only if there is a single input entry -q NUM , --quality=NUM : compression level (0-11); bigger values cause denser, but slower compression -t , --test : test file integrity mode -v , --verbose : increase output verbosity -w NUM , --lgwin=NUM : set LZ77 window size (0, 10-24) (default: 22); window size is (2**NUM - 16) ; 0 lets compressor decide over the optimal value; bigger windows size improve density; decoder might require up to window size memory to operate -S SUF , --suffix=SUF : output file suffix (default: .br ) -V , --version : display version and exit -Z , --best : use best compression level (default); same as \" -q 11 \"","title":"OPTIONS"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/#see-also","text":"brotli file format is defined in RFC 7932 . brotli is open-sourced under the MIT License . Mailing list: https://groups.google.com/forum/#!forum/brotli","title":"SEE ALSO"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/c/tools/brotli/#bugs","text":"Report bugs at: https://github.com/google/brotli/issues","title":"BUGS"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/python/","text":"This directory contains the code for the Python brotli module, bro.py tool, and roundtrip tests. Only Python 2.7+ is supported. We provide a Makefile to simplify common development commands. Installation \u00b6 If you just want to install the latest release of the Python brotli module, we recommend installing from PyPI : $ pip install brotli Alternatively, you may install directly from source by running the following command from this directory: $ make install Development \u00b6 You may run the following commands from this directory: $ make # Build the module in-place $ make test # Test the module $ make clean # Remove all temporary files and build output If you wish to make the module available while still being able to edit the source files, you can use the setuptools \" development mode \": $ make develop # Install the module in \"development mode\" Code Style \u00b6 Brotli's code follows the Google Python Style Guide . To automatically format your code, first install YAPF : $ pip install yapf Then, to format all files in the project, you can run: $ make fix # Automatically format code See the YAPF usage documentation for more information.","title":"python"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/python/#installation","text":"If you just want to install the latest release of the Python brotli module, we recommend installing from PyPI : $ pip install brotli Alternatively, you may install directly from source by running the following command from this directory: $ make install","title":"Installation"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/python/#development","text":"You may run the following commands from this directory: $ make # Build the module in-place $ make test # Test the module $ make clean # Remove all temporary files and build output If you wish to make the module available while still being able to edit the source files, you can use the setuptools \" development mode \": $ make develop # Install the module in \"development mode\"","title":"Development"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/python/#code-style","text":"Brotli's code follows the Google Python Style Guide . To automatically format your code, first install YAPF : $ pip install yapf Then, to format all files in the project, you can run: $ make fix # Automatically format code See the YAPF usage documentation for more information.","title":"Code Style"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/","text":"Introduction \u00b6 In this directory we publish simple tools to analyze backward reference distance distributions in LZ77 compression. We developed these tools to be able to make more efficient encoding of distances in large-window brotli. In large-window compression the average cost of a backward reference distance is higher, and this may allow for more advanced encoding strategies, such as delta coding or an increase in context size, to bring significant compression density improvements. Our tools visualize the backward references as histogram images, i.e., one pixel in the image shows how many distances of a certain range exist at a certain locality in the data. The human visual system is excellent at pattern detection, so we tried to roughly identify patterns visually before going into more quantitative analysis. These tools can turn out to be useful in development of other LZ77-based compressors and we hope you try them out. Tools \u00b6 find_opt_references \u00b6 This tool generates optimal (match-length-wise) backward references for every position in the input files and stores them in *.dist file described below. Example usage: find_opt_references input.txt output.dist draw_histogram \u00b6 This tool generates a visualization of the distribution of backward references stored in *.dist file. The original file size has to be specified as a second parameter. The output is a grayscale PGM (binary) image. Example usage: draw_histogram input.dist 65536 output.pgm Here's an example of resulting image: draw_diff \u00b6 This tool generates a diff PPM (binary) image between two input 8-bit PGM (binary) images. Input images must be of same size. Useful for comparing different backward references distributions for same input file. Normally used for comparison of output images from draw_histogram tool. Example usage: draw_diff image1.pgm image2.pgm diff.ppm For example the diff of this image and this image looks like this: Backward distance file format \u00b6 The format of *.dist files is as follows: [[ 0| match length][ 1|position|distance]...] [1 byte| 4 bytes][1 byte| 4 bytes| 4 bytes] More verbose explanation: for each backward reference there is a position-distance pair, also a copy length may be specified. Copy length is prefixed with flag byte 0, position-distance pair is prefixed with flag byte 1. Each number is a 32-bit integer. Copy length always comes before position-distance pair. Standalone copy length is allowed, in this case it is ignored. Here's an example of how to read from *.dist file: #include \"read_dist.h\" FILE * f ; int copy , pos , dist ; while ( ReadBackwardReference ( fin , & copy , & pos , & dist )) { ... }","title":"README"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/#introduction","text":"In this directory we publish simple tools to analyze backward reference distance distributions in LZ77 compression. We developed these tools to be able to make more efficient encoding of distances in large-window brotli. In large-window compression the average cost of a backward reference distance is higher, and this may allow for more advanced encoding strategies, such as delta coding or an increase in context size, to bring significant compression density improvements. Our tools visualize the backward references as histogram images, i.e., one pixel in the image shows how many distances of a certain range exist at a certain locality in the data. The human visual system is excellent at pattern detection, so we tried to roughly identify patterns visually before going into more quantitative analysis. These tools can turn out to be useful in development of other LZ77-based compressors and we hope you try them out.","title":"Introduction"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/#tools","text":"","title":"Tools"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/#find_opt_references","text":"This tool generates optimal (match-length-wise) backward references for every position in the input files and stores them in *.dist file described below. Example usage: find_opt_references input.txt output.dist","title":"find_opt_references"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/#draw_histogram","text":"This tool generates a visualization of the distribution of backward references stored in *.dist file. The original file size has to be specified as a second parameter. The output is a grayscale PGM (binary) image. Example usage: draw_histogram input.dist 65536 output.pgm Here's an example of resulting image:","title":"draw_histogram"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/#draw_diff","text":"This tool generates a diff PPM (binary) image between two input 8-bit PGM (binary) images. Input images must be of same size. Useful for comparing different backward references distributions for same input file. Normally used for comparison of output images from draw_histogram tool. Example usage: draw_diff image1.pgm image2.pgm diff.ppm For example the diff of this image and this image looks like this:","title":"draw_diff"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/#backward-distance-file-format","text":"The format of *.dist files is as follows: [[ 0| match length][ 1|position|distance]...] [1 byte| 4 bytes][1 byte| 4 bytes| 4 bytes] More verbose explanation: for each backward reference there is a position-distance pair, also a copy length may be specified. Copy length is prefixed with flag byte 0, position-distance pair is prefixed with flag byte 1. Each number is a 32-bit integer. Copy length always comes before position-distance pair. Standalone copy length is allowed, in this case it is ignored. Here's an example of how to read from *.dist file: #include \"read_dist.h\" FILE * f ; int copy , pos , dist ; while ( ReadBackwardReference ( fin , & copy , & pos , & dist )) { ... }","title":"Backward distance file format"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/","text":"libdivsufsort \u00b6 libdivsufsort is a software library that implements a lightweight suffix array construction algorithm. News \u00b6 2015-03-21: The project has moved from Google Code to GitHub Introduction \u00b6 This library provides a simple and an efficient C API to construct a suffix array and a Burrows-Wheeler transformed string from a given string over a constant-size alphabet. The algorithm runs in O(n log n) worst-case time using only 5n+O(1) bytes of memory space, where n is the length of the string. Build requirements \u00b6 An ANSI C Compiler (e.g. GNU GCC) CMake version 2.4.2 or newer CMake-supported build tool Building on GNU/Linux \u00b6 Get the source code from GitHub. You can either use git to clone the repository git clone https://github.com/y-256/libdivsufsort.git or download a zip file directly Create a build directory in the package source directory. $ cd libdivsufsort $ mkdir build $ cd build Configure the package for your system. If you want to install to a different location, change the -DCMAKE_INSTALL_PREFIX option. $ cmake -DCMAKE_BUILD_TYPE = \"Release\" \\ -DCMAKE_INSTALL_PREFIX = \"/usr/local\" .. Compile the package. $ make (Optional) Install the library and header files. $ sudo make install API \u00b6 /* Data types */ typedef int32_t saint_t ; typedef int32_t saidx_t ; typedef uint8_t sauchar_t ; /* * Constructs the suffix array of a given string. * @param T[0..n-1] The input string. * @param SA[0..n-1] The output array or suffixes. * @param n The length of the given string. * @return 0 if no error occurred, -1 or -2 otherwise. */ saint_t divsufsort ( const sauchar_t * T , saidx_t * SA , saidx_t n ); /* * Constructs the burrows-wheeler transformed string of a given string. * @param T[0..n-1] The input string. * @param U[0..n-1] The output string. (can be T) * @param A[0..n-1] The temporary array. (can be NULL) * @param n The length of the given string. * @return The primary index if no error occurred, -1 or -2 otherwise. */ saidx_t divbwt ( const sauchar_t * T , sauchar_t * U , saidx_t * A , saidx_t n ); Example Usage \u00b6 #include <stdio.h> #include <stdlib.h> #include <string.h> #include <divsufsort.h> int main () { // intput data char * Text = \"abracadabra\" ; int n = strlen ( Text ); int i , j ; // allocate int * SA = ( int * ) malloc ( n * sizeof ( int )); // sort divsufsort (( unsigned char * ) Text , SA , n ); // output for ( i = 0 ; i < n ; ++ i ) { printf ( \"SA[%2d] = %2d: \" , i , SA [ i ]); for ( j = SA [ i ]; j < n ; ++ j ) { printf ( \"%c\" , Text [ j ]); } printf ( \"$ \\n \" ); } // deallocate free ( SA ); return 0 ; } See the examples directory for a few other examples. Benchmarks \u00b6 See Benchmarks page for details. License \u00b6 libdivsufsort is released under the MIT license . The MIT License (MIT) Copyright \u00a9 2003 Yuta Mori All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Author \u00b6 Yuta Mori","title":"README"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#libdivsufsort","text":"libdivsufsort is a software library that implements a lightweight suffix array construction algorithm.","title":"libdivsufsort"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#news","text":"2015-03-21: The project has moved from Google Code to GitHub","title":"News"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#introduction","text":"This library provides a simple and an efficient C API to construct a suffix array and a Burrows-Wheeler transformed string from a given string over a constant-size alphabet. The algorithm runs in O(n log n) worst-case time using only 5n+O(1) bytes of memory space, where n is the length of the string.","title":"Introduction"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#build-requirements","text":"An ANSI C Compiler (e.g. GNU GCC) CMake version 2.4.2 or newer CMake-supported build tool","title":"Build requirements"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#building-on-gnulinux","text":"Get the source code from GitHub. You can either use git to clone the repository git clone https://github.com/y-256/libdivsufsort.git or download a zip file directly Create a build directory in the package source directory. $ cd libdivsufsort $ mkdir build $ cd build Configure the package for your system. If you want to install to a different location, change the -DCMAKE_INSTALL_PREFIX option. $ cmake -DCMAKE_BUILD_TYPE = \"Release\" \\ -DCMAKE_INSTALL_PREFIX = \"/usr/local\" .. Compile the package. $ make (Optional) Install the library and header files. $ sudo make install","title":"Building on GNU/Linux"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#api","text":"/* Data types */ typedef int32_t saint_t ; typedef int32_t saidx_t ; typedef uint8_t sauchar_t ; /* * Constructs the suffix array of a given string. * @param T[0..n-1] The input string. * @param SA[0..n-1] The output array or suffixes. * @param n The length of the given string. * @return 0 if no error occurred, -1 or -2 otherwise. */ saint_t divsufsort ( const sauchar_t * T , saidx_t * SA , saidx_t n ); /* * Constructs the burrows-wheeler transformed string of a given string. * @param T[0..n-1] The input string. * @param U[0..n-1] The output string. (can be T) * @param A[0..n-1] The temporary array. (can be NULL) * @param n The length of the given string. * @return The primary index if no error occurred, -1 or -2 otherwise. */ saidx_t divbwt ( const sauchar_t * T , sauchar_t * U , saidx_t * A , saidx_t n );","title":"API"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#example-usage","text":"#include <stdio.h> #include <stdlib.h> #include <string.h> #include <divsufsort.h> int main () { // intput data char * Text = \"abracadabra\" ; int n = strlen ( Text ); int i , j ; // allocate int * SA = ( int * ) malloc ( n * sizeof ( int )); // sort divsufsort (( unsigned char * ) Text , SA , n ); // output for ( i = 0 ; i < n ; ++ i ) { printf ( \"SA[%2d] = %2d: \" , i , SA [ i ]); for ( j = SA [ i ]; j < n ; ++ j ) { printf ( \"%c\" , Text [ j ]); } printf ( \"$ \\n \" ); } // deallocate free ( SA ); return 0 ; } See the examples directory for a few other examples.","title":"Example Usage"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#benchmarks","text":"See Benchmarks page for details.","title":"Benchmarks"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#license","text":"libdivsufsort is released under the MIT license . The MIT License (MIT) Copyright \u00a9 2003 Yuta Mori All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/#author","text":"Yuta Mori","title":"Author"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/","text":"libdivsufsort Change Log \u00b6 See full changelog at: https://github.com/y-256/libdivsufsort/commits 2.0.1 - 2010-11-11 \u00b6 Fixed \u00b6 Wrong variable used in divbwt function Enclose some string variables with double quotation marks in include/CMakeLists.txt Fix typo in include/CMakeLists.txt 2.0.0 - 2008-08-23 \u00b6 Changed \u00b6 Switch the build system to CMake Improve the performance of the suffix-sorting algorithm Added \u00b6 OpenMP support 64-bit version of divsufsort","title":"CHANGELOG"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/#libdivsufsort-change-log","text":"See full changelog at: https://github.com/y-256/libdivsufsort/commits","title":"libdivsufsort Change Log"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/#201-2010-11-11","text":"","title":"2.0.1 - 2010-11-11"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/#fixed","text":"Wrong variable used in divbwt function Enclose some string variables with double quotation marks in include/CMakeLists.txt Fix typo in include/CMakeLists.txt","title":"Fixed"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/#200-2008-08-23","text":"","title":"2.0.0 - 2008-08-23"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/#changed","text":"Switch the build system to CMake Improve the performance of the suffix-sorting algorithm","title":"Changed"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/research/libdivsufsort/CHANGELOG/#added","text":"OpenMP support 64-bit version of divsufsort","title":"Added"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/brotli/scripts/dictionary/","text":"Set of tools that can be used to download brotli RFC, extract and validate binary dictionary, and generate dictionary derivatives (e.g. Java DictionaryData class constants).","title":"dictionary"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/","text":"Step by step to generate sample self-signed X.509 certificate chain and sign data with PKCS7 structure \u00b6 This readme demonstrates how to generate 3-layer X.509 certificate chain (RootCA -> IntermediateCA -> SigningCert) with OpenSSL commands, and user MUST set a UNIQUE Subject Name (\"Common Name\") on these three different certificates. How to generate a self-signed X.509 certificate chain via OPENSSL \u00b6 Set OPENSSL environment. NOTE: Below steps are required for Windows. Linux may already have the OPENSSL environment correctly. set OPENSSL_HOME = c : \\ home \\ openssl \\ openssl -[ version ] set OPENSSL_CONF =% OPENSSL_HOME % \\ apps \\ openssl . cnf When a user uses OpenSSL (req or ca command) to generate the certificates, OpenSSL will use the openssl.cnf file as the configuration data (can use \"-config path/to/openssl.cnf\" to describe the specific config file). The user need check the openssl.cnf file, to find your CA path setting, e.g. check if the path exists in [ CA_default ] section. [ CA_default ] dir = ./demoCA # Where everything is kept You may need the following steps for initialization: rd ./demoCA /S/Q mkdir ./demoCA echo.>./demoCA/index.txt echo 01 > ./demoCA/serial mkdir ./demoCA/newcerts OpenSSL will apply the options from the specified sections in openssl.cnf when creating certificates or certificate signing requests. Make sure your configuration in openssl.cnf is correct and rational for certificate constraints. The following sample sections were used when generating test certificates in this readme. ... [ req ] default_bits = 2048 default_keyfile = privkey.pem distinguished_name = req_distinguished_name attributes = req_attributes x509_extensions = v3_ca # The extensions to add to the self signed cert ... [ v3_ca ] # Extensions for a typical Root CA. subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer basicConstraints = critical,CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ v3_intermediate_ca ] # Extensions for a typical intermediate CA. subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ usr_cert ] # Extensions for user end certificates. basicConstraints = CA:FALSE nsCertType = client, email subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection ... Generate the certificate chain: NOTE: User MUST set a UNIQUE \"Common Name\" on the different certificate 1) Generate the Root Pair: Generate a root key: openssl genrsa -aes256 -out TestRoot.key 2048 Generate a self-signed root certificate: openssl req -extensions v3_ca -new -x509 -days 3650 -key TestRoot.key -out TestRoot.crt openssl x509 -in TestRoot.crt -out TestRoot.cer -outform DER openssl x509 -inform DER -in TestRoot.cer -outform PEM -out TestRoot.pub.pem 2) Generate the Intermediate Pair: Generate the intermediate key: openssl genrsa -aes256 -out TestSub.key 2048 Generate the intermediate certificate: openssl req -new -days 3650 -key TestSub.key -out TestSub.csr openssl ca -extensions v3_intermediate_ca -in TestSub.csr -days 3650 -out TestSub.crt -cert TestRoot.crt -keyfile TestRoot.key openssl x509 -in TestSub.crt -out TestSub.cer -outform DER openssl x509 -inform DER -in TestSub.cer -outform PEM -out TestSub.pub.pem 3) Generate User Key Pair for Data Signing: Generate User key: openssl genrsa -aes256 -out TestCert.key 2048 Generate User certificate: openssl req -new -days 3650 -key TestCert.key -out TestCert.csr openssl ca -extensions usr_cert -in TestCert.csr -days 3650 -out TestCert.crt -cert TestSub.crt -keyfile TestSub.key openssl x509 -in TestCert.crt -out TestCert.cer -outform DER openssl x509 -inform DER -in TestCert.cer -outform PEM -out TestCert.pub.pem Convert Key and Certificate for signing. Password is removed with -nodes flag for convenience in this sample. openssl pkcs12 -export -out TestCert.pfx -inkey TestCert.key -in TestCert.crt openssl pkcs12 -in TestCert.pfx -nodes -out TestCert.pem Verify Data Signing & Verification with new X.509 Certificate Chain 1) Sign a Binary File to generate a detached PKCS7 signature: openssl smime -sign -binary -signer TestCert.pem -outform DER -md sha256 -certfile TestSub.pub.pem -out test.bin.p7 -in test.bin 2) Verify PKCS7 Signature of a Binary File: openssl smime -verify -inform DER -in test.bin.p7 -content test.bin -CAfile TestRoot.pub.pem -out test.org.bin Generate DSC PCD include files for Certificate \u00b6 The BinToPcd utility can be used to convert the binary Certificate file to a text file can be included from a DSC file to set a PCD to the contents of the Certificate file. The following 2 PCDs can be set to the PKCS7 Certificate value. The first one supports a single certificate. The second one supports multiple certificate values using the XDR format. * gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer * gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr Generate DSC PCD include files: BinToPcd.py -i TestRoot.cer -p gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer -o TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc BinToPcd.py -i TestRoot.cer -p gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr -x -o TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc These files can be used in !include statements in DSC file PCD sections. For example: Platform scoped fixed at build PCD section [PcdsFixedAtBuild] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc Platform scoped patchable in module PCD section [PcdsPatchableInModule] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc Module scoped fixed at build PCD section [Components] FmpDevicePkg/FmpDxe/FmpDxe.inf { <PcdsFixedAtBuild> !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc }","title":"Pkcs7Sign"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/#step-by-step-to-generate-sample-self-signed-x509-certificate-chain-and-sign-data-with-pkcs7-structure","text":"This readme demonstrates how to generate 3-layer X.509 certificate chain (RootCA -> IntermediateCA -> SigningCert) with OpenSSL commands, and user MUST set a UNIQUE Subject Name (\"Common Name\") on these three different certificates.","title":"Step by step to generate sample self-signed X.509 certificate chain and sign data with PKCS7 structure"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/#how-to-generate-a-self-signed-x509-certificate-chain-via-openssl","text":"Set OPENSSL environment. NOTE: Below steps are required for Windows. Linux may already have the OPENSSL environment correctly. set OPENSSL_HOME = c : \\ home \\ openssl \\ openssl -[ version ] set OPENSSL_CONF =% OPENSSL_HOME % \\ apps \\ openssl . cnf When a user uses OpenSSL (req or ca command) to generate the certificates, OpenSSL will use the openssl.cnf file as the configuration data (can use \"-config path/to/openssl.cnf\" to describe the specific config file). The user need check the openssl.cnf file, to find your CA path setting, e.g. check if the path exists in [ CA_default ] section. [ CA_default ] dir = ./demoCA # Where everything is kept You may need the following steps for initialization: rd ./demoCA /S/Q mkdir ./demoCA echo.>./demoCA/index.txt echo 01 > ./demoCA/serial mkdir ./demoCA/newcerts OpenSSL will apply the options from the specified sections in openssl.cnf when creating certificates or certificate signing requests. Make sure your configuration in openssl.cnf is correct and rational for certificate constraints. The following sample sections were used when generating test certificates in this readme. ... [ req ] default_bits = 2048 default_keyfile = privkey.pem distinguished_name = req_distinguished_name attributes = req_attributes x509_extensions = v3_ca # The extensions to add to the self signed cert ... [ v3_ca ] # Extensions for a typical Root CA. subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer basicConstraints = critical,CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ v3_intermediate_ca ] # Extensions for a typical intermediate CA. subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ usr_cert ] # Extensions for user end certificates. basicConstraints = CA:FALSE nsCertType = client, email subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection ... Generate the certificate chain: NOTE: User MUST set a UNIQUE \"Common Name\" on the different certificate 1) Generate the Root Pair: Generate a root key: openssl genrsa -aes256 -out TestRoot.key 2048 Generate a self-signed root certificate: openssl req -extensions v3_ca -new -x509 -days 3650 -key TestRoot.key -out TestRoot.crt openssl x509 -in TestRoot.crt -out TestRoot.cer -outform DER openssl x509 -inform DER -in TestRoot.cer -outform PEM -out TestRoot.pub.pem 2) Generate the Intermediate Pair: Generate the intermediate key: openssl genrsa -aes256 -out TestSub.key 2048 Generate the intermediate certificate: openssl req -new -days 3650 -key TestSub.key -out TestSub.csr openssl ca -extensions v3_intermediate_ca -in TestSub.csr -days 3650 -out TestSub.crt -cert TestRoot.crt -keyfile TestRoot.key openssl x509 -in TestSub.crt -out TestSub.cer -outform DER openssl x509 -inform DER -in TestSub.cer -outform PEM -out TestSub.pub.pem 3) Generate User Key Pair for Data Signing: Generate User key: openssl genrsa -aes256 -out TestCert.key 2048 Generate User certificate: openssl req -new -days 3650 -key TestCert.key -out TestCert.csr openssl ca -extensions usr_cert -in TestCert.csr -days 3650 -out TestCert.crt -cert TestSub.crt -keyfile TestSub.key openssl x509 -in TestCert.crt -out TestCert.cer -outform DER openssl x509 -inform DER -in TestCert.cer -outform PEM -out TestCert.pub.pem Convert Key and Certificate for signing. Password is removed with -nodes flag for convenience in this sample. openssl pkcs12 -export -out TestCert.pfx -inkey TestCert.key -in TestCert.crt openssl pkcs12 -in TestCert.pfx -nodes -out TestCert.pem Verify Data Signing & Verification with new X.509 Certificate Chain 1) Sign a Binary File to generate a detached PKCS7 signature: openssl smime -sign -binary -signer TestCert.pem -outform DER -md sha256 -certfile TestSub.pub.pem -out test.bin.p7 -in test.bin 2) Verify PKCS7 Signature of a Binary File: openssl smime -verify -inform DER -in test.bin.p7 -content test.bin -CAfile TestRoot.pub.pem -out test.org.bin","title":"How to generate a self-signed X.509 certificate chain via OPENSSL"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/#generate-dsc-pcd-include-files-for-certificate","text":"The BinToPcd utility can be used to convert the binary Certificate file to a text file can be included from a DSC file to set a PCD to the contents of the Certificate file. The following 2 PCDs can be set to the PKCS7 Certificate value. The first one supports a single certificate. The second one supports multiple certificate values using the XDR format. * gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer * gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr Generate DSC PCD include files: BinToPcd.py -i TestRoot.cer -p gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer -o TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc BinToPcd.py -i TestRoot.cer -p gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr -x -o TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc These files can be used in !include statements in DSC file PCD sections. For example: Platform scoped fixed at build PCD section [PcdsFixedAtBuild] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc Platform scoped patchable in module PCD section [PcdsPatchableInModule] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc Module scoped fixed at build PCD section [Components] FmpDevicePkg/FmpDxe/FmpDxe.inf { <PcdsFixedAtBuild> !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc }","title":"Generate DSC PCD include files for Certificate"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/","text":"Introduction \u00b6 Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli Build instructions \u00b6 Vcpkg \u00b6 You can download and install brotli using the vcpkg dependency manager: git clone https://github.com/Microsoft/vcpkg.git cd vcpkg ./bootstrap-vcpkg.sh ./vcpkg integrate install vcpkg install brotli The brotli port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please create an issue or pull request on the vcpkg repository. Autotools-style CMake \u00b6 configure-cmake is an autotools-style configure script for CMake-based projects (not supported on Windows). The basic commands to build, test and install brotli are: $ mkdir out && cd out $ ../configure-cmake $ make $ make test $ make install By default, debug binaries are built. To generate \"release\" Makefile specify --disable-debug option to configure-cmake . Bazel \u00b6 See Bazel CMake \u00b6 The basic commands to build and install brotli are: $ mkdir out && cd out $ cmake -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = ./installed .. $ cmake --build . --config Release --target install You can use other CMake configuration. Premake5 \u00b6 See Premake5 Python \u00b6 To install the latest release of the Python module, run the following: $ pip install brotli To install the tip-of-the-tree version, run: $ pip install --upgrade git+https://github.com/google/brotli See the Python readme for more details on installing from source, development, and testing. Benchmarks \u00b6 Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark Related projects \u00b6 Disclaimer: Brotli authors take no responsibility for the third party projects mentioned in this section. Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli Hand ported decoder / encoder in haxe by Dominik Homberger. Output source code: JavaScript, PHP, Python, Java and C# 7Zip plugin Dart native bindings","title":"README"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#introduction","text":"Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli","title":"Introduction"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#build-instructions","text":"","title":"Build instructions"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#vcpkg","text":"You can download and install brotli using the vcpkg dependency manager: git clone https://github.com/Microsoft/vcpkg.git cd vcpkg ./bootstrap-vcpkg.sh ./vcpkg integrate install vcpkg install brotli The brotli port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please create an issue or pull request on the vcpkg repository.","title":"Vcpkg"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#autotools-style-cmake","text":"configure-cmake is an autotools-style configure script for CMake-based projects (not supported on Windows). The basic commands to build, test and install brotli are: $ mkdir out && cd out $ ../configure-cmake $ make $ make test $ make install By default, debug binaries are built. To generate \"release\" Makefile specify --disable-debug option to configure-cmake .","title":"Autotools-style CMake"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#bazel","text":"See Bazel","title":"Bazel"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#cmake","text":"The basic commands to build and install brotli are: $ mkdir out && cd out $ cmake -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = ./installed .. $ cmake --build . --config Release --target install You can use other CMake configuration.","title":"CMake"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#premake5","text":"See Premake5","title":"Premake5"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#python","text":"To install the latest release of the Python module, run the following: $ pip install brotli To install the tip-of-the-tree version, run: $ pip install --upgrade git+https://github.com/google/brotli See the Python readme for more details on installing from source, development, and testing.","title":"Python"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#benchmarks","text":"Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark","title":"Benchmarks"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/#related-projects","text":"Disclaimer: Brotli authors take no responsibility for the third party projects mentioned in this section. Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli Hand ported decoder / encoder in haxe by Dominik Homberger. Output source code: JavaScript, PHP, Python, Java and C# 7Zip plugin Dart native bindings","title":"Related projects"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/CONTRIBUTING/","text":"Want to contribute? Great! First, read this page (including the small print at the end). Before you contribute \u00b6 Before we can use your code, you must sign the [Google Individual Contributor License Agreement] ( https://cla.developers.google.com/about/google-individual ) (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first through the issue tracker with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on. Code reviews \u00b6 All submissions, including submissions by project members, require review. We use Github pull requests for this purpose. The small print \u00b6 Contributions made by corporations are covered by a different agreement than the one above, the [Software Grant and Corporate Contributor License Agreement] ( https://cla.developers.google.com/about/google-corporate ).","title":"CONTRIBUTING"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/CONTRIBUTING/#before-you-contribute","text":"Before we can use your code, you must sign the [Google Individual Contributor License Agreement] ( https://cla.developers.google.com/about/google-individual ) (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first through the issue tracker with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on.","title":"Before you contribute"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/CONTRIBUTING/#code-reviews","text":"All submissions, including submissions by project members, require review. We use Github pull requests for this purpose.","title":"Code reviews"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/CONTRIBUTING/#the-small-print","text":"Contributions made by corporations are covered by a different agreement than the one above, the [Software Grant and Corporate Contributor License Agreement] ( https://cla.developers.google.com/about/google-corporate ).","title":"The small print"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/","text":"brotli(1) -- brotli, unbrotli - compress or decompress files \u00b6 SYNOPSIS \u00b6 brotli [ OPTION|FILE ]... unbrotli is equivalent to brotli --decompress DESCRIPTION \u00b6 brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2-nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. brotli command line syntax similar to gzip (1) and zstd (1) . Unlike gzip (1) , source files are preserved by default. It is possible to remove them after processing by using the --rm option . Arguments that look like \" --name \" or \" --name=value \" are options . Every option has a short form \" -x \" or \" -x value \". Multiple short form options could be coalesced: \" --decompress --stdout --suffix=.b \" works the same as \" -d -s -S .b \" and \" -dsS .b \" brotli has 3 operation modes: default mode is compression; --decompress option activates decompression mode; --test option switches to integrity test mode; this option is equivalent to \" --decompress --stdout \" except that the decompressed data is discarded instead of being written to standard output. Every non-option argument is a file entry. If no files are given or file is \" - \", brotli reads from standard input. All arguments after \" -- \" are file entries. Unless --stdout or --output is specified, files are written to a new file whose name is derived from the source file name: when compressing, a suffix is appended to the source filename to get the target filename when decompressing, a suffix is removed from the source filename to get the target filename Default suffix is .br , but it could be specified with --suffix option. Conflicting or duplicate options are not allowed. OPTIONS \u00b6 -# : compression level (0-9); bigger values cause denser, but slower compression -c , --stdout : write on standard output -d , --decompress : decompress mode -f , --force : force output file overwrite -h , --help : display this help and exit -j , --rm : remove source file(s); gzip (1) -like behaviour -k , --keep : keep source file(s); zstd (1) -like behaviour -n , --no-copy-stat : do not copy source file(s) attributes -o FILE , --output=FILE output file; valid only if there is a single input entry -q NUM , --quality=NUM : compression level (0-11); bigger values cause denser, but slower compression -t , --test : test file integrity mode -v , --verbose : increase output verbosity -w NUM , --lgwin=NUM : set LZ77 window size (0, 10-24) (default: 22); window size is (2**NUM - 16) ; 0 lets compressor decide over the optimal value; bigger windows size improve density; decoder might require up to window size memory to operate -S SUF , --suffix=SUF : output file suffix (default: .br ) -V , --version : display version and exit -Z , --best : use best compression level (default); same as \" -q 11 \" SEE ALSO \u00b6 brotli file format is defined in RFC 7932 . brotli is open-sourced under the MIT License . Mailing list: https://groups.google.com/forum/#!forum/brotli BUGS \u00b6 Report bugs at: https://github.com/google/brotli/issues","title":"tools"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/#brotli1-brotli-unbrotli-compress-or-decompress-files","text":"","title":"brotli(1) -- brotli, unbrotli - compress or decompress files"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/#synopsis","text":"brotli [ OPTION|FILE ]... unbrotli is equivalent to brotli --decompress","title":"SYNOPSIS"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/#description","text":"brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2-nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. brotli command line syntax similar to gzip (1) and zstd (1) . Unlike gzip (1) , source files are preserved by default. It is possible to remove them after processing by using the --rm option . Arguments that look like \" --name \" or \" --name=value \" are options . Every option has a short form \" -x \" or \" -x value \". Multiple short form options could be coalesced: \" --decompress --stdout --suffix=.b \" works the same as \" -d -s -S .b \" and \" -dsS .b \" brotli has 3 operation modes: default mode is compression; --decompress option activates decompression mode; --test option switches to integrity test mode; this option is equivalent to \" --decompress --stdout \" except that the decompressed data is discarded instead of being written to standard output. Every non-option argument is a file entry. If no files are given or file is \" - \", brotli reads from standard input. All arguments after \" -- \" are file entries. Unless --stdout or --output is specified, files are written to a new file whose name is derived from the source file name: when compressing, a suffix is appended to the source filename to get the target filename when decompressing, a suffix is removed from the source filename to get the target filename Default suffix is .br , but it could be specified with --suffix option. Conflicting or duplicate options are not allowed.","title":"DESCRIPTION"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/#options","text":"-# : compression level (0-9); bigger values cause denser, but slower compression -c , --stdout : write on standard output -d , --decompress : decompress mode -f , --force : force output file overwrite -h , --help : display this help and exit -j , --rm : remove source file(s); gzip (1) -like behaviour -k , --keep : keep source file(s); zstd (1) -like behaviour -n , --no-copy-stat : do not copy source file(s) attributes -o FILE , --output=FILE output file; valid only if there is a single input entry -q NUM , --quality=NUM : compression level (0-11); bigger values cause denser, but slower compression -t , --test : test file integrity mode -v , --verbose : increase output verbosity -w NUM , --lgwin=NUM : set LZ77 window size (0, 10-24) (default: 22); window size is (2**NUM - 16) ; 0 lets compressor decide over the optimal value; bigger windows size improve density; decoder might require up to window size memory to operate -S SUF , --suffix=SUF : output file suffix (default: .br ) -V , --version : display version and exit -Z , --best : use best compression level (default); same as \" -q 11 \"","title":"OPTIONS"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/#see-also","text":"brotli file format is defined in RFC 7932 . brotli is open-sourced under the MIT License . Mailing list: https://groups.google.com/forum/#!forum/brotli","title":"SEE ALSO"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/c/tools/brotli/#bugs","text":"Report bugs at: https://github.com/google/brotli/issues","title":"BUGS"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/python/","text":"This directory contains the code for the Python brotli module, bro.py tool, and roundtrip tests. Only Python 2.7+ is supported. We provide a Makefile to simplify common development commands. Installation \u00b6 If you just want to install the latest release of the Python brotli module, we recommend installing from PyPI : $ pip install brotli Alternatively, you may install directly from source by running the following command from this directory: $ make install Development \u00b6 You may run the following commands from this directory: $ make # Build the module in-place $ make test # Test the module $ make clean # Remove all temporary files and build output If you wish to make the module available while still being able to edit the source files, you can use the setuptools \" development mode \": $ make develop # Install the module in \"development mode\" Code Style \u00b6 Brotli's code follows the Google Python Style Guide . To automatically format your code, first install YAPF : $ pip install yapf Then, to format all files in the project, you can run: $ make fix # Automatically format code See the YAPF usage documentation for more information.","title":"python"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/python/#installation","text":"If you just want to install the latest release of the Python brotli module, we recommend installing from PyPI : $ pip install brotli Alternatively, you may install directly from source by running the following command from this directory: $ make install","title":"Installation"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/python/#development","text":"You may run the following commands from this directory: $ make # Build the module in-place $ make test # Test the module $ make clean # Remove all temporary files and build output If you wish to make the module available while still being able to edit the source files, you can use the setuptools \" development mode \": $ make develop # Install the module in \"development mode\"","title":"Development"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/python/#code-style","text":"Brotli's code follows the Google Python Style Guide . To automatically format your code, first install YAPF : $ pip install yapf Then, to format all files in the project, you can run: $ make fix # Automatically format code See the YAPF usage documentation for more information.","title":"Code Style"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/","text":"Introduction \u00b6 In this directory we publish simple tools to analyze backward reference distance distributions in LZ77 compression. We developed these tools to be able to make more efficient encoding of distances in large-window brotli. In large-window compression the average cost of a backward reference distance is higher, and this may allow for more advanced encoding strategies, such as delta coding or an increase in context size, to bring significant compression density improvements. Our tools visualize the backward references as histogram images, i.e., one pixel in the image shows how many distances of a certain range exist at a certain locality in the data. The human visual system is excellent at pattern detection, so we tried to roughly identify patterns visually before going into more quantitative analysis. These tools can turn out to be useful in development of other LZ77-based compressors and we hope you try them out. Tools \u00b6 find_opt_references \u00b6 This tool generates optimal (match-length-wise) backward references for every position in the input files and stores them in *.dist file described below. Example usage: find_opt_references input.txt output.dist draw_histogram \u00b6 This tool generates a visualization of the distribution of backward references stored in *.dist file. The original file size has to be specified as a second parameter. The output is a grayscale PGM (binary) image. Example usage: draw_histogram input.dist 65536 output.pgm Here's an example of resulting image: draw_diff \u00b6 This tool generates a diff PPM (binary) image between two input 8-bit PGM (binary) images. Input images must be of same size. Useful for comparing different backward references distributions for same input file. Normally used for comparison of output images from draw_histogram tool. Example usage: draw_diff image1.pgm image2.pgm diff.ppm For example the diff of this image and this image looks like this: Backward distance file format \u00b6 The format of *.dist files is as follows: [[ 0| match length][ 1|position|distance]...] [1 byte| 4 bytes][1 byte| 4 bytes| 4 bytes] More verbose explanation: for each backward reference there is a position-distance pair, also a copy length may be specified. Copy length is prefixed with flag byte 0, position-distance pair is prefixed with flag byte 1. Each number is a 32-bit integer. Copy length always comes before position-distance pair. Standalone copy length is allowed, in this case it is ignored. Here's an example of how to read from *.dist file: #include \"read_dist.h\" FILE * f ; int copy , pos , dist ; while ( ReadBackwardReference ( fin , & copy , & pos , & dist )) { ... }","title":"README"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/#introduction","text":"In this directory we publish simple tools to analyze backward reference distance distributions in LZ77 compression. We developed these tools to be able to make more efficient encoding of distances in large-window brotli. In large-window compression the average cost of a backward reference distance is higher, and this may allow for more advanced encoding strategies, such as delta coding or an increase in context size, to bring significant compression density improvements. Our tools visualize the backward references as histogram images, i.e., one pixel in the image shows how many distances of a certain range exist at a certain locality in the data. The human visual system is excellent at pattern detection, so we tried to roughly identify patterns visually before going into more quantitative analysis. These tools can turn out to be useful in development of other LZ77-based compressors and we hope you try them out.","title":"Introduction"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/#tools","text":"","title":"Tools"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/#find_opt_references","text":"This tool generates optimal (match-length-wise) backward references for every position in the input files and stores them in *.dist file described below. Example usage: find_opt_references input.txt output.dist","title":"find_opt_references"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/#draw_histogram","text":"This tool generates a visualization of the distribution of backward references stored in *.dist file. The original file size has to be specified as a second parameter. The output is a grayscale PGM (binary) image. Example usage: draw_histogram input.dist 65536 output.pgm Here's an example of resulting image:","title":"draw_histogram"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/#draw_diff","text":"This tool generates a diff PPM (binary) image between two input 8-bit PGM (binary) images. Input images must be of same size. Useful for comparing different backward references distributions for same input file. Normally used for comparison of output images from draw_histogram tool. Example usage: draw_diff image1.pgm image2.pgm diff.ppm For example the diff of this image and this image looks like this:","title":"draw_diff"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/#backward-distance-file-format","text":"The format of *.dist files is as follows: [[ 0| match length][ 1|position|distance]...] [1 byte| 4 bytes][1 byte| 4 bytes| 4 bytes] More verbose explanation: for each backward reference there is a position-distance pair, also a copy length may be specified. Copy length is prefixed with flag byte 0, position-distance pair is prefixed with flag byte 1. Each number is a 32-bit integer. Copy length always comes before position-distance pair. Standalone copy length is allowed, in this case it is ignored. Here's an example of how to read from *.dist file: #include \"read_dist.h\" FILE * f ; int copy , pos , dist ; while ( ReadBackwardReference ( fin , & copy , & pos , & dist )) { ... }","title":"Backward distance file format"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/","text":"libdivsufsort \u00b6 libdivsufsort is a software library that implements a lightweight suffix array construction algorithm. News \u00b6 2015-03-21: The project has moved from Google Code to GitHub Introduction \u00b6 This library provides a simple and an efficient C API to construct a suffix array and a Burrows-Wheeler transformed string from a given string over a constant-size alphabet. The algorithm runs in O(n log n) worst-case time using only 5n+O(1) bytes of memory space, where n is the length of the string. Build requirements \u00b6 An ANSI C Compiler (e.g. GNU GCC) CMake version 2.4.2 or newer CMake-supported build tool Building on GNU/Linux \u00b6 Get the source code from GitHub. You can either use git to clone the repository git clone https://github.com/y-256/libdivsufsort.git or download a zip file directly Create a build directory in the package source directory. $ cd libdivsufsort $ mkdir build $ cd build Configure the package for your system. If you want to install to a different location, change the -DCMAKE_INSTALL_PREFIX option. $ cmake -DCMAKE_BUILD_TYPE = \"Release\" \\ -DCMAKE_INSTALL_PREFIX = \"/usr/local\" .. Compile the package. $ make (Optional) Install the library and header files. $ sudo make install API \u00b6 /* Data types */ typedef int32_t saint_t ; typedef int32_t saidx_t ; typedef uint8_t sauchar_t ; /* * Constructs the suffix array of a given string. * @param T[0..n-1] The input string. * @param SA[0..n-1] The output array or suffixes. * @param n The length of the given string. * @return 0 if no error occurred, -1 or -2 otherwise. */ saint_t divsufsort ( const sauchar_t * T , saidx_t * SA , saidx_t n ); /* * Constructs the burrows-wheeler transformed string of a given string. * @param T[0..n-1] The input string. * @param U[0..n-1] The output string. (can be T) * @param A[0..n-1] The temporary array. (can be NULL) * @param n The length of the given string. * @return The primary index if no error occurred, -1 or -2 otherwise. */ saidx_t divbwt ( const sauchar_t * T , sauchar_t * U , saidx_t * A , saidx_t n ); Example Usage \u00b6 #include <stdio.h> #include <stdlib.h> #include <string.h> #include <divsufsort.h> int main () { // intput data char * Text = \"abracadabra\" ; int n = strlen ( Text ); int i , j ; // allocate int * SA = ( int * ) malloc ( n * sizeof ( int )); // sort divsufsort (( unsigned char * ) Text , SA , n ); // output for ( i = 0 ; i < n ; ++ i ) { printf ( \"SA[%2d] = %2d: \" , i , SA [ i ]); for ( j = SA [ i ]; j < n ; ++ j ) { printf ( \"%c\" , Text [ j ]); } printf ( \"$ \\n \" ); } // deallocate free ( SA ); return 0 ; } See the examples directory for a few other examples. Benchmarks \u00b6 See Benchmarks page for details. License \u00b6 libdivsufsort is released under the MIT license . The MIT License (MIT) Copyright \u00a9 2003 Yuta Mori All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Author \u00b6 Yuta Mori","title":"README"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#libdivsufsort","text":"libdivsufsort is a software library that implements a lightweight suffix array construction algorithm.","title":"libdivsufsort"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#news","text":"2015-03-21: The project has moved from Google Code to GitHub","title":"News"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#introduction","text":"This library provides a simple and an efficient C API to construct a suffix array and a Burrows-Wheeler transformed string from a given string over a constant-size alphabet. The algorithm runs in O(n log n) worst-case time using only 5n+O(1) bytes of memory space, where n is the length of the string.","title":"Introduction"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#build-requirements","text":"An ANSI C Compiler (e.g. GNU GCC) CMake version 2.4.2 or newer CMake-supported build tool","title":"Build requirements"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#building-on-gnulinux","text":"Get the source code from GitHub. You can either use git to clone the repository git clone https://github.com/y-256/libdivsufsort.git or download a zip file directly Create a build directory in the package source directory. $ cd libdivsufsort $ mkdir build $ cd build Configure the package for your system. If you want to install to a different location, change the -DCMAKE_INSTALL_PREFIX option. $ cmake -DCMAKE_BUILD_TYPE = \"Release\" \\ -DCMAKE_INSTALL_PREFIX = \"/usr/local\" .. Compile the package. $ make (Optional) Install the library and header files. $ sudo make install","title":"Building on GNU/Linux"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#api","text":"/* Data types */ typedef int32_t saint_t ; typedef int32_t saidx_t ; typedef uint8_t sauchar_t ; /* * Constructs the suffix array of a given string. * @param T[0..n-1] The input string. * @param SA[0..n-1] The output array or suffixes. * @param n The length of the given string. * @return 0 if no error occurred, -1 or -2 otherwise. */ saint_t divsufsort ( const sauchar_t * T , saidx_t * SA , saidx_t n ); /* * Constructs the burrows-wheeler transformed string of a given string. * @param T[0..n-1] The input string. * @param U[0..n-1] The output string. (can be T) * @param A[0..n-1] The temporary array. (can be NULL) * @param n The length of the given string. * @return The primary index if no error occurred, -1 or -2 otherwise. */ saidx_t divbwt ( const sauchar_t * T , sauchar_t * U , saidx_t * A , saidx_t n );","title":"API"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#example-usage","text":"#include <stdio.h> #include <stdlib.h> #include <string.h> #include <divsufsort.h> int main () { // intput data char * Text = \"abracadabra\" ; int n = strlen ( Text ); int i , j ; // allocate int * SA = ( int * ) malloc ( n * sizeof ( int )); // sort divsufsort (( unsigned char * ) Text , SA , n ); // output for ( i = 0 ; i < n ; ++ i ) { printf ( \"SA[%2d] = %2d: \" , i , SA [ i ]); for ( j = SA [ i ]; j < n ; ++ j ) { printf ( \"%c\" , Text [ j ]); } printf ( \"$ \\n \" ); } // deallocate free ( SA ); return 0 ; } See the examples directory for a few other examples.","title":"Example Usage"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#benchmarks","text":"See Benchmarks page for details.","title":"Benchmarks"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#license","text":"libdivsufsort is released under the MIT license . The MIT License (MIT) Copyright \u00a9 2003 Yuta Mori All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/#author","text":"Yuta Mori","title":"Author"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/","text":"libdivsufsort Change Log \u00b6 See full changelog at: https://github.com/y-256/libdivsufsort/commits 2.0.1 - 2010-11-11 \u00b6 Fixed \u00b6 Wrong variable used in divbwt function Enclose some string variables with double quotation marks in include/CMakeLists.txt Fix typo in include/CMakeLists.txt 2.0.0 - 2008-08-23 \u00b6 Changed \u00b6 Switch the build system to CMake Improve the performance of the suffix-sorting algorithm Added \u00b6 OpenMP support 64-bit version of divsufsort","title":"CHANGELOG"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/#libdivsufsort-change-log","text":"See full changelog at: https://github.com/y-256/libdivsufsort/commits","title":"libdivsufsort Change Log"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/#201-2010-11-11","text":"","title":"2.0.1 - 2010-11-11"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/#fixed","text":"Wrong variable used in divbwt function Enclose some string variables with double quotation marks in include/CMakeLists.txt Fix typo in include/CMakeLists.txt","title":"Fixed"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/#200-2008-08-23","text":"","title":"2.0.0 - 2008-08-23"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/#changed","text":"Switch the build system to CMake Improve the performance of the suffix-sorting algorithm","title":"Changed"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/research/libdivsufsort/CHANGELOG/#added","text":"OpenMP support 64-bit version of divsufsort","title":"Added"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/brotli/scripts/dictionary/","text":"Set of tools that can be used to download brotli RFC, extract and validate binary dictionary, and generate dictionary derivatives (e.g. Java DictionaryData class constants).","title":"dictionary"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/","text":"DeviceStateLib \u00b6 About \u00b6 The MsCorePkg provides the necessary functions to store platform specific device states. These device states can then be queried by any element within the boot environment to enable special code paths. In this library implementation a bitmask is stored in a PCD to signify what modes are active. The default bits in the bitmask are set in DeviceStateLib.h - but each platform is expected to implement its own header to define the platform specific device states or to define any of the unused bits: BIT 0: DEVICE_STATE_SECUREBOOT_OFF - UEFI Secure Boot disabled BIT 1: DEVICE_STATE_MANUFACTURING_MODE - Device is in an OEM defined manufacturing mode BIT 2: DEVICE_STATE_DEVELOPMENT_BUILD_ENABLED - Device is a development build. Non-production features might be enabled BIT 3: DEVICE_STATE_SOURCE_DEBUG_ENABLED - Source debug mode is enabled allowing a user to connect and control the device BIT 4: DEVICE_STATE_UNDEFINED - Set by the platform BIT 5: DEVICE_STATE_UNIT_TEST_MODE - Device has a unit test build. Some features are disabled to allow for unit tests in UEFI Shell BIT 24: DEVICE_STATE_PLATFORM_MODE_0 BIT 25: DEVICE_STATE_PLATFORM_MODE_1 BIT 26: DEVICE_STATE_PLATFORM_MODE_2 BIT 27: DEVICE_STATE_PLATFORM_MODE_3 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Device State Lib"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/#devicestatelib","text":"","title":"DeviceStateLib"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/#about","text":"The MsCorePkg provides the necessary functions to store platform specific device states. These device states can then be queried by any element within the boot environment to enable special code paths. In this library implementation a bitmask is stored in a PCD to signify what modes are active. The default bits in the bitmask are set in DeviceStateLib.h - but each platform is expected to implement its own header to define the platform specific device states or to define any of the unused bits: BIT 0: DEVICE_STATE_SECUREBOOT_OFF - UEFI Secure Boot disabled BIT 1: DEVICE_STATE_MANUFACTURING_MODE - Device is in an OEM defined manufacturing mode BIT 2: DEVICE_STATE_DEVELOPMENT_BUILD_ENABLED - Device is a development build. Non-production features might be enabled BIT 3: DEVICE_STATE_SOURCE_DEBUG_ENABLED - Source debug mode is enabled allowing a user to connect and control the device BIT 4: DEVICE_STATE_UNDEFINED - Set by the platform BIT 5: DEVICE_STATE_UNIT_TEST_MODE - Device has a unit test build. Some features are disabled to allow for unit tests in UEFI Shell BIT 24: DEVICE_STATE_PLATFORM_MODE_0 BIT 25: DEVICE_STATE_PLATFORM_MODE_1 BIT 26: DEVICE_STATE_PLATFORM_MODE_2 BIT 27: DEVICE_STATE_PLATFORM_MODE_3","title":"About"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '.' Traceback (most recent call last): File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\macros\\plugin.py\", line 261, in render md_template = self.env.from_string(markdown) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 100, in template jinja2.exceptions.TemplateSyntaxError: unexpected '.'","title":"README"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '.' Traceback (most recent call last): File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\macros\\plugin.py\", line 261, in render md_template = self.env.from_string(markdown) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"c:\\hostedtoolcache\\windows\\python\\3.8.5\\x64\\lib\\site-packages\\jinja2\\_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 100, in template jinja2.exceptions.TemplateSyntaxError: unexpected '.'","title":"Macro Rendering Error"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/","text":"Oniguruma syntax (operator) configuration \u00b6 Documented for Oniguruma 6.9.5 (2020/01/23) Overview \u00b6 This document details how to configure Oniguruma's syntax, by describing the desired syntax operators and behaviors in an instance of the OnigSyntaxType struct, just like the built-in Oniguruma syntaxes do. Configuration operators are bit flags, and are broken into multiple groups, somewhat arbitrarily, because Oniguruma takes its configuration as a trio of 32-bit unsigned int values, assigned as the first three fields in an OnigSyntaxType struct: typedef struct { unsigned int op ; unsigned int op2 ; unsigned int behavior ; OnigOptionType options ; /* default option */ OnigMetaCharTableType meta_char_table ; } OnigSyntaxType ; The first group of configuration flags ( op ) roughly corresponds to the configuration for \"basic regex.\" The second group ( op2 ) roughly corresponds to the configuration for \"advanced regex.\" And the third group ( behavior ) describes more-or-less what to do for broken input, bad input, or other corner-case regular expressions whose meaning is not well-defined. These three groups of flags are described in full below, and tables of their usages for various syntaxes follow. The options field describes the default compile options to use if the caller does not specify any options when invoking onig_new() . The meta_char_table field is used exclusively by the ONIG_SYN_OP_VARIABLE_META_CHARACTERS option, which allows the various regex metacharacters, like * and ? , to be replaced with alternates (for example, SQL typically uses % instead of .* and _ instead of ? ). Group One Flags (op) \u00b6 This group contains \"basic regex\" constructs, features common to most regex systems. 0. ONIG_SYN_OP_VARIABLE_META_CHARACTERS \u00b6 Set in: none Enables support for onig_set_meta_char() , which allows you to provide alternate characters that will be used instead of the six special characters that are normally these characters below: ONIG_META_CHAR_ESCAPE : \\ ONIG_META_CHAR_ANYCHAR : . ONIG_META_CHAR_ANYTIME : * ONIG_META_CHAR_ZERO_OR_ONE_TIME : ? ONIG_META_CHAR_ONE_OR_MORE_TIME : + ONIG_META_CHAR_ANYCHAR_ANYTIME : Equivalent in normal regex to .* , but supported explicitly so that Oniguruma can support matching SQL % wildcards or shell * wildcards. If this flag is set, then the values defined using onig_set_meta_char() will be used; if this flag is clear, then the default regex characters will be used instead, and data set by onig_set_meta_char() will be ignored. 1. ONIG_SYN_OP_DOT_ANYCHAR (enable . ) \u00b6 Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Java, Perl, Perl_NG, Ruby Enables support for the standard . metacharacter, meaning \"any one character.\" You usually want this flag on unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than . instead. 2. ONIG_SYN_OP_ASTERISK_ZERO_INF (enable r* ) \u00b6 Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the standard r* metacharacter, meaning \"zero or more r's.\" You usually want this flag set unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than * instead. 3. ONIG_SYN_OP_ESC_ASTERISK_ZERO_INF (enable r\\* ) \u00b6 Set in: none Enables support for an escaped r\\* metacharacter, meaning \"zero or more r's.\" This is useful if you have disabled support for the normal r* metacharacter because you want * to simply match a literal * character, but you still want some way of activating \"zero or more\" behavior. 4. ONIG_SYN_OP_PLUS_ONE_INF (enable r+ ) \u00b6 Set in: Oniguruma, PosixExtended, Emacs, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the standard r+ metacharacter, meaning \"one or more r's.\" You usually want this flag set unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than + instead. 5. ONIG_SYN_OP_ESC_PLUS_ONE_INF (enable r\\+ ) \u00b6 Set in: Grep Enables support for an escaped r\\+ metacharacter, meaning \"one or more r's.\" This is useful if you have disabled support for the normal r+ metacharacter because you want + to simply match a literal + character, but you still want some way of activating \"one or more\" behavior. 6. ONIG_SYN_OP_QMARK_ZERO_ONE (enable r? ) \u00b6 Set in: Oniguruma, PosixExtended, Emacs, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the standard r? metacharacter, meaning \"zero or one r\" or \"an optional r.\" You usually want this flag set unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than ? instead. 7. ONIG_SYN_OP_ESC_QMARK_ZERO_ONE (enable r\\? ) \u00b6 Set in: Grep Enables support for an escaped r\\? metacharacter, meaning \"zero or one r\" or \"an optional r.\" This is useful if you have disabled support for the normal r? metacharacter because you want ? to simply match a literal ? character, but you still want some way of activating \"optional\" behavior. 8. ONIG_SYN_OP_BRACE_INTERVAL (enable r{l,u} ) \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the r{lower,upper} range form, common to more advanced regex engines, which lets you specify precisely a minimum and maximum range on how many r's must match (and not simply \"zero or more\"). This form also allows r{count} to specify a precise count of r's that must match. This form also allows r{lower,} to be equivalent to r{lower,infinity} . If and only if the ONIG_SYN_ALLOW_INTERVAL_LOW_ABBREV behavior flag is set, this form also allows r{,upper} to be equivalent to r{0,upper} ; otherwise, r{,upper} will be treated as an error. 9. ONIG_SYN_OP_ESC_BRACE_INTERVAL (enable \\{ and \\} ) \u00b6 Set in: PosixBasic, Emacs, Grep Enables support for an escaped r\\{lower,upper\\} range form. This is useful if you have disabled support for the normal r{...} range form and want curly braces to simply match literal curly brace characters, but you still want some way of activating \"range\" behavior. 10. ONIG_SYN_OP_VBAR_ALT (enable r|s ) \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common r|s alternation operator. You usually want this flag set. 11. ONIG_SYN_OP_ESC_VBAR_ALT (enable \\| ) \u00b6 Set in: Emacs, Grep Enables support for an escaped r\\|s alternation form. This is useful if you have disabled support for the normal r|s alternation form and want | to simply match a literal | character, but you still want some way of activating \"alternate\" behavior. 12. ONIG_SYN_OP_LPAREN_SUBEXP (enable (r) ) \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common (...) grouping-and-capturing operators. You usually want this flag set. 13. ONIG_SYN_OP_ESC_LPAREN_SUBEXP (enable \\( and \\) ) \u00b6 Set in: PosixBasic, Emacs, Grep Enables support for escaped \\(...\\) grouping-and-capturing operators. This is useful if you have disabled support for the normal (...) grouping-and-capturing operators and want parentheses to simply match literal parenthesis characters, but you still want some way of activating \"grouping\" or \"capturing\" behavior. 14. ONIG_SYN_OP_ESC_AZ_BUF_ANCHOR (enable \\A and \\Z and \\z ) \u00b6 Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the anchors \\A (start-of-string), \\Z (end-of-string or newline-at-end-of-string), and \\z (end-of-string) escapes. (If the escape metacharacter has been changed from the default of \\ , this option will recognize that metacharacter instead.) 15. ONIG_SYN_OP_ESC_CAPITAL_G_BEGIN_ANCHOR (enable \\G ) \u00b6 Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the special anchor \\G (start-of-previous-match). (If the escape metacharacter has been changed from the default of \\ , this option will recognize that metacharacter instead.) Note that OnigRegex / regex_t are not stateful objects, and do not record the location of the previous match. The \\G flag uses the start parameter explicitly passed to onig_search() (or onig_search_with_param() to determine the \"start of the previous match,\" so if the caller always passes the start of the entire buffer as the function's start parameter, then \\G will behave exactly the same as \\A . 16. ONIG_SYN_OP_DECIMAL_BACKREF (enable \\num ) \u00b6 Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for subsequent matches to back references to prior capture groups (...) using the common \\num syntax (like \\3 ). If this flag is clear, then a numeric escape like \\3 will either be treated as a literal 3 , or, if ONIG_SYN_OP_ESC_OCTAL3 is set, will be treated as an octal character code \\3 . You usually want this enabled, and it is enabled by default in every built-in syntax. 17. ONIG_SYN_OP_BRACKET_CC (enable [...] ) \u00b6 Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for recognizing character classes, like [a-z] . If this flag is not set, [ and ] will be treated as ordinary literal characters instead of as metacharacters. You usually want this enabled, and it is enabled by default in every built-in syntax. 18. ONIG_SYN_OP_ESC_W_WORD (enable \\w and \\W ) \u00b6 Set in: Oniguruma, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\w and \\W shorthand forms. These match \"word characters,\" whose meaning varies depending on the encoding being used. In ASCII encoding, \\w is equivalent to [A-Za-z0-9_] . In most other encodings, \\w matches many more characters, including accented letters, Greek letters, Cyrillic letters, Braille letters and numbers, Runic letters, Hebrew letters, Arabic letters and numerals, Chinese Han ideographs, Japanese Katakana and Hiragana, Korean Hangul, and generally any symbol that could qualify as a phonetic \"letter\" or counting \"number\" in any language. (Note that emoji are not considered \"word characters.\") \\W always matches the opposite of whatever \\w matches. 19. ONIG_SYN_OP_ESC_LTGT_WORD_BEGIN_END (enable \\< and \\> ) \u00b6 Set in: Grep, GnuRegex Enables support for the GNU-specific \\< and \\> word-boundary metacharacters. These work like the \\b word-boundary metacharacter, but only match at one end of the word or the other: \\< only matches at a transition from a non-word character to a word character (i.e., at the start of a word), and \\> only matches at a transition from a word character to a non-word character (i.e., at the end of a word). Most regex syntaxes do not support these metacharacters. 20. ONIG_SYN_OP_ESC_B_WORD_BOUND (enable \\b and \\B ) \u00b6 Set in: Oniguruma, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\b and \\B word-boundary metacharacters. The \\b metacharacter matches a zero-width position at a transition from word-characters to non-word-characters, or vice versa. The \\B metacharacter matches at all positions not matched by \\b . See details in ONIG_SYN_OP_ESC_W_WORD above for an explanation as to which characters are considered \"word characters.\" 21. ONIG_SYN_OP_ESC_S_WHITE_SPACE (enable \\s and \\S ) \u00b6 Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\s and \\S whitespace-matching metacharacters. The \\s metacharacter in ASCII encoding is exactly equivalent to the character class [\\t\\n\\v\\f\\r ] , or characters codes 9 through 13 (inclusive), and 32. The \\s metacharacter in Unicode is exactly equivalent to the character class [\\t\\n\\v\\f\\r \\x85\\xA0\\x1680\\x2000-\\x200A\\x2028-\\x2029\\x202F\\x205F\\x3000] \u2014 that is, it matches the same as ASCII, plus U+0085 (next line), U+00A0 (nonbreaking space), U+1680 (Ogham space mark), U+2000 (en quad) through U+200A (hair space) (this range includes several widths of Unicode spaces), U+2028 (line separator) through U+2029 (paragraph separator), U+202F (narrow no-break space), U+205F (medium mathematical space), and U+3000 (CJK ideographic space). All non-Unicode encodings are handled by converting their code points to the appropriate Unicode-equivalent code points, and then matching according to Unicode rules. \\S always matches any one character that is not in the set matched by \\s . 22. ONIG_SYN_OP_ESC_D_DIGIT (enable \\d and \\D ) \u00b6 Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\d and \\D digit-matching metacharacters. The \\d metacharacter in ASCII encoding is exactly equivalent to the character class [0-9] , or characters codes 48 through 57 (inclusive). The \\d metacharacter in Unicode matches [0-9] , as well as digits in Arabic, Devanagari, Bengali, Laotian, Mongolian, CJK fullwidth numerals, and many more. All non-Unicode encodings are handled by converting their code points to the appropriate Unicode-equivalent code points, and then matching according to Unicode rules. \\D always matches any one character that is not in the set matched by \\d . 23. ONIG_SYN_OP_LINE_ANCHOR (enable ^r and r$ ) \u00b6 Set in: Oniguruma, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common ^ and $ line-anchor metacharacters. In single-line mode, ^ matches the start of the input buffer, and $ matches the end of the input buffer. In multi-line mode, ^ matches if the preceding character is \\n ; and $ matches if the following character is \\n . (Note that Oniguruma does not recognize other newline types: It only matches ^ and $ against \\n : not \\r , not \\r\\n , not the U+2028 line separator, and not any other form.) 24. ONIG_SYN_OP_POSIX_BRACKET (enable POSIX [:xxxx:] ) \u00b6 Set in: Oniguruma, PosixBasic, PosixExtended, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the POSIX [:xxxx:] character classes, like [:alpha:] and [:digit:] . The supported POSIX character classes are alnum , alpha , blank , cntrl , digit , graph , lower , print , punct , space , upper , xdigit , ascii , word . 25. ONIG_SYN_OP_QMARK_NON_GREEDY (enable r?? , r*? , r+? , and r{n,m}? ) \u00b6 Set in: Oniguruma, Perl, Java, Perl_NG, Ruby Enables support for lazy (non-greedy) quantifiers: That is, if you append a ? after another quantifier such as ? , * , + , or {n,m} , Oniguruma will try to match as little as possible instead of as much as possible. 26. ONIG_SYN_OP_ESC_CONTROL_CHARS (enable \\n , \\r , \\t , etc.) \u00b6 Set in: Oniguruma, PosixBasic, PosixExtended, Java, Perl, Perl_NG, Ruby Enables support for C-style control-code escapes, like \\n and \\r . Specifically, this recognizes \\a (7), \\b (8), \\t (9), \\n (10), \\f (12), \\r (13), and \\e (27). If ONIG_SYN_OP2_ESC_V_VTAB is enabled (see below), this also enables support for recognizing \\v as code point 11. 27. ONIG_SYN_OP_ESC_C_CONTROL (enable \\cx control codes) \u00b6 Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for named control-code escapes, like \\cm or \\cM for code-point 13. In this shorthand form, control codes may be specified by \\c (for \"Control\") followed by an alphabetic letter, a-z or A-Z, indicating which code point to represent (1 through 26). So \\cA is code point 1, and \\cZ is code point 26. 28. ONIG_SYN_OP_ESC_OCTAL3 (enable \\OOO octal codes) \u00b6 Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for octal-style escapes of up to three digits, like \\1 for code point 1, and \\177 for code point 127. Octal values greater than 255 will result in an error message. 29. ONIG_SYN_OP_ESC_X_HEX2 (enable \\xHH hex codes) \u00b6 Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for hexadecimal-style escapes of up to two digits, like \\x1 for code point 1, and \\x7F for code point 127. 30. ONIG_SYN_OP_ESC_X_BRACE_HEX8 (enable \\x{7HHHHHHH} hex codes) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for brace-wrapped hexadecimal-style escapes of up to eight digits, like \\x{1} for code point 1, and \\x{FFFE} for code point 65534. 31. ONIG_SYN_OP_ESC_O_BRACE_OCTAL (enable \\o{1OOOOOOOOOO} octal codes) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for brace-wrapped octal-style escapes of up to eleven digits, like \\o{1} for code point 1, and \\o{177776} for code point 65534. (New feature as of Oniguruma 6.3.) Group Two Flags (op2) \u00b6 This group contains support for lesser-known regex syntax constructs. 0. ONIG_SYN_OP2_ESC_CAPITAL_Q_QUOTE (enable \\Q...\\E ) \u00b6 Set in: Java, Perl, Perl_NG Enables support for \"quoted\" parts of a pattern: Between \\Q and \\E , all syntax parsing is turned off, so that metacharacters like * and + will no longer be treated as metacharacters, and instead will be matched as literal * and + , as if they had been escaped with \\* and \\+ . 1. ONIG_SYN_OP2_QMARK_GROUP_EFFECT (enable (?...) ) \u00b6 Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for the fairly-common (?...) grouping operator, which controls precedence but which does not capture its contents. 2. ONIG_SYN_OP2_OPTION_PERL (enable options (?imsx) and (?-imsx) ) \u00b6 Set in: Java, Perl, Perl_NG Enables support of regex options. (i,m,s,x) The supported toggle-able options for this flag are: i - Case-insensitivity m - Multi-line mode ( ^ and $ match at \\n as well as start/end of buffer) s - Single-line mode ( . can match \\n ) x - Extended pattern (free-formatting: whitespace will ignored) 3. ONIG_SYN_OP2_OPTION_RUBY (enable options (?imx) and (?-imx) ) \u00b6 Set in: Oniguruma, Ruby Enables support of regex options. (i,m,x) The supported toggle-able options for this flag are: i - Case-insensitivity m - Multi-line mode ( . can match \\n ) x - Extended pattern (free-formatting: whitespace will ignored) 4. ONIG_SYN_OP2_PLUS_POSSESSIVE_REPEAT (enable r?+ , r*+ , and r++ ) \u00b6 Set in: Oniguruma, Ruby Enables support for the possessive quantifiers ?+ , *+ , and ++ , which work similarly to ? and * and + , respectively, but which do not backtrack after matching: Like the normal greedy quantifiers, they match as much as possible, but they do not attempt to match less than their maximum possible extent if subsequent parts of the pattern fail to match. 5. ONIG_SYN_OP2_PLUS_POSSESSIVE_INTERVAL (enable r{n,m}+ ) \u00b6 Set in: Java Enables support for the possessive quantifier {n,m}+ , which works similarly to {n,m} , but which does not backtrack after matching: Like the normal greedy quantifier, it matches as much as possible, but it do not attempt to match less than its maximum possible extent if subsequent parts of the pattern fail to match. 6. ONIG_SYN_OP2_CCLASS_SET_OP (enable && within [...] ) \u00b6 Set in: Oniguruma, Java, Ruby Enables support for character-class intersection . For example, with this feature enabled, you can write [a-z&&[^aeiou]] to produce a character class of only consonants, or [\\0-\\37&&[^\\n\\r]] to produce a character class of all control codes except newlines. 7. ONIG_SYN_OP2_QMARK_LT_NAMED_GROUP (enable named captures (?<name>...) ) \u00b6 Set in: Oniguruma, Perl_NG, Ruby Enables support for naming capture groups, so that instead of having to refer to captures by position (like \\3 or $3 ), you can refer to them by names (like server and path ). This supports the Perl/Ruby naming syntaxes (?<name>...) and (?'name'...) , but not the Python (?P<name>...) syntax. 8. ONIG_SYN_OP2_ESC_K_NAMED_BACKREF (enable named backreferences \\k<name> ) \u00b6 Set in: Oniguruma, Perl_NG, Ruby Enables support for substituted backreferences by name, not just by position. This supports using \\k'name' in addition to supporting \\k<name> . This also supports an Oniguruma-specific extension that lets you specify the distance of the match, if the capture matched multiple times, by writing \\k<name+n> or \\k<name-n> . 9. ONIG_SYN_OP2_ESC_G_SUBEXP_CALL (enable backreferences \\g<name> and \\g<n> ) \u00b6 Set in: Oniguruma, Perl_NG, Ruby Enables support for substituted backreferences by both name and position using the same syntax. This supports using \\g'name' and \\g'1' in addition to supporting \\g<name> and \\g<1> . 10. ONIG_SYN_OP2_ATMARK_CAPTURE_HISTORY (enable (?@...) and (?@<name>...) ) \u00b6 Set in: none Enables support for capture history , which can answer via the onig_*capture*() functions exactly which captures were matched, how many times, and where in the input they were matched, by placing ?@ in front of the capture. Per Oniguruma's regex syntax documentation (appendix A-5): /(?@a)*/.match(\"aaa\") ==> [<0-1>, <1-2>, <2-3>] This can require substantial memory, is primarily useful for debugging, and is not enabled by default in any syntax. 11. ONIG_SYN_OP2_ESC_CAPITAL_C_BAR_CONTROL (enable \\C-x ) \u00b6 Set in: Oniguruma, Ruby Enables support for Ruby legacy control-code escapes, like \\C-m or \\C-M for code-point 13. In this shorthand form, control codes may be specified by \\C- (for \"Control\") followed by a single character (or equivalent), indicating which code point to represent, based on that character's lowest five bits. So, like \\c , you can represent code-point 10 with \\C-j , but you can also represent it with \\C-* as well. See also ONIG_SYN_OP_ESC_C_CONTROL, which enables the more-common \\cx syntax. 12. ONIG_SYN_OP2_ESC_CAPITAL_M_BAR_META (enable \\M-x ) \u00b6 Set in: Oniguruma, Ruby Enables support for Ruby legacy meta-code escapes. When you write \\M-x , Oniguruma will match an x whose 8 th bit is set (i.e., the character code of x will be or'ed with 0x80 ). So, for example, you can match \\x81 using \\x81 , or you can write \\M-\\1 . This is mostly useful when working with legacy 8-bit character encodings. 13. ONIG_SYN_OP2_ESC_V_VTAB (enable \\v as vertical tab) \u00b6 Set in: Oniguruma, Java, Ruby Enables support for a C-style \\v escape code, meaning \"vertical tab.\" If enabled, \\v will be equivalent to ASCII code point 11. 14. ONIG_SYN_OP2_ESC_U_HEX4 (enable \\uHHHH for Unicode) \u00b6 Set in: Oniguruma, Java, Ruby Enables support for a Java-style \\uHHHH escape code for representing Unicode code-points by number, using up to four hexadecimal digits (up to \\uFFFF ). So, for example, \\u221E will match an infinity symbol, \u221e . For code points larger than four digits, like the emoji \ud83d\udea1 (aerial tramway, or code point U+1F6A1), you must either represent the character directly using an encoding like UTF-8, or you must enable support for ONIG_SYN_OP_ESC_X_BRACE_HEX8 or ONIG_SYN_OP_ESC_O_BRACE_OCTAL, which support more than four digits. (New feature as of Oniguruma 6.7.) 15. ONIG_SYN_OP2_ESC_GNU_BUF_ANCHOR (enable \\` and \\' anchors) \u00b6 Set in: Emacs This flag makes the \\` and \\' escapes function identically to \\A and \\z , respectively (when ONIG_SYN_OP_ESC_AZ_BUF_ANCHOR is enabled). These anchor forms are very obscure, and rarely supported by other regex libraries. 16. ONIG_SYN_OP2_ESC_P_BRACE_CHAR_PROPERTY (enable \\p{...} and \\P{...} ) \u00b6 Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for an alternate syntax for POSIX character classes; instead of writing [:alpha:] when this is enabled, you can instead write \\p{alpha} . See also ONIG_SYN_OP_POSIX_BRACKET for the classic POSIX form. 17. ONIG_SYN_OP2_ESC_P_BRACE_CIRCUMFLEX_NOT (enable \\p{^...} and \\P{^...} ) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for an alternate syntax for POSIX character classes; instead of writing [:^alpha:] when this is enabled, you can instead write \\p{^alpha} . See also ONIG_SYN_OP_POSIX_BRACKET for the classic POSIX form. 18. ONIG_SYN_OP2_CHAR_PROPERTY_PREFIX_IS \u00b6 (not presently used) 19. ONIG_SYN_OP2_ESC_H_XDIGIT (enable \\h and \\H ) \u00b6 Set in: Oniguruma, Ruby Enables support for the Ruby-specific shorthand \\h and \\H metacharacters. Somewhat like \\d matches decimal digits, \\h matches hexadecimal digits \u2014 that is, characters in [0-9a-fA-F] . \\H matches the opposite of whatever \\h matches. 20. ONIG_SYN_OP2_INEFFECTIVE_ESCAPE (disable \\ ) \u00b6 Set in: As-is If set, this disables all escape codes, shorthands, and metacharacters that start with \\ (or whatever the configured escape character is), allowing \\ to be treated as a literal \\ . You usually do not want this flag to be enabled. 21. ONIG_SYN_OP2_QMARK_LPAREN_IF_ELSE (enable (?(...)then|else) ) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for conditional inclusion of subsequent regex patterns based on whether a prior named or numbered capture matched, or based on whether a pattern will match. This supports many different forms, including: (?(<foo>)then|else) - condition based on a capture by name. (?('foo')then|else) - condition based on a capture by name. (?(3)then|else) - condition based on a capture by number. (?(+3)then|else) - forward conditional to a future match, by relative position. (?(-3)then|else) - backward conditional to a prior match, by relative position. (?(foo)then|else) - this matches a pattern foo . (foo is any sub-expression) (New feature as of Oniguruma 6.5.) 22. ONIG_SYN_OP2_ESC_CAPITAL_K_KEEP (enable \\K ) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for \\K , which excludes all content before it from the overall regex match (i.e., capture #0). So, for example, pattern foo\\Kbar would match foobar , but capture #0 would only include bar . (New feature as of Oniguruma 6.5.) 23. ONIG_SYN_OP2_ESC_CAPITAL_R_GENERAL_NEWLINE (enable \\R ) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for \\R , the \"general newline\" shorthand, which matches (\\r\\n|[\\n\\v\\f\\r\\u0085\\u2028\\u2029]) (obviously, the Unicode values are cannot be matched in ASCII encodings). (New feature as of Oniguruma 6.5.) 24. ONIG_SYN_OP2_ESC_CAPITAL_N_O_SUPER_DOT (enable \\N and \\O ) \u00b6 Set in: Oniguruma, Perl, Perl_NG Enables support for \\N and \\O . \\N is \"not a line break,\" which is much like the standard . metacharacter, except that while . can be affected by the single-line setting, \\N always matches exactly one character that is not one of the various line-break characters (like \\n and \\r ). \\O matches exactly one character, regardless of whether single-line or multi-line mode are enabled or disabled. (New feature as of Oniguruma 6.5.) 25. ONIG_SYN_OP2_QMARK_TILDE_ABSENT_GROUP (enable (?~...) ) \u00b6 Set in: Oniguruma, Ruby Enables support for the (?~r) \"absent operator\" syntax, which matches as much as possible as long as the result doesn't match pattern r . This is not the same as negative lookahead or negative lookbehind. Among the most useful examples of this is \\/\\*(?~\\*\\/)\\*\\/ , which matches C-style comments by simply saying \"starts with /*, ends with */, and doesn't contain a */ in between.\" A full explanation of this feature is complicated, but it is useful, and an excellent article about it is available on Medium . (New feature as of Oniguruma 6.5.) 26. ONIG_SYN_OP2_ESC_X_Y_TEXT_SEGMENT (enable \\X and \\Y and \\y ) \u00b6 Set in: Oniguruma, Perl, Perl_NG, Ruby \\X is another variation on . , designed to support Unicode, in that it matches a full grapheme cluster . In Unicode, \u00e0 can be encoded as one code point, U+00E0 , or as two, U+0061 U+0300 . If those are further escaped using UTF-8, the former becomes two bytes, and the latter becomes three. Unfortunately, . would naively match only one or two bytes, depending on the encoding, and would likely incorrectly match anything from just a to a broken half of a code point. \\X is designed to fix this: It matches the full \u00e0 , no matter how \u00e0 is encoded or decomposed. \\y matches a cluster boundary, i.e., a zero-width position between graphemes, somewhat like \\b matches boundaries between words. \\Y matches the opposite of \\y , that is, a zero-width position between code points in the middle of a grapheme. (New feature as of Oniguruma 6.6.) 27. ONIG_SYN_OP2_QMARK_PERL_SUBEXP_CALL (enable (?R) and (?&name) ) \u00b6 Set in: Perl_NG Enables support for substituted backreferences by both name and position using Perl-5-specific syntax. This supports using (?R3) and (?&name) to reference previous (and future) matches, similar to the more-common \\g<3> and \\g<name> backreferences. (New feature as of Oniguruma 6.7.) 28. ONIG_SYN_OP2_QMARK_BRACE_CALLOUT_CONTENTS (enable (?{...}) ) \u00b6 Set in: Oniguruma, Perl, Perl_NG Enables support for Perl-style \"callouts\" \u2014 pattern substitutions that result from invoking a callback method. When (?{foo}) is reached in a pattern, the callback function set in onig_set_progress_callout() will be invoked, and be able to perform custom computation during the pattern match (and during backtracking). Full documentation for this advanced feature can be found in the Oniguruma docs/CALLOUT.md file, with an example in samples/callout.c . (New feature as of Oniguruma 6.8.) 29. ONIG_SYN_OP2_ASTERISK_CALLOUT_NAME (enable (*name) ) \u00b6 Set in: Oniguruma, Perl, Perl_NG Enables support for Perl-style \"callouts\" \u2014 pattern substitutions that result from invoking a callback method. When (*foo) is reached in a pattern, the callback function set in onig_set_callout_of_name() will be invoked, passing the given name foo to it, and it can perform custom computation during the pattern match (and during backtracking). Full documentation for this advanced feature can be found in the Oniguruma docs/CALLOUT.md file, with an example in samples/callout.c . (New feature as of Oniguruma 6.8.) 30. ONIG_SYN_OP2_OPTION_ONIGURUMA (enable options (?imxWSDPy) and (?-imxWDSP) ) \u00b6 Set in: Oniguruma Enables support of regex options. (i,m,x,W,S,D,P,y) (New feature as of Oniguruma 6.9.2) i - Case-insensitivity m - Multi-line mode ( . can match \\n ) x - Extended pattern (free-formatting: whitespace will ignored) W - ASCII only word. D - ASCII only digit. S - ASCII only space. P - ASCII only POSIX properties. (includes W,D,S) Syntax Flags (syn) \u00b6 This group contains rules to handle corner cases and constructs that are errors in some syntaxes but not in others. 0. ONIG_SYN_CONTEXT_INDEP_REPEAT_OPS (independent ? , * , + , {n,m} ) \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby This flag specifies how to handle operators like ? and * when they aren't directly attached to an operand, as in ^* or (*) : Are they an error, are they discarded, or are they taken as literals? If this flag is clear, they are taken as literals; otherwise, the ONIG_SYN_CONTEXT_INVALID_REPEAT_OPS flag determines if they are errors or if they are discarded. 1. ONIG_SYN_CONTEXT_INVALID_REPEAT_OPS (error or ignore independent operators) \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby If ONIG_SYN_CONTEXT_INDEP_REPEAT_OPS is set, this flag controls what happens when independent operators appear in a pattern: If this flag is set, then independent operators produce an error message; if this flag is clear, then independent operators are silently discarded. 2. ONIG_SYN_ALLOW_UNMATCHED_CLOSE_SUBEXP (allow ...)... ) \u00b6 Set in: PosixExtended This flag, if set, causes a ) character without a preceding ( to be treated as a literal ) , equivalent to \\) . If this flag is clear, then an unmatched ) character will produce an error message. 3. ONIG_SYN_ALLOW_INVALID_INTERVAL (allow {??? ) \u00b6 Set in: Oniguruma, GnuRegex, Java, Perl, Perl_NG, Ruby This flag, if set, causes an invalid range, like foo{bar} or foo{} , to be silently discarded, as if foo had been written instead. If clear, an invalid range will produce an error message. 4. ONIG_SYN_ALLOW_INTERVAL_LOW_ABBREV (allow {,n} to mean {0,n} ) \u00b6 Set in: Oniguruma, Ruby If this flag is set, then r{,n} will be treated as equivalent to writing {0,n} . If this flag is clear, then r{,n} will produce an error message. Note that regardless of whether this flag is set or clear, if ONIG_SYN_OP_BRACE_INTERVAL is enabled, then r{n,} will always be legal: This flag only controls the behavior of the opposite form, r{,n} . 5. ONIG_SYN_STRICT_CHECK_BACKREF (error on invalid backrefs) \u00b6 Set in: none If this flag is set, an invalid backref, like \\1 in a pattern with no captures, will produce an error. If this flag is clear, then an invalid backref will be equivalent to the empty string. No built-in syntax has this flag enabled. 6. ONIG_SYN_DIFFERENT_LEN_ALT_LOOK_BEHIND (allow (?<=a|bc) ) \u00b6 Set in: Oniguruma, Java, Ruby If this flag is set, lookbehind patterns with alternate options may have differing lengths among those options. If this flag is clear, lookbehind patterns with options must have each option have identical length to the other options. Oniguruma can handle either form, but not all regex engines can, so for compatibility, Oniguruma allows you to cause regexes for other regex engines to fail if they might depend on this rule. 7. ONIG_SYN_CAPTURE_ONLY_NAMED_GROUP (prefer \\k<name> over \\3 ) \u00b6 Set in: Oniguruma, Perl_NG, Ruby If this flag is set on the syntax and ONIG_OPTION_CAPTURE_GROUP is set when calling Oniguruma, then if a name is used on any capture, all captures must also use names: A single use of a named capture prohibits the use of numbered captures. 8. ONIG_SYN_ALLOW_MULTIPLEX_DEFINITION_NAME (allow (?<x>)...(?<x>) ) \u00b6 Set in: Oniguruma, Perl_NG, Ruby If this flag is set, multiple capture groups may use the same name. If this flag is clear, then reuse of a name will produce an error message. 9. ONIG_SYN_FIXED_INTERVAL_IS_GREEDY_ONLY ( a{n}? is equivalent to (?:a{n})? ) \u00b6 Set in: Oniguruma, Ruby If this flag is set, then intervals of a fixed size will ignore a lazy (non-greedy) ? quantifier and treat it as an optional match (an ordinary r? ), since \"match as little as possible\" is meaningless for a fixed-size interval. If this flag is clear, then r{n}? will mean the same as r{n} , and the useless ? will be discarded. 10. ONIG_SYN_ISOLATED_OPTION_CONTINUE_BRANCH ( ..(?i).. ) \u00b6 Set in: Perl, Perl_NG, Java If this flag is set, then an isolated option doesn't break the branch and affects until the end of the group (or end of the pattern). If this flag is not set, then an isolated option is interpreted as the starting point of a new branch. /a(?i)b|c/ ==> /a(?i:b|c)/ 11. ONIG_SYN_VARIABLE_LEN_LOOK_BEHIND ( (?<=...a+...) ) \u00b6 Set in: Oniguruma, Java If this flag is set, then a variable length expressions are allowed in look-behind. 20. ONIG_SYN_NOT_NEWLINE_IN_NEGATIVE_CC (add \\n to [^...] ) \u00b6 Set in: Grep If this flag is set, all newline characters (like \\n ) will be excluded from a negative character class automatically, as if the pattern had been written as [^...\\n] . If this flag is clear, negative character classes do not automatically exclude newlines, and only exclude those characters and ranges written in them. 21. ONIG_SYN_BACKSLASH_ESCAPE_IN_CC (allow [...\\w...] ) \u00b6 Set in: Oniguruma, GnuRegex, Java, Perl, Perl_NG, Ruby If this flag is set, shorthands like \\w are allowed to describe characters in character classes. If this flag is clear, shorthands like \\w are treated as a redundantly-escaped literal w . 22. ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC (silently discard [z-a] ) \u00b6 Set in: Emacs, Grep If this flag is set, then character ranges like [z-a] that are broken or contain no characters will be silently ignored. If this flag is clear, then broken or empty character ranges will produce an error message. 23. ONIG_SYN_ALLOW_DOUBLE_RANGE_OP_IN_CC (treat [0-9-a] as [0-9\\-a] ) \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby If this flag is set, then a trailing - after a character range will be taken as a literal - , as if it had been escaped as \\- . If this flag is clear, then a trailing - after a character range will produce an error message. 24. ONIG_SYN_WARN_CC_OP_NOT_ESCAPED (warn on [[...] and [-x] ) \u00b6 Set in: Oniguruma, Ruby If this flag is set, Oniguruma will be stricter about warning for bad forms in character classes: [[...] will produce a warning, but [\\[...] will not; [-x] will produce a warning, but [\\-x] will not; [x&&-y] will produce a warning, while [x&&\\-y] will not; and so on. If this flag is clear, all of these warnings will be silently discarded. 25. ONIG_SYN_WARN_REDUNDANT_NESTED_REPEAT (warn on (?:a*)+ ) \u00b6 Set in: Oniguruma, Ruby If this flag is set, Oniguruma will warn about nested repeat operators those have no meaning, like (?:a*)+ . If this flag is clear, Oniguruma will allow the nested repeat operators without warning about them. 26. ONIG_SYN_ALLOW_INVALID_CODE_END_OF_RANGE_IN_CC (allow [a-\\x{7fffffff}]) \u00b6 Set in: Oniguruma If this flag is set, then invalid code points at the end of range in character class are allowed. 31. ONIG_SYN_CONTEXT_INDEP_ANCHORS \u00b6 Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby Not currently used, and does nothing. (But still set in several syntaxes for some reason.) Usage tables \u00b6 These tables show which of the built-in syntaxes use which flags and options, for easy comparison between them. Group One Flags (op) \u00b6 ID Option PosB PosEx Emacs Grep Gnu Java Perl PeNG Ruby Onig 0 ONIG_SYN_OP_VARIABLE_META_CHARACTERS - - - - - - - - - - 1 ONIG_SYN_OP_DOT_ANYCHAR Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 2 ONIG_SYN_OP_ASTERISK_ZERO_INF Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 3 ONIG_SYN_OP_ESC_ASTERISK_ZERO_INF - - - - - - - - - - 4 ONIG_SYN_OP_PLUS_ONE_INF - Yes Yes - Yes Yes Yes Yes Yes Yes 5 ONIG_SYN_OP_ESC_PLUS_ONE_INF - - - Yes - - - - - - 6 ONIG_SYN_OP_QMARK_ZERO_ONE - Yes Yes - Yes Yes Yes Yes Yes Yes 7 ONIG_SYN_OP_ESC_QMARK_ZERO_ONE - - - Yes - - - - - - 8 ONIG_SYN_OP_BRACE_INTERVAL - Yes - - Yes Yes Yes Yes Yes Yes 9 ONIG_SYN_OP_ESC_BRACE_INTERVAL Yes - Yes Yes - - - - - - 10 ONIG_SYN_OP_VBAR_ALT - Yes - - Yes Yes Yes Yes Yes Yes 11 ONIG_SYN_OP_ESC_VBAR_ALT - - Yes Yes - - - - - - 12 ONIG_SYN_OP_LPAREN_SUBEXP - Yes - - Yes Yes Yes Yes Yes Yes 13 ONIG_SYN_OP_ESC_LPAREN_SUBEXP Yes - Yes Yes - - - - - - 14 ONIG_SYN_OP_ESC_AZ_BUF_ANCHOR - - - - Yes Yes Yes Yes Yes Yes 15 ONIG_SYN_OP_ESC_CAPITAL_G_BEGIN_ANCHOR - - - - Yes Yes Yes Yes Yes Yes 16 ONIG_SYN_OP_DECIMAL_BACKREF Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 17 ONIG_SYN_OP_BRACKET_CC Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 18 ONIG_SYN_OP_ESC_W_WORD - - - Yes Yes Yes Yes Yes Yes Yes 19 ONIG_SYN_OP_ESC_LTGT_WORD_BEGIN_END - - - Yes Yes - - - - - 20 ONIG_SYN_OP_ESC_B_WORD_BOUND - - - Yes Yes Yes Yes Yes Yes Yes 21 ONIG_SYN_OP_ESC_S_WHITE_SPACE - - - - Yes Yes Yes Yes Yes Yes 22 ONIG_SYN_OP_ESC_D_DIGIT - - - - Yes Yes Yes Yes Yes Yes 23 ONIG_SYN_OP_LINE_ANCHOR - - Yes Yes Yes Yes Yes Yes Yes Yes 24 ONIG_SYN_OP_POSIX_BRACKET Yes Yes Yes - Yes Yes Yes Yes Yes Yes 25 ONIG_SYN_OP_QMARK_NON_GREEDY - - - - - Yes Yes Yes Yes Yes 26 ONIG_SYN_OP_ESC_CONTROL_CHARS Yes Yes - - - Yes Yes Yes Yes Yes 27 ONIG_SYN_OP_ESC_C_CONTROL - - - - - Yes Yes Yes Yes Yes 28 ONIG_SYN_OP_ESC_OCTAL3 - - - - - Yes Yes Yes Yes Yes 29 ONIG_SYN_OP_ESC_X_HEX2 - - - - - Yes Yes Yes Yes Yes 30 ONIG_SYN_OP_ESC_X_BRACE_HEX8 - - - - - - Yes Yes Yes Yes 31 ONIG_SYN_OP_ESC_O_BRACE_OCTAL - - - - - - Yes Yes Yes Yes Group Two Flags (op2) \u00b6 ID Option PosB PosEx Emacs Grep Gnu Java Perl PeNG Ruby Onig 0 ONIG_SYN_OP2_ESC_CAPITAL_Q_QUOTE - - - - - Yes Yes Yes - - 1 ONIG_SYN_OP2_QMARK_GROUP_EFFECT - - - - - Yes Yes Yes Yes Yes 2 ONIG_SYN_OP2_OPTION_PERL - - - - - Yes Yes Yes - - 3 ONIG_SYN_OP2_OPTION_RUBY - - - - - - - - Yes - 4 ONIG_SYN_OP2_PLUS_POSSESSIVE_REPEAT - - - - - - - - Yes Yes 5 ONIG_SYN_OP2_PLUS_POSSESSIVE_INTERVAL - - - - - Yes - - - - 6 ONIG_SYN_OP2_CCLASS_SET_OP - - - - - - - Yes Yes Yes 7 ONIG_SYN_OP2_QMARK_LT_NAMED_GROUP - - - - - - - Yes Yes Yes 8 ONIG_SYN_OP2_ESC_K_NAMED_BACKREF - - - - - - - Yes Yes Yes 9 ONIG_SYN_OP2_ESC_G_SUBEXP_CALL - - - - - - - Yes Yes Yes 10 ONIG_SYN_OP2_ATMARK_CAPTURE_HISTORY - - - - - - - - - - 11 ONIG_SYN_OP2_ESC_CAPITAL_C_BAR_CONTROL - - - - - - - - Yes Yes 12 ONIG_SYN_OP2_ESC_CAPITAL_M_BAR_META - - - - - - - - Yes Yes 13 ONIG_SYN_OP2_ESC_V_VTAB - - - - - Yes - - Yes Yes 14 ONIG_SYN_OP2_ESC_U_HEX4 - - - - - Yes - - Yes Yes 15 ONIG_SYN_OP2_ESC_GNU_BUF_ANCHOR - - Yes - - - - - - - 16 ONIG_SYN_OP2_ESC_P_BRACE_CHAR_PROPERTY - - - - - Yes Yes Yes Yes Yes 17 ONIG_SYN_OP2_ESC_P_BRACE_CIRCUMFLEX_NOT - - - - - - Yes Yes Yes Yes 18 ONIG_SYN_OP2_CHAR_PROPERTY_PREFIX_IS - - - - - - - - - - 19 ONIG_SYN_OP2_ESC_H_XDIGIT - - - - - - - - Yes Yes 20 ONIG_SYN_OP2_INEFFECTIVE_ESCAPE - - - - - - - - - - 21 ONIG_SYN_OP2_QMARK_LPAREN_IF_ELSE - - - - - - Yes Yes Yes Yes 22 ONIG_SYN_OP2_ESC_CAPITAL_K_KEEP - - - - - - Yes Yes Yes Yes 23 ONIG_SYN_OP2_ESC_CAPITAL_R_GENERAL_NEWLINE - - - - - - Yes Yes Yes Yes 24 ONIG_SYN_OP2_ESC_CAPITAL_N_O_SUPER_DOT - - - - - - Yes Yes - Yes 25 ONIG_SYN_OP2_QMARK_TILDE_ABSENT_GROUP - - - - - - - - Yes Yes 26 ONIG_SYN_OP2_ESC_X_Y_TEXT_SEGMENT - - - - - - Yes Yes Yes Yes 27 ONIG_SYN_OP2_QMARK_PERL_SUBEXP_CALL - - - - - - - Yes - - 28 ONIG_SYN_OP2_QMARK_BRACE_CALLOUT_CONTENTS - - - - - - Yes Yes Yes - 29 ONIG_SYN_OP2_ASTERISK_CALLOUT_NAME - - - - - - Yes Yes Yes - 30 ONIG_SYN_OP2_OPTION_ONIGURUMA - - - - - - - - - Yes Syntax Flags (syn) \u00b6 ID Option PosB PosEx Emacs Grep Gnu Java Perl PeNG Ruby Onig 0 ONIG_SYN_CONTEXT_INDEP_REPEAT_OPS - Yes - - Yes Yes Yes Yes Yes Yes 1 ONIG_SYN_CONTEXT_INVALID_REPEAT_OPS - - - - Yes Yes Yes Yes Yes Yes 2 ONIG_SYN_ALLOW_UNMATCHED_CLOSE_SUBEXP - Yes - - - - - - - - 3 ONIG_SYN_ALLOW_INVALID_INTERVAL - - - - Yes Yes Yes Yes Yes Yes 4 ONIG_SYN_ALLOW_INTERVAL_LOW_ABBREV - - - - - - - - Yes Yes 5 ONIG_SYN_STRICT_CHECK_BACKREF - - - - - - - - - - 6 ONIG_SYN_DIFFERENT_LEN_ALT_LOOK_BEHIND - - - - - Yes - - Yes Yes 7 ONIG_SYN_CAPTURE_ONLY_NAMED_GROUP - - - - - - - Yes Yes Yes 8 ONIG_SYN_ALLOW_MULTIPLEX_DEFINITION_NAME - - - - - - - Yes Yes Yes 9 ONIG_SYN_FIXED_INTERVAL_IS_GREEDY_ONLY - - - - - - - - Yes Yes 10 ONIG_SYN_ISOLATED_OPTION_CONTINUE_BRANCH - - - - - Yes Yes Yes - - 11 ONIG_SYN_VARIABLE_LEN_LOOK_BEHIND - - - - - Yes - - - Yes 20 ONIG_SYN_NOT_NEWLINE_IN_NEGATIVE_CC - - - Yes - - - - - - 21 ONIG_SYN_BACKSLASH_ESCAPE_IN_CC - - - - Yes Yes Yes Yes Yes Yes 22 ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC - - Yes Yes - - - - - - 23 ONIG_SYN_ALLOW_DOUBLE_RANGE_OP_IN_CC - Yes - - Yes Yes Yes Yes Yes Yes 24 ONIG_SYN_WARN_CC_OP_NOT_ESCAPED - - - - - - - - Yes Yes 25 ONIG_SYN_WARN_REDUNDANT_NESTED_REPEAT - - - - - - - - Yes Yes 26 ONIG_SYN_ALLOW_INVALID_CODE_END_OF_RANGE_IN_CC - - - - - - - - - Yes 31 ONIG_SYN_CONTEXT_INDEP_ANCHORS - Yes - - Yes Yes Yes Yes Yes Yes","title":"doc"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#oniguruma-syntax-operator-configuration","text":"Documented for Oniguruma 6.9.5 (2020/01/23)","title":"Oniguruma syntax (operator) configuration"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#overview","text":"This document details how to configure Oniguruma's syntax, by describing the desired syntax operators and behaviors in an instance of the OnigSyntaxType struct, just like the built-in Oniguruma syntaxes do. Configuration operators are bit flags, and are broken into multiple groups, somewhat arbitrarily, because Oniguruma takes its configuration as a trio of 32-bit unsigned int values, assigned as the first three fields in an OnigSyntaxType struct: typedef struct { unsigned int op ; unsigned int op2 ; unsigned int behavior ; OnigOptionType options ; /* default option */ OnigMetaCharTableType meta_char_table ; } OnigSyntaxType ; The first group of configuration flags ( op ) roughly corresponds to the configuration for \"basic regex.\" The second group ( op2 ) roughly corresponds to the configuration for \"advanced regex.\" And the third group ( behavior ) describes more-or-less what to do for broken input, bad input, or other corner-case regular expressions whose meaning is not well-defined. These three groups of flags are described in full below, and tables of their usages for various syntaxes follow. The options field describes the default compile options to use if the caller does not specify any options when invoking onig_new() . The meta_char_table field is used exclusively by the ONIG_SYN_OP_VARIABLE_META_CHARACTERS option, which allows the various regex metacharacters, like * and ? , to be replaced with alternates (for example, SQL typically uses % instead of .* and _ instead of ? ).","title":"Overview"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#group-one-flags-op","text":"This group contains \"basic regex\" constructs, features common to most regex systems.","title":"Group One Flags (op)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#0-onig_syn_op_variable_meta_characters","text":"Set in: none Enables support for onig_set_meta_char() , which allows you to provide alternate characters that will be used instead of the six special characters that are normally these characters below: ONIG_META_CHAR_ESCAPE : \\ ONIG_META_CHAR_ANYCHAR : . ONIG_META_CHAR_ANYTIME : * ONIG_META_CHAR_ZERO_OR_ONE_TIME : ? ONIG_META_CHAR_ONE_OR_MORE_TIME : + ONIG_META_CHAR_ANYCHAR_ANYTIME : Equivalent in normal regex to .* , but supported explicitly so that Oniguruma can support matching SQL % wildcards or shell * wildcards. If this flag is set, then the values defined using onig_set_meta_char() will be used; if this flag is clear, then the default regex characters will be used instead, and data set by onig_set_meta_char() will be ignored.","title":"0. ONIG_SYN_OP_VARIABLE_META_CHARACTERS"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#1-onig_syn_op_dot_anychar-enable","text":"Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Java, Perl, Perl_NG, Ruby Enables support for the standard . metacharacter, meaning \"any one character.\" You usually want this flag on unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than . instead.","title":"1. ONIG_SYN_OP_DOT_ANYCHAR (enable .)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#2-onig_syn_op_asterisk_zero_inf-enable-r","text":"Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the standard r* metacharacter, meaning \"zero or more r's.\" You usually want this flag set unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than * instead.","title":"2. ONIG_SYN_OP_ASTERISK_ZERO_INF (enable r*)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#3-onig_syn_op_esc_asterisk_zero_inf-enable-r","text":"Set in: none Enables support for an escaped r\\* metacharacter, meaning \"zero or more r's.\" This is useful if you have disabled support for the normal r* metacharacter because you want * to simply match a literal * character, but you still want some way of activating \"zero or more\" behavior.","title":"3. ONIG_SYN_OP_ESC_ASTERISK_ZERO_INF (enable r\\*)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#4-onig_syn_op_plus_one_inf-enable-r","text":"Set in: Oniguruma, PosixExtended, Emacs, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the standard r+ metacharacter, meaning \"one or more r's.\" You usually want this flag set unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than + instead.","title":"4. ONIG_SYN_OP_PLUS_ONE_INF (enable r+)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#5-onig_syn_op_esc_plus_one_inf-enable-r","text":"Set in: Grep Enables support for an escaped r\\+ metacharacter, meaning \"one or more r's.\" This is useful if you have disabled support for the normal r+ metacharacter because you want + to simply match a literal + character, but you still want some way of activating \"one or more\" behavior.","title":"5. ONIG_SYN_OP_ESC_PLUS_ONE_INF (enable r\\+)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#6-onig_syn_op_qmark_zero_one-enable-r","text":"Set in: Oniguruma, PosixExtended, Emacs, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the standard r? metacharacter, meaning \"zero or one r\" or \"an optional r.\" You usually want this flag set unless you have turned on ONIG_SYN_OP_VARIABLE_META_CHARACTERS so that you can use a metacharacter other than ? instead.","title":"6. ONIG_SYN_OP_QMARK_ZERO_ONE (enable r?)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#7-onig_syn_op_esc_qmark_zero_one-enable-r","text":"Set in: Grep Enables support for an escaped r\\? metacharacter, meaning \"zero or one r\" or \"an optional r.\" This is useful if you have disabled support for the normal r? metacharacter because you want ? to simply match a literal ? character, but you still want some way of activating \"optional\" behavior.","title":"7. ONIG_SYN_OP_ESC_QMARK_ZERO_ONE (enable r\\?)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#8-onig_syn_op_brace_interval-enable-rlu","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the r{lower,upper} range form, common to more advanced regex engines, which lets you specify precisely a minimum and maximum range on how many r's must match (and not simply \"zero or more\"). This form also allows r{count} to specify a precise count of r's that must match. This form also allows r{lower,} to be equivalent to r{lower,infinity} . If and only if the ONIG_SYN_ALLOW_INTERVAL_LOW_ABBREV behavior flag is set, this form also allows r{,upper} to be equivalent to r{0,upper} ; otherwise, r{,upper} will be treated as an error.","title":"8. ONIG_SYN_OP_BRACE_INTERVAL (enable r{l,u})"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#9-onig_syn_op_esc_brace_interval-enable-and","text":"Set in: PosixBasic, Emacs, Grep Enables support for an escaped r\\{lower,upper\\} range form. This is useful if you have disabled support for the normal r{...} range form and want curly braces to simply match literal curly brace characters, but you still want some way of activating \"range\" behavior.","title":"9. ONIG_SYN_OP_ESC_BRACE_INTERVAL (enable \\{ and \\})"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#10-onig_syn_op_vbar_alt-enable-rs","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common r|s alternation operator. You usually want this flag set.","title":"10. ONIG_SYN_OP_VBAR_ALT (enable r|s)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#11-onig_syn_op_esc_vbar_alt-enable","text":"Set in: Emacs, Grep Enables support for an escaped r\\|s alternation form. This is useful if you have disabled support for the normal r|s alternation form and want | to simply match a literal | character, but you still want some way of activating \"alternate\" behavior.","title":"11. ONIG_SYN_OP_ESC_VBAR_ALT (enable \\|)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#12-onig_syn_op_lparen_subexp-enable-r","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common (...) grouping-and-capturing operators. You usually want this flag set.","title":"12. ONIG_SYN_OP_LPAREN_SUBEXP (enable (r))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#13-onig_syn_op_esc_lparen_subexp-enable-and","text":"Set in: PosixBasic, Emacs, Grep Enables support for escaped \\(...\\) grouping-and-capturing operators. This is useful if you have disabled support for the normal (...) grouping-and-capturing operators and want parentheses to simply match literal parenthesis characters, but you still want some way of activating \"grouping\" or \"capturing\" behavior.","title":"13. ONIG_SYN_OP_ESC_LPAREN_SUBEXP (enable \\( and \\))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#14-onig_syn_op_esc_az_buf_anchor-enable-a-and-z-and-z","text":"Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the anchors \\A (start-of-string), \\Z (end-of-string or newline-at-end-of-string), and \\z (end-of-string) escapes. (If the escape metacharacter has been changed from the default of \\ , this option will recognize that metacharacter instead.)","title":"14. ONIG_SYN_OP_ESC_AZ_BUF_ANCHOR (enable \\A and \\Z and \\z)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#15-onig_syn_op_esc_capital_g_begin_anchor-enable-g","text":"Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the special anchor \\G (start-of-previous-match). (If the escape metacharacter has been changed from the default of \\ , this option will recognize that metacharacter instead.) Note that OnigRegex / regex_t are not stateful objects, and do not record the location of the previous match. The \\G flag uses the start parameter explicitly passed to onig_search() (or onig_search_with_param() to determine the \"start of the previous match,\" so if the caller always passes the start of the entire buffer as the function's start parameter, then \\G will behave exactly the same as \\A .","title":"15. ONIG_SYN_OP_ESC_CAPITAL_G_BEGIN_ANCHOR (enable \\G)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#16-onig_syn_op_decimal_backref-enable-num","text":"Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for subsequent matches to back references to prior capture groups (...) using the common \\num syntax (like \\3 ). If this flag is clear, then a numeric escape like \\3 will either be treated as a literal 3 , or, if ONIG_SYN_OP_ESC_OCTAL3 is set, will be treated as an octal character code \\3 . You usually want this enabled, and it is enabled by default in every built-in syntax.","title":"16. ONIG_SYN_OP_DECIMAL_BACKREF (enable \\num)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#17-onig_syn_op_bracket_cc-enable","text":"Set in: Oniguruma, PosixBasic, PosixExtended, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for recognizing character classes, like [a-z] . If this flag is not set, [ and ] will be treated as ordinary literal characters instead of as metacharacters. You usually want this enabled, and it is enabled by default in every built-in syntax.","title":"17. ONIG_SYN_OP_BRACKET_CC (enable [...])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#18-onig_syn_op_esc_w_word-enable-w-and-w","text":"Set in: Oniguruma, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\w and \\W shorthand forms. These match \"word characters,\" whose meaning varies depending on the encoding being used. In ASCII encoding, \\w is equivalent to [A-Za-z0-9_] . In most other encodings, \\w matches many more characters, including accented letters, Greek letters, Cyrillic letters, Braille letters and numbers, Runic letters, Hebrew letters, Arabic letters and numerals, Chinese Han ideographs, Japanese Katakana and Hiragana, Korean Hangul, and generally any symbol that could qualify as a phonetic \"letter\" or counting \"number\" in any language. (Note that emoji are not considered \"word characters.\") \\W always matches the opposite of whatever \\w matches.","title":"18. ONIG_SYN_OP_ESC_W_WORD (enable \\w and \\W)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#19-onig_syn_op_esc_ltgt_word_begin_end-enable-and","text":"Set in: Grep, GnuRegex Enables support for the GNU-specific \\< and \\> word-boundary metacharacters. These work like the \\b word-boundary metacharacter, but only match at one end of the word or the other: \\< only matches at a transition from a non-word character to a word character (i.e., at the start of a word), and \\> only matches at a transition from a word character to a non-word character (i.e., at the end of a word). Most regex syntaxes do not support these metacharacters.","title":"19. ONIG_SYN_OP_ESC_LTGT_WORD_BEGIN_END (enable \\&lt; and \\&gt;)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#20-onig_syn_op_esc_b_word_bound-enable-b-and-b","text":"Set in: Oniguruma, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\b and \\B word-boundary metacharacters. The \\b metacharacter matches a zero-width position at a transition from word-characters to non-word-characters, or vice versa. The \\B metacharacter matches at all positions not matched by \\b . See details in ONIG_SYN_OP_ESC_W_WORD above for an explanation as to which characters are considered \"word characters.\"","title":"20. ONIG_SYN_OP_ESC_B_WORD_BOUND (enable \\b and \\B)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#21-onig_syn_op_esc_s_white_space-enable-s-and-s","text":"Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\s and \\S whitespace-matching metacharacters. The \\s metacharacter in ASCII encoding is exactly equivalent to the character class [\\t\\n\\v\\f\\r ] , or characters codes 9 through 13 (inclusive), and 32. The \\s metacharacter in Unicode is exactly equivalent to the character class [\\t\\n\\v\\f\\r \\x85\\xA0\\x1680\\x2000-\\x200A\\x2028-\\x2029\\x202F\\x205F\\x3000] \u2014 that is, it matches the same as ASCII, plus U+0085 (next line), U+00A0 (nonbreaking space), U+1680 (Ogham space mark), U+2000 (en quad) through U+200A (hair space) (this range includes several widths of Unicode spaces), U+2028 (line separator) through U+2029 (paragraph separator), U+202F (narrow no-break space), U+205F (medium mathematical space), and U+3000 (CJK ideographic space). All non-Unicode encodings are handled by converting their code points to the appropriate Unicode-equivalent code points, and then matching according to Unicode rules. \\S always matches any one character that is not in the set matched by \\s .","title":"21. ONIG_SYN_OP_ESC_S_WHITE_SPACE (enable \\s and \\S)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#22-onig_syn_op_esc_d_digit-enable-d-and-d","text":"Set in: Oniguruma, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common \\d and \\D digit-matching metacharacters. The \\d metacharacter in ASCII encoding is exactly equivalent to the character class [0-9] , or characters codes 48 through 57 (inclusive). The \\d metacharacter in Unicode matches [0-9] , as well as digits in Arabic, Devanagari, Bengali, Laotian, Mongolian, CJK fullwidth numerals, and many more. All non-Unicode encodings are handled by converting their code points to the appropriate Unicode-equivalent code points, and then matching according to Unicode rules. \\D always matches any one character that is not in the set matched by \\d .","title":"22. ONIG_SYN_OP_ESC_D_DIGIT (enable \\d and \\D)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#23-onig_syn_op_line_anchor-enable-r-and-r","text":"Set in: Oniguruma, Emacs, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the common ^ and $ line-anchor metacharacters. In single-line mode, ^ matches the start of the input buffer, and $ matches the end of the input buffer. In multi-line mode, ^ matches if the preceding character is \\n ; and $ matches if the following character is \\n . (Note that Oniguruma does not recognize other newline types: It only matches ^ and $ against \\n : not \\r , not \\r\\n , not the U+2028 line separator, and not any other form.)","title":"23. ONIG_SYN_OP_LINE_ANCHOR (enable ^r and r$)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#24-onig_syn_op_posix_bracket-enable-posix-xxxx","text":"Set in: Oniguruma, PosixBasic, PosixExtended, Grep, GnuRegex, Perl, Java, Perl_NG, Ruby Enables support for the POSIX [:xxxx:] character classes, like [:alpha:] and [:digit:] . The supported POSIX character classes are alnum , alpha , blank , cntrl , digit , graph , lower , print , punct , space , upper , xdigit , ascii , word .","title":"24. ONIG_SYN_OP_POSIX_BRACKET (enable POSIX [:xxxx:])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#25-onig_syn_op_qmark_non_greedy-enable-r-r-r-and-rnm","text":"Set in: Oniguruma, Perl, Java, Perl_NG, Ruby Enables support for lazy (non-greedy) quantifiers: That is, if you append a ? after another quantifier such as ? , * , + , or {n,m} , Oniguruma will try to match as little as possible instead of as much as possible.","title":"25. ONIG_SYN_OP_QMARK_NON_GREEDY (enable r??, r*?, r+?, and r{n,m}?)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#26-onig_syn_op_esc_control_chars-enable-n-r-t-etc","text":"Set in: Oniguruma, PosixBasic, PosixExtended, Java, Perl, Perl_NG, Ruby Enables support for C-style control-code escapes, like \\n and \\r . Specifically, this recognizes \\a (7), \\b (8), \\t (9), \\n (10), \\f (12), \\r (13), and \\e (27). If ONIG_SYN_OP2_ESC_V_VTAB is enabled (see below), this also enables support for recognizing \\v as code point 11.","title":"26. ONIG_SYN_OP_ESC_CONTROL_CHARS (enable \\n, \\r, \\t, etc.)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#27-onig_syn_op_esc_c_control-enable-cx-control-codes","text":"Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for named control-code escapes, like \\cm or \\cM for code-point 13. In this shorthand form, control codes may be specified by \\c (for \"Control\") followed by an alphabetic letter, a-z or A-Z, indicating which code point to represent (1 through 26). So \\cA is code point 1, and \\cZ is code point 26.","title":"27. ONIG_SYN_OP_ESC_C_CONTROL (enable \\cx control codes)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#28-onig_syn_op_esc_octal3-enable-ooo-octal-codes","text":"Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for octal-style escapes of up to three digits, like \\1 for code point 1, and \\177 for code point 127. Octal values greater than 255 will result in an error message.","title":"28. ONIG_SYN_OP_ESC_OCTAL3 (enable \\OOO octal codes)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#29-onig_syn_op_esc_x_hex2-enable-xhh-hex-codes","text":"Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for hexadecimal-style escapes of up to two digits, like \\x1 for code point 1, and \\x7F for code point 127.","title":"29. ONIG_SYN_OP_ESC_X_HEX2 (enable \\xHH hex codes)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#30-onig_syn_op_esc_x_brace_hex8-enable-x7hhhhhhh-hex-codes","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for brace-wrapped hexadecimal-style escapes of up to eight digits, like \\x{1} for code point 1, and \\x{FFFE} for code point 65534.","title":"30. ONIG_SYN_OP_ESC_X_BRACE_HEX8 (enable \\x{7HHHHHHH} hex codes)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#31-onig_syn_op_esc_o_brace_octal-enable-o1oooooooooo-octal-codes","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for brace-wrapped octal-style escapes of up to eleven digits, like \\o{1} for code point 1, and \\o{177776} for code point 65534. (New feature as of Oniguruma 6.3.)","title":"31. ONIG_SYN_OP_ESC_O_BRACE_OCTAL (enable \\o{1OOOOOOOOOO} octal codes)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#group-two-flags-op2","text":"This group contains support for lesser-known regex syntax constructs.","title":"Group Two Flags (op2)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#0-onig_syn_op2_esc_capital_q_quote-enable-qe","text":"Set in: Java, Perl, Perl_NG Enables support for \"quoted\" parts of a pattern: Between \\Q and \\E , all syntax parsing is turned off, so that metacharacters like * and + will no longer be treated as metacharacters, and instead will be matched as literal * and + , as if they had been escaped with \\* and \\+ .","title":"0. ONIG_SYN_OP2_ESC_CAPITAL_Q_QUOTE (enable \\Q...\\E)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#1-onig_syn_op2_qmark_group_effect-enable","text":"Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for the fairly-common (?...) grouping operator, which controls precedence but which does not capture its contents.","title":"1. ONIG_SYN_OP2_QMARK_GROUP_EFFECT (enable (?...))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#2-onig_syn_op2_option_perl-enable-options-imsx-and-imsx","text":"Set in: Java, Perl, Perl_NG Enables support of regex options. (i,m,s,x) The supported toggle-able options for this flag are: i - Case-insensitivity m - Multi-line mode ( ^ and $ match at \\n as well as start/end of buffer) s - Single-line mode ( . can match \\n ) x - Extended pattern (free-formatting: whitespace will ignored)","title":"2. ONIG_SYN_OP2_OPTION_PERL (enable options (?imsx) and (?-imsx))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#3-onig_syn_op2_option_ruby-enable-options-imx-and-imx","text":"Set in: Oniguruma, Ruby Enables support of regex options. (i,m,x) The supported toggle-able options for this flag are: i - Case-insensitivity m - Multi-line mode ( . can match \\n ) x - Extended pattern (free-formatting: whitespace will ignored)","title":"3. ONIG_SYN_OP2_OPTION_RUBY (enable options (?imx) and (?-imx))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#4-onig_syn_op2_plus_possessive_repeat-enable-r-r-and-r","text":"Set in: Oniguruma, Ruby Enables support for the possessive quantifiers ?+ , *+ , and ++ , which work similarly to ? and * and + , respectively, but which do not backtrack after matching: Like the normal greedy quantifiers, they match as much as possible, but they do not attempt to match less than their maximum possible extent if subsequent parts of the pattern fail to match.","title":"4. ONIG_SYN_OP2_PLUS_POSSESSIVE_REPEAT (enable r?+, r*+, and r++)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#5-onig_syn_op2_plus_possessive_interval-enable-rnm","text":"Set in: Java Enables support for the possessive quantifier {n,m}+ , which works similarly to {n,m} , but which does not backtrack after matching: Like the normal greedy quantifier, it matches as much as possible, but it do not attempt to match less than its maximum possible extent if subsequent parts of the pattern fail to match.","title":"5. ONIG_SYN_OP2_PLUS_POSSESSIVE_INTERVAL (enable r{n,m}+)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#6-onig_syn_op2_cclass_set_op-enable-within","text":"Set in: Oniguruma, Java, Ruby Enables support for character-class intersection . For example, with this feature enabled, you can write [a-z&&[^aeiou]] to produce a character class of only consonants, or [\\0-\\37&&[^\\n\\r]] to produce a character class of all control codes except newlines.","title":"6. ONIG_SYN_OP2_CCLASS_SET_OP (enable &amp;&amp; within [...])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#7-onig_syn_op2_qmark_lt_named_group-enable-named-captures-name","text":"Set in: Oniguruma, Perl_NG, Ruby Enables support for naming capture groups, so that instead of having to refer to captures by position (like \\3 or $3 ), you can refer to them by names (like server and path ). This supports the Perl/Ruby naming syntaxes (?<name>...) and (?'name'...) , but not the Python (?P<name>...) syntax.","title":"7. ONIG_SYN_OP2_QMARK_LT_NAMED_GROUP (enable named captures (?&lt;name&gt;...))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#8-onig_syn_op2_esc_k_named_backref-enable-named-backreferences-kname","text":"Set in: Oniguruma, Perl_NG, Ruby Enables support for substituted backreferences by name, not just by position. This supports using \\k'name' in addition to supporting \\k<name> . This also supports an Oniguruma-specific extension that lets you specify the distance of the match, if the capture matched multiple times, by writing \\k<name+n> or \\k<name-n> .","title":"8. ONIG_SYN_OP2_ESC_K_NAMED_BACKREF (enable named backreferences \\k&lt;name&gt;)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#9-onig_syn_op2_esc_g_subexp_call-enable-backreferences-gname-and-gn","text":"Set in: Oniguruma, Perl_NG, Ruby Enables support for substituted backreferences by both name and position using the same syntax. This supports using \\g'name' and \\g'1' in addition to supporting \\g<name> and \\g<1> .","title":"9. ONIG_SYN_OP2_ESC_G_SUBEXP_CALL (enable backreferences \\g&lt;name&gt; and \\g&lt;n&gt;)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#10-onig_syn_op2_atmark_capture_history-enable-and-name","text":"Set in: none Enables support for capture history , which can answer via the onig_*capture*() functions exactly which captures were matched, how many times, and where in the input they were matched, by placing ?@ in front of the capture. Per Oniguruma's regex syntax documentation (appendix A-5): /(?@a)*/.match(\"aaa\") ==> [<0-1>, <1-2>, <2-3>] This can require substantial memory, is primarily useful for debugging, and is not enabled by default in any syntax.","title":"10. ONIG_SYN_OP2_ATMARK_CAPTURE_HISTORY (enable (?@...) and (?@&lt;name&gt;...))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#11-onig_syn_op2_esc_capital_c_bar_control-enable-c-x","text":"Set in: Oniguruma, Ruby Enables support for Ruby legacy control-code escapes, like \\C-m or \\C-M for code-point 13. In this shorthand form, control codes may be specified by \\C- (for \"Control\") followed by a single character (or equivalent), indicating which code point to represent, based on that character's lowest five bits. So, like \\c , you can represent code-point 10 with \\C-j , but you can also represent it with \\C-* as well. See also ONIG_SYN_OP_ESC_C_CONTROL, which enables the more-common \\cx syntax.","title":"11. ONIG_SYN_OP2_ESC_CAPITAL_C_BAR_CONTROL (enable \\C-x)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#12-onig_syn_op2_esc_capital_m_bar_meta-enable-m-x","text":"Set in: Oniguruma, Ruby Enables support for Ruby legacy meta-code escapes. When you write \\M-x , Oniguruma will match an x whose 8 th bit is set (i.e., the character code of x will be or'ed with 0x80 ). So, for example, you can match \\x81 using \\x81 , or you can write \\M-\\1 . This is mostly useful when working with legacy 8-bit character encodings.","title":"12. ONIG_SYN_OP2_ESC_CAPITAL_M_BAR_META (enable \\M-x)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#13-onig_syn_op2_esc_v_vtab-enable-v-as-vertical-tab","text":"Set in: Oniguruma, Java, Ruby Enables support for a C-style \\v escape code, meaning \"vertical tab.\" If enabled, \\v will be equivalent to ASCII code point 11.","title":"13. ONIG_SYN_OP2_ESC_V_VTAB (enable \\v as vertical tab)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#14-onig_syn_op2_esc_u_hex4-enable-uhhhh-for-unicode","text":"Set in: Oniguruma, Java, Ruby Enables support for a Java-style \\uHHHH escape code for representing Unicode code-points by number, using up to four hexadecimal digits (up to \\uFFFF ). So, for example, \\u221E will match an infinity symbol, \u221e . For code points larger than four digits, like the emoji \ud83d\udea1 (aerial tramway, or code point U+1F6A1), you must either represent the character directly using an encoding like UTF-8, or you must enable support for ONIG_SYN_OP_ESC_X_BRACE_HEX8 or ONIG_SYN_OP_ESC_O_BRACE_OCTAL, which support more than four digits. (New feature as of Oniguruma 6.7.)","title":"14. ONIG_SYN_OP2_ESC_U_HEX4 (enable \\uHHHH for Unicode)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#15-onig_syn_op2_esc_gnu_buf_anchor-enable-and-anchors","text":"Set in: Emacs This flag makes the \\` and \\' escapes function identically to \\A and \\z , respectively (when ONIG_SYN_OP_ESC_AZ_BUF_ANCHOR is enabled). These anchor forms are very obscure, and rarely supported by other regex libraries.","title":"15. ONIG_SYN_OP2_ESC_GNU_BUF_ANCHOR (enable \\` and \\' anchors)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#16-onig_syn_op2_esc_p_brace_char_property-enable-p-and-p","text":"Set in: Oniguruma, Java, Perl, Perl_NG, Ruby Enables support for an alternate syntax for POSIX character classes; instead of writing [:alpha:] when this is enabled, you can instead write \\p{alpha} . See also ONIG_SYN_OP_POSIX_BRACKET for the classic POSIX form.","title":"16. ONIG_SYN_OP2_ESC_P_BRACE_CHAR_PROPERTY (enable \\p{...} and \\P{...})"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#17-onig_syn_op2_esc_p_brace_circumflex_not-enable-p-and-p","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for an alternate syntax for POSIX character classes; instead of writing [:^alpha:] when this is enabled, you can instead write \\p{^alpha} . See also ONIG_SYN_OP_POSIX_BRACKET for the classic POSIX form.","title":"17. ONIG_SYN_OP2_ESC_P_BRACE_CIRCUMFLEX_NOT (enable \\p{^...} and \\P{^...})"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#18-onig_syn_op2_char_property_prefix_is","text":"(not presently used)","title":"18. ONIG_SYN_OP2_CHAR_PROPERTY_PREFIX_IS"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#19-onig_syn_op2_esc_h_xdigit-enable-h-and-h","text":"Set in: Oniguruma, Ruby Enables support for the Ruby-specific shorthand \\h and \\H metacharacters. Somewhat like \\d matches decimal digits, \\h matches hexadecimal digits \u2014 that is, characters in [0-9a-fA-F] . \\H matches the opposite of whatever \\h matches.","title":"19. ONIG_SYN_OP2_ESC_H_XDIGIT (enable \\h and \\H)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#20-onig_syn_op2_ineffective_escape-disable","text":"Set in: As-is If set, this disables all escape codes, shorthands, and metacharacters that start with \\ (or whatever the configured escape character is), allowing \\ to be treated as a literal \\ . You usually do not want this flag to be enabled.","title":"20. ONIG_SYN_OP2_INEFFECTIVE_ESCAPE (disable \\)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#21-onig_syn_op2_qmark_lparen_if_else-enable-thenelse","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for conditional inclusion of subsequent regex patterns based on whether a prior named or numbered capture matched, or based on whether a pattern will match. This supports many different forms, including: (?(<foo>)then|else) - condition based on a capture by name. (?('foo')then|else) - condition based on a capture by name. (?(3)then|else) - condition based on a capture by number. (?(+3)then|else) - forward conditional to a future match, by relative position. (?(-3)then|else) - backward conditional to a prior match, by relative position. (?(foo)then|else) - this matches a pattern foo . (foo is any sub-expression) (New feature as of Oniguruma 6.5.)","title":"21. ONIG_SYN_OP2_QMARK_LPAREN_IF_ELSE (enable (?(...)then|else))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#22-onig_syn_op2_esc_capital_k_keep-enable-k","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for \\K , which excludes all content before it from the overall regex match (i.e., capture #0). So, for example, pattern foo\\Kbar would match foobar , but capture #0 would only include bar . (New feature as of Oniguruma 6.5.)","title":"22. ONIG_SYN_OP2_ESC_CAPITAL_K_KEEP (enable \\K)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#23-onig_syn_op2_esc_capital_r_general_newline-enable-r","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby Enables support for \\R , the \"general newline\" shorthand, which matches (\\r\\n|[\\n\\v\\f\\r\\u0085\\u2028\\u2029]) (obviously, the Unicode values are cannot be matched in ASCII encodings). (New feature as of Oniguruma 6.5.)","title":"23. ONIG_SYN_OP2_ESC_CAPITAL_R_GENERAL_NEWLINE (enable \\R)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#24-onig_syn_op2_esc_capital_n_o_super_dot-enable-n-and-o","text":"Set in: Oniguruma, Perl, Perl_NG Enables support for \\N and \\O . \\N is \"not a line break,\" which is much like the standard . metacharacter, except that while . can be affected by the single-line setting, \\N always matches exactly one character that is not one of the various line-break characters (like \\n and \\r ). \\O matches exactly one character, regardless of whether single-line or multi-line mode are enabled or disabled. (New feature as of Oniguruma 6.5.)","title":"24. ONIG_SYN_OP2_ESC_CAPITAL_N_O_SUPER_DOT (enable \\N and \\O)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#25-onig_syn_op2_qmark_tilde_absent_group-enable","text":"Set in: Oniguruma, Ruby Enables support for the (?~r) \"absent operator\" syntax, which matches as much as possible as long as the result doesn't match pattern r . This is not the same as negative lookahead or negative lookbehind. Among the most useful examples of this is \\/\\*(?~\\*\\/)\\*\\/ , which matches C-style comments by simply saying \"starts with /*, ends with */, and doesn't contain a */ in between.\" A full explanation of this feature is complicated, but it is useful, and an excellent article about it is available on Medium . (New feature as of Oniguruma 6.5.)","title":"25. ONIG_SYN_OP2_QMARK_TILDE_ABSENT_GROUP (enable (?~...))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#26-onig_syn_op2_esc_x_y_text_segment-enable-x-and-y-and-y","text":"Set in: Oniguruma, Perl, Perl_NG, Ruby \\X is another variation on . , designed to support Unicode, in that it matches a full grapheme cluster . In Unicode, \u00e0 can be encoded as one code point, U+00E0 , or as two, U+0061 U+0300 . If those are further escaped using UTF-8, the former becomes two bytes, and the latter becomes three. Unfortunately, . would naively match only one or two bytes, depending on the encoding, and would likely incorrectly match anything from just a to a broken half of a code point. \\X is designed to fix this: It matches the full \u00e0 , no matter how \u00e0 is encoded or decomposed. \\y matches a cluster boundary, i.e., a zero-width position between graphemes, somewhat like \\b matches boundaries between words. \\Y matches the opposite of \\y , that is, a zero-width position between code points in the middle of a grapheme. (New feature as of Oniguruma 6.6.)","title":"26. ONIG_SYN_OP2_ESC_X_Y_TEXT_SEGMENT (enable \\X and \\Y and \\y)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#27-onig_syn_op2_qmark_perl_subexp_call-enable-r-and-name","text":"Set in: Perl_NG Enables support for substituted backreferences by both name and position using Perl-5-specific syntax. This supports using (?R3) and (?&name) to reference previous (and future) matches, similar to the more-common \\g<3> and \\g<name> backreferences. (New feature as of Oniguruma 6.7.)","title":"27. ONIG_SYN_OP2_QMARK_PERL_SUBEXP_CALL (enable (?R) and (?&amp;name))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#28-onig_syn_op2_qmark_brace_callout_contents-enable","text":"Set in: Oniguruma, Perl, Perl_NG Enables support for Perl-style \"callouts\" \u2014 pattern substitutions that result from invoking a callback method. When (?{foo}) is reached in a pattern, the callback function set in onig_set_progress_callout() will be invoked, and be able to perform custom computation during the pattern match (and during backtracking). Full documentation for this advanced feature can be found in the Oniguruma docs/CALLOUT.md file, with an example in samples/callout.c . (New feature as of Oniguruma 6.8.)","title":"28. ONIG_SYN_OP2_QMARK_BRACE_CALLOUT_CONTENTS (enable (?{...}))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#29-onig_syn_op2_asterisk_callout_name-enable-name","text":"Set in: Oniguruma, Perl, Perl_NG Enables support for Perl-style \"callouts\" \u2014 pattern substitutions that result from invoking a callback method. When (*foo) is reached in a pattern, the callback function set in onig_set_callout_of_name() will be invoked, passing the given name foo to it, and it can perform custom computation during the pattern match (and during backtracking). Full documentation for this advanced feature can be found in the Oniguruma docs/CALLOUT.md file, with an example in samples/callout.c . (New feature as of Oniguruma 6.8.)","title":"29. ONIG_SYN_OP2_ASTERISK_CALLOUT_NAME (enable (*name))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#30-onig_syn_op2_option_oniguruma-enable-options-imxwsdpy-and-imxwdsp","text":"Set in: Oniguruma Enables support of regex options. (i,m,x,W,S,D,P,y) (New feature as of Oniguruma 6.9.2) i - Case-insensitivity m - Multi-line mode ( . can match \\n ) x - Extended pattern (free-formatting: whitespace will ignored) W - ASCII only word. D - ASCII only digit. S - ASCII only space. P - ASCII only POSIX properties. (includes W,D,S)","title":"30. ONIG_SYN_OP2_OPTION_ONIGURUMA (enable options (?imxWSDPy) and (?-imxWDSP))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#syntax-flags-syn","text":"This group contains rules to handle corner cases and constructs that are errors in some syntaxes but not in others.","title":"Syntax Flags (syn)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#0-onig_syn_context_indep_repeat_ops-independent-nm","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby This flag specifies how to handle operators like ? and * when they aren't directly attached to an operand, as in ^* or (*) : Are they an error, are they discarded, or are they taken as literals? If this flag is clear, they are taken as literals; otherwise, the ONIG_SYN_CONTEXT_INVALID_REPEAT_OPS flag determines if they are errors or if they are discarded.","title":"0. ONIG_SYN_CONTEXT_INDEP_REPEAT_OPS (independent ?, *, +, {n,m})"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#1-onig_syn_context_invalid_repeat_ops-error-or-ignore-independent-operators","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby If ONIG_SYN_CONTEXT_INDEP_REPEAT_OPS is set, this flag controls what happens when independent operators appear in a pattern: If this flag is set, then independent operators produce an error message; if this flag is clear, then independent operators are silently discarded.","title":"1. ONIG_SYN_CONTEXT_INVALID_REPEAT_OPS (error or ignore independent operators)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#2-onig_syn_allow_unmatched_close_subexp-allow","text":"Set in: PosixExtended This flag, if set, causes a ) character without a preceding ( to be treated as a literal ) , equivalent to \\) . If this flag is clear, then an unmatched ) character will produce an error message.","title":"2. ONIG_SYN_ALLOW_UNMATCHED_CLOSE_SUBEXP (allow ...)...)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#3-onig_syn_allow_invalid_interval-allow","text":"Set in: Oniguruma, GnuRegex, Java, Perl, Perl_NG, Ruby This flag, if set, causes an invalid range, like foo{bar} or foo{} , to be silently discarded, as if foo had been written instead. If clear, an invalid range will produce an error message.","title":"3. ONIG_SYN_ALLOW_INVALID_INTERVAL (allow {???)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#4-onig_syn_allow_interval_low_abbrev-allow-n-to-mean-0n","text":"Set in: Oniguruma, Ruby If this flag is set, then r{,n} will be treated as equivalent to writing {0,n} . If this flag is clear, then r{,n} will produce an error message. Note that regardless of whether this flag is set or clear, if ONIG_SYN_OP_BRACE_INTERVAL is enabled, then r{n,} will always be legal: This flag only controls the behavior of the opposite form, r{,n} .","title":"4. ONIG_SYN_ALLOW_INTERVAL_LOW_ABBREV (allow {,n} to mean {0,n})"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#5-onig_syn_strict_check_backref-error-on-invalid-backrefs","text":"Set in: none If this flag is set, an invalid backref, like \\1 in a pattern with no captures, will produce an error. If this flag is clear, then an invalid backref will be equivalent to the empty string. No built-in syntax has this flag enabled.","title":"5. ONIG_SYN_STRICT_CHECK_BACKREF (error on invalid backrefs)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#6-onig_syn_different_len_alt_look_behind-allow-abc","text":"Set in: Oniguruma, Java, Ruby If this flag is set, lookbehind patterns with alternate options may have differing lengths among those options. If this flag is clear, lookbehind patterns with options must have each option have identical length to the other options. Oniguruma can handle either form, but not all regex engines can, so for compatibility, Oniguruma allows you to cause regexes for other regex engines to fail if they might depend on this rule.","title":"6. ONIG_SYN_DIFFERENT_LEN_ALT_LOOK_BEHIND (allow (?&lt;=a|bc))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#7-onig_syn_capture_only_named_group-prefer-kname-over-3","text":"Set in: Oniguruma, Perl_NG, Ruby If this flag is set on the syntax and ONIG_OPTION_CAPTURE_GROUP is set when calling Oniguruma, then if a name is used on any capture, all captures must also use names: A single use of a named capture prohibits the use of numbered captures.","title":"7. ONIG_SYN_CAPTURE_ONLY_NAMED_GROUP (prefer \\k&lt;name&gt; over \\3)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#8-onig_syn_allow_multiplex_definition_name-allow-xx","text":"Set in: Oniguruma, Perl_NG, Ruby If this flag is set, multiple capture groups may use the same name. If this flag is clear, then reuse of a name will produce an error message.","title":"8. ONIG_SYN_ALLOW_MULTIPLEX_DEFINITION_NAME (allow (?&lt;x&gt;)...(?&lt;x&gt;))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#9-onig_syn_fixed_interval_is_greedy_only-an-is-equivalent-to-an","text":"Set in: Oniguruma, Ruby If this flag is set, then intervals of a fixed size will ignore a lazy (non-greedy) ? quantifier and treat it as an optional match (an ordinary r? ), since \"match as little as possible\" is meaningless for a fixed-size interval. If this flag is clear, then r{n}? will mean the same as r{n} , and the useless ? will be discarded.","title":"9. ONIG_SYN_FIXED_INTERVAL_IS_GREEDY_ONLY (a{n}? is equivalent to (?:a{n})?)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#10-onig_syn_isolated_option_continue_branch-i","text":"Set in: Perl, Perl_NG, Java If this flag is set, then an isolated option doesn't break the branch and affects until the end of the group (or end of the pattern). If this flag is not set, then an isolated option is interpreted as the starting point of a new branch. /a(?i)b|c/ ==> /a(?i:b|c)/","title":"10. ONIG_SYN_ISOLATED_OPTION_CONTINUE_BRANCH (..(?i)..)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#11-onig_syn_variable_len_look_behind-a","text":"Set in: Oniguruma, Java If this flag is set, then a variable length expressions are allowed in look-behind.","title":"11. ONIG_SYN_VARIABLE_LEN_LOOK_BEHIND ((?&lt;=...a+...))"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#20-onig_syn_not_newline_in_negative_cc-add-n-to","text":"Set in: Grep If this flag is set, all newline characters (like \\n ) will be excluded from a negative character class automatically, as if the pattern had been written as [^...\\n] . If this flag is clear, negative character classes do not automatically exclude newlines, and only exclude those characters and ranges written in them.","title":"20. ONIG_SYN_NOT_NEWLINE_IN_NEGATIVE_CC (add \\n to [^...])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#21-onig_syn_backslash_escape_in_cc-allow-w","text":"Set in: Oniguruma, GnuRegex, Java, Perl, Perl_NG, Ruby If this flag is set, shorthands like \\w are allowed to describe characters in character classes. If this flag is clear, shorthands like \\w are treated as a redundantly-escaped literal w .","title":"21. ONIG_SYN_BACKSLASH_ESCAPE_IN_CC (allow [...\\w...])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#22-onig_syn_allow_empty_range_in_cc-silently-discard-z-a","text":"Set in: Emacs, Grep If this flag is set, then character ranges like [z-a] that are broken or contain no characters will be silently ignored. If this flag is clear, then broken or empty character ranges will produce an error message.","title":"22. ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC (silently discard [z-a])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#23-onig_syn_allow_double_range_op_in_cc-treat-0-9-a-as-0-9-a","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby If this flag is set, then a trailing - after a character range will be taken as a literal - , as if it had been escaped as \\- . If this flag is clear, then a trailing - after a character range will produce an error message.","title":"23. ONIG_SYN_ALLOW_DOUBLE_RANGE_OP_IN_CC (treat [0-9-a] as [0-9\\-a])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#24-onig_syn_warn_cc_op_not_escaped-warn-on-and-x","text":"Set in: Oniguruma, Ruby If this flag is set, Oniguruma will be stricter about warning for bad forms in character classes: [[...] will produce a warning, but [\\[...] will not; [-x] will produce a warning, but [\\-x] will not; [x&&-y] will produce a warning, while [x&&\\-y] will not; and so on. If this flag is clear, all of these warnings will be silently discarded.","title":"24. ONIG_SYN_WARN_CC_OP_NOT_ESCAPED (warn on [[...] and [-x])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#25-onig_syn_warn_redundant_nested_repeat-warn-on-a","text":"Set in: Oniguruma, Ruby If this flag is set, Oniguruma will warn about nested repeat operators those have no meaning, like (?:a*)+ . If this flag is clear, Oniguruma will allow the nested repeat operators without warning about them.","title":"25. ONIG_SYN_WARN_REDUNDANT_NESTED_REPEAT (warn on (?:a*)+)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#26-onig_syn_allow_invalid_code_end_of_range_in_cc-allow-a-x7fffffff","text":"Set in: Oniguruma If this flag is set, then invalid code points at the end of range in character class are allowed.","title":"26. ONIG_SYN_ALLOW_INVALID_CODE_END_OF_RANGE_IN_CC (allow [a-\\x{7fffffff}])"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#31-onig_syn_context_indep_anchors","text":"Set in: Oniguruma, PosixExtended, GnuRegex, Java, Perl, Perl_NG, Ruby Not currently used, and does nothing. (But still set in several syntaxes for some reason.)","title":"31. ONIG_SYN_CONTEXT_INDEP_ANCHORS"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#usage-tables","text":"These tables show which of the built-in syntaxes use which flags and options, for easy comparison between them.","title":"Usage tables"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#group-one-flags-op_1","text":"ID Option PosB PosEx Emacs Grep Gnu Java Perl PeNG Ruby Onig 0 ONIG_SYN_OP_VARIABLE_META_CHARACTERS - - - - - - - - - - 1 ONIG_SYN_OP_DOT_ANYCHAR Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 2 ONIG_SYN_OP_ASTERISK_ZERO_INF Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 3 ONIG_SYN_OP_ESC_ASTERISK_ZERO_INF - - - - - - - - - - 4 ONIG_SYN_OP_PLUS_ONE_INF - Yes Yes - Yes Yes Yes Yes Yes Yes 5 ONIG_SYN_OP_ESC_PLUS_ONE_INF - - - Yes - - - - - - 6 ONIG_SYN_OP_QMARK_ZERO_ONE - Yes Yes - Yes Yes Yes Yes Yes Yes 7 ONIG_SYN_OP_ESC_QMARK_ZERO_ONE - - - Yes - - - - - - 8 ONIG_SYN_OP_BRACE_INTERVAL - Yes - - Yes Yes Yes Yes Yes Yes 9 ONIG_SYN_OP_ESC_BRACE_INTERVAL Yes - Yes Yes - - - - - - 10 ONIG_SYN_OP_VBAR_ALT - Yes - - Yes Yes Yes Yes Yes Yes 11 ONIG_SYN_OP_ESC_VBAR_ALT - - Yes Yes - - - - - - 12 ONIG_SYN_OP_LPAREN_SUBEXP - Yes - - Yes Yes Yes Yes Yes Yes 13 ONIG_SYN_OP_ESC_LPAREN_SUBEXP Yes - Yes Yes - - - - - - 14 ONIG_SYN_OP_ESC_AZ_BUF_ANCHOR - - - - Yes Yes Yes Yes Yes Yes 15 ONIG_SYN_OP_ESC_CAPITAL_G_BEGIN_ANCHOR - - - - Yes Yes Yes Yes Yes Yes 16 ONIG_SYN_OP_DECIMAL_BACKREF Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 17 ONIG_SYN_OP_BRACKET_CC Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 18 ONIG_SYN_OP_ESC_W_WORD - - - Yes Yes Yes Yes Yes Yes Yes 19 ONIG_SYN_OP_ESC_LTGT_WORD_BEGIN_END - - - Yes Yes - - - - - 20 ONIG_SYN_OP_ESC_B_WORD_BOUND - - - Yes Yes Yes Yes Yes Yes Yes 21 ONIG_SYN_OP_ESC_S_WHITE_SPACE - - - - Yes Yes Yes Yes Yes Yes 22 ONIG_SYN_OP_ESC_D_DIGIT - - - - Yes Yes Yes Yes Yes Yes 23 ONIG_SYN_OP_LINE_ANCHOR - - Yes Yes Yes Yes Yes Yes Yes Yes 24 ONIG_SYN_OP_POSIX_BRACKET Yes Yes Yes - Yes Yes Yes Yes Yes Yes 25 ONIG_SYN_OP_QMARK_NON_GREEDY - - - - - Yes Yes Yes Yes Yes 26 ONIG_SYN_OP_ESC_CONTROL_CHARS Yes Yes - - - Yes Yes Yes Yes Yes 27 ONIG_SYN_OP_ESC_C_CONTROL - - - - - Yes Yes Yes Yes Yes 28 ONIG_SYN_OP_ESC_OCTAL3 - - - - - Yes Yes Yes Yes Yes 29 ONIG_SYN_OP_ESC_X_HEX2 - - - - - Yes Yes Yes Yes Yes 30 ONIG_SYN_OP_ESC_X_BRACE_HEX8 - - - - - - Yes Yes Yes Yes 31 ONIG_SYN_OP_ESC_O_BRACE_OCTAL - - - - - - Yes Yes Yes Yes","title":"Group One Flags (op)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#group-two-flags-op2_1","text":"ID Option PosB PosEx Emacs Grep Gnu Java Perl PeNG Ruby Onig 0 ONIG_SYN_OP2_ESC_CAPITAL_Q_QUOTE - - - - - Yes Yes Yes - - 1 ONIG_SYN_OP2_QMARK_GROUP_EFFECT - - - - - Yes Yes Yes Yes Yes 2 ONIG_SYN_OP2_OPTION_PERL - - - - - Yes Yes Yes - - 3 ONIG_SYN_OP2_OPTION_RUBY - - - - - - - - Yes - 4 ONIG_SYN_OP2_PLUS_POSSESSIVE_REPEAT - - - - - - - - Yes Yes 5 ONIG_SYN_OP2_PLUS_POSSESSIVE_INTERVAL - - - - - Yes - - - - 6 ONIG_SYN_OP2_CCLASS_SET_OP - - - - - - - Yes Yes Yes 7 ONIG_SYN_OP2_QMARK_LT_NAMED_GROUP - - - - - - - Yes Yes Yes 8 ONIG_SYN_OP2_ESC_K_NAMED_BACKREF - - - - - - - Yes Yes Yes 9 ONIG_SYN_OP2_ESC_G_SUBEXP_CALL - - - - - - - Yes Yes Yes 10 ONIG_SYN_OP2_ATMARK_CAPTURE_HISTORY - - - - - - - - - - 11 ONIG_SYN_OP2_ESC_CAPITAL_C_BAR_CONTROL - - - - - - - - Yes Yes 12 ONIG_SYN_OP2_ESC_CAPITAL_M_BAR_META - - - - - - - - Yes Yes 13 ONIG_SYN_OP2_ESC_V_VTAB - - - - - Yes - - Yes Yes 14 ONIG_SYN_OP2_ESC_U_HEX4 - - - - - Yes - - Yes Yes 15 ONIG_SYN_OP2_ESC_GNU_BUF_ANCHOR - - Yes - - - - - - - 16 ONIG_SYN_OP2_ESC_P_BRACE_CHAR_PROPERTY - - - - - Yes Yes Yes Yes Yes 17 ONIG_SYN_OP2_ESC_P_BRACE_CIRCUMFLEX_NOT - - - - - - Yes Yes Yes Yes 18 ONIG_SYN_OP2_CHAR_PROPERTY_PREFIX_IS - - - - - - - - - - 19 ONIG_SYN_OP2_ESC_H_XDIGIT - - - - - - - - Yes Yes 20 ONIG_SYN_OP2_INEFFECTIVE_ESCAPE - - - - - - - - - - 21 ONIG_SYN_OP2_QMARK_LPAREN_IF_ELSE - - - - - - Yes Yes Yes Yes 22 ONIG_SYN_OP2_ESC_CAPITAL_K_KEEP - - - - - - Yes Yes Yes Yes 23 ONIG_SYN_OP2_ESC_CAPITAL_R_GENERAL_NEWLINE - - - - - - Yes Yes Yes Yes 24 ONIG_SYN_OP2_ESC_CAPITAL_N_O_SUPER_DOT - - - - - - Yes Yes - Yes 25 ONIG_SYN_OP2_QMARK_TILDE_ABSENT_GROUP - - - - - - - - Yes Yes 26 ONIG_SYN_OP2_ESC_X_Y_TEXT_SEGMENT - - - - - - Yes Yes Yes Yes 27 ONIG_SYN_OP2_QMARK_PERL_SUBEXP_CALL - - - - - - - Yes - - 28 ONIG_SYN_OP2_QMARK_BRACE_CALLOUT_CONTENTS - - - - - - Yes Yes Yes - 29 ONIG_SYN_OP2_ASTERISK_CALLOUT_NAME - - - - - - Yes Yes Yes - 30 ONIG_SYN_OP2_OPTION_ONIGURUMA - - - - - - - - - Yes","title":"Group Two Flags (op2)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/RegularExpressionDxe/oniguruma/doc/SYNTAX/#syntax-flags-syn_1","text":"ID Option PosB PosEx Emacs Grep Gnu Java Perl PeNG Ruby Onig 0 ONIG_SYN_CONTEXT_INDEP_REPEAT_OPS - Yes - - Yes Yes Yes Yes Yes Yes 1 ONIG_SYN_CONTEXT_INVALID_REPEAT_OPS - - - - Yes Yes Yes Yes Yes Yes 2 ONIG_SYN_ALLOW_UNMATCHED_CLOSE_SUBEXP - Yes - - - - - - - - 3 ONIG_SYN_ALLOW_INVALID_INTERVAL - - - - Yes Yes Yes Yes Yes Yes 4 ONIG_SYN_ALLOW_INTERVAL_LOW_ABBREV - - - - - - - - Yes Yes 5 ONIG_SYN_STRICT_CHECK_BACKREF - - - - - - - - - - 6 ONIG_SYN_DIFFERENT_LEN_ALT_LOOK_BEHIND - - - - - Yes - - Yes Yes 7 ONIG_SYN_CAPTURE_ONLY_NAMED_GROUP - - - - - - - Yes Yes Yes 8 ONIG_SYN_ALLOW_MULTIPLEX_DEFINITION_NAME - - - - - - - Yes Yes Yes 9 ONIG_SYN_FIXED_INTERVAL_IS_GREEDY_ONLY - - - - - - - - Yes Yes 10 ONIG_SYN_ISOLATED_OPTION_CONTINUE_BRANCH - - - - - Yes Yes Yes - - 11 ONIG_SYN_VARIABLE_LEN_LOOK_BEHIND - - - - - Yes - - - Yes 20 ONIG_SYN_NOT_NEWLINE_IN_NEGATIVE_CC - - - Yes - - - - - - 21 ONIG_SYN_BACKSLASH_ESCAPE_IN_CC - - - - Yes Yes Yes Yes Yes Yes 22 ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC - - Yes Yes - - - - - - 23 ONIG_SYN_ALLOW_DOUBLE_RANGE_OP_IN_CC - Yes - - Yes Yes Yes Yes Yes Yes 24 ONIG_SYN_WARN_CC_OP_NOT_ESCAPED - - - - - - - - Yes Yes 25 ONIG_SYN_WARN_REDUNDANT_NESTED_REPEAT - - - - - - - - Yes Yes 26 ONIG_SYN_ALLOW_INVALID_CODE_END_OF_RANGE_IN_CC - - - - - - - - - Yes 31 ONIG_SYN_CONTEXT_INDEP_ANCHORS - Yes - - Yes Yes Yes Yes Yes Yes","title":"Syntax Flags (syn)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/","text":"UEFI Variable Policy \u00b6 Summary \u00b6 UEFI Variable Policy spec aims to describe the DXE protocol interface which allows enforcing certain rules on certain UEFI variables. The protocol allows communication with the Variable Policy Engine which performs the policy enforcement. The Variable Policy is comprised of a set of policy entries which describe, per UEFI variable (identified by namespace GUID and variable name) the following rules: Required variable attributes Prohibited variable attributes Minimum variable size Maximum variable size Locking: Locking \"immediately\" Locking on creation Locking based on a state of another variable The spec assumes that the Variable Policy Engine runs in a trusted enclave, potentially off the main CPU that runs UEFI. For that reason, it is assumed that the Variable Policy Engine has no concept of UEFI events, and that the communication from the DXE driver to the trusted enclave is proprietary. At power-on, the Variable Policy Engine is: Enabled -- present policy entries are evaluated on variable access calls. Unlocked -- new policy entries can be registered. Policy is expected to be clear on power-on. Policy is volatile and not preserved across system reset. DXE Protocol \u00b6 typedef struct { UINT64 Revision; DISABLE_VARIABLE_POLICY DisableVariablePolicy; IS_VARIABLE_POLICY_ENABLED IsVariablePolicyEnabled; REGISTER_VARIABLE_POLICY RegisterVariablePolicy; DUMP_VARIABLE_POLICY DumpVariablePolicy; LOCK_VARIABLE_POLICY LockVariablePolicy; } _VARIABLE_POLICY_PROTOCOL; typedef _VARIABLE_POLICY_PROTOCOL VARIABLE_POLICY_PROTOCOL; extern EFI_GUID gVariablePolicyProtocolGuid; ## Include/Protocol/VariablePolicy.h gVariablePolicyProtocolGuid = { 0x81D1675C, 0x86F6, 0x48DF, { 0xBD, 0x95, 0x9A, 0x6E, 0x4F, 0x09, 0x25, 0xC3 } } DisableVariablePolicy \u00b6 Function prototype: EFI_STATUS EFIAPI DisableVariablePolicy ( VOID ); DisableVariablePolicy call disables the Variable Policy Engine, so that the present policy entries are no longer taken into account on variable access calls. This call effectively turns off the variable policy verification for this boot. This also disables UEFI Authenticated Variable protections including Secure Boot. DisableVariablePolicy can only be called once during boot. If called more than once, it will return EFI_ALREADY_STARTED . Note, this process is irreversible until the next system reset -- there is no \"EnablePolicy\" protocol function. IsVariablePolicyEnabled \u00b6 Function prototype: EFI_STATUS EFIAPI IsVariablePolicyEnabled ( OUT BOOLEAN * State ); IsVariablePolicyEnabled accepts a pointer to a Boolean in which it will store TRUE if Variable Policy Engine is enabled, or FALSE if Variable Policy Engine is disabled. The function returns EFI_SUCCESS . RegisterVariablePolicy \u00b6 Function prototype: EFI_STATUS EFIAPI RegisterVariablePolicy ( IN CONST VARIABLE_POLICY_ENTRY * PolicyEntry ); RegisterVariablePolicy call accepts a pointer to a policy entry structure and returns the status of policy registration. If the Variable Policy Engine is not locked and the policy structures are valid, the function will return EFI_SUCCESS . If the Variable Policy Engine is locked, RegisterVariablePolicy call will return EFI_WRITE_PROTECTED and will not register the policy entry. Bulk registration is not supported at this time due to the requirements around error handling on each policy registration. Upon successful registration of a policy entry, Variable Policy Engine will then evaluate this entry on subsequent variable access calls (as long as Variable Policy Engine hasn't been disabled). DumpVariablePolicy \u00b6 Function prototype: EFI_STATUS EFIAPI DumpVariablePolicy ( OUT UINT8 * Policy , IN OUT UINT32 * Size ); DumpVariablePolicy call accepts a pointer to a buffer and a pointer to the size of the buffer as parameters and returns the status of placing the policy into the buffer. On first call to DumpVariablePolicy one should pass NULL as the buffer and a pointer to 0 as the Size variable and DumpVariablePolicy will return EFI_BUFFER_TOO_SMALL and will populate the Size parameter with the size of the needed buffer to store the policy. This way, the caller can allocate the buffer of correct size and call DumpVariablePolicy again. The function will populate the buffer with policy and return EFI_SUCCESS . LockVariablePolicy \u00b6 Function prototype: EFI_STATUS EFIAPI LockVariablePolicy ( VOID ); LockVariablePolicy locks the Variable Policy Engine, i.e. prevents any new policy entries from getting registered in this boot ( RegisterVariablePolicy calls will fail with EFI_WRITE_PROTECTED status code returned). Policy Structure \u00b6 The structure below is meant for the DXE protocol calling interface, when communicating to the Variable Policy Engine, thus the pragma pack directive. How these policies are stored in memory is up to the implementation. #pragma pack(1) typedef struct { UINT32 Version ; UINT16 Size ; UINT16 OffsetToName ; EFI_GUID Namespace ; UINT32 MinSize ; UINT32 MaxSize ; UINT32 AttributesMustHave ; UINT32 AttributesCantHave ; UINT8 LockPolicyType ; UINT8 Reserved [ 3 ]; // UINT8 LockPolicy[]; // Variable Length Field // CHAR16 Name[]; // Variable Length Field } VARIABLE_POLICY_ENTRY ; The struct VARIABLE_POLICY_ENTRY above describes the layout for a policy entry. The first element, Size , is the size of the policy entry, then followed by OffsetToName -- the number of bytes from the beginning of the struct to the name of the UEFI variable targeted by the policy entry. The name can contain wildcards to match more than one variable, more on this in the Wildcards section. The rest of the struct elements are self-explanatory. #define VARIABLE_POLICY_TYPE_NO_LOCK 0 #define VARIABLE_POLICY_TYPE_LOCK_NOW 1 #define VARIABLE_POLICY_TYPE_LOCK_ON_CREATE 2 #define VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE 3 LockPolicyType can have the following values: VARIABLE_POLICY_TYPE_NO_LOCK -- means that no variable locking is performed. However, the attribute and size constraints are still enforced. LockPolicy field is size 0. VARIABLE_POLICY_TYPE_LOCK_NOW -- means that the variable starts being locked immediately after policy entry registration. If the variable doesn't exist at this point, being LockedNow means it cannot be created on this boot. LockPolicy field is size 0. VARIABLE_POLICY_TYPE_LOCK_ON_CREATE -- means that the variable starts being locked after it is created. This allows for variable creation and protection after LockVariablePolicy() function has been called. The LockPolicy field is size 0. VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE -- means that the Variable Policy Engine will examine the state/contents of another variable to determine if the variable referenced in the policy entry is locked. typedef struct { EFI_GUID Namespace ; UINT8 Value ; UINT8 Reserved ; // CHAR16 Name[]; // Variable Length Field } VARIABLE_LOCK_ON_VAR_STATE_POLICY ; If LockPolicyType is VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE , then the final element in the policy entry struct is of type VARIABLE_LOCK_ON_VAR_STATE_POLICY , which lists the namespace GUID, name (no wildcards here), and value of the variable which state determines the locking of the variable referenced in the policy entry. The \"locking\" variable must be 1 byte in terms of payload size. If the Referenced variable contents match the Value of the VARIABLE_LOCK_ON_VAR_STATE_POLICY structure, the lock will be considered active and the target variable will be locked. If the Reference variable does not exist (ie. returns EFI_NOT_FOUND ), this policy will be considered inactive. Variable Name Wildcards \u00b6 Two types of wildcards can be used in the UEFI variable name field in a policy entry: If the Name is a zero-length array (easily checked by comparing fields Size and OffsetToName -- if they're the same, then the Name is zero-length), then all variables in the namespace specified by the provided GUID are targeted by the policy entry. Character \"#\" in the Name corresponds to one numeric character (0-9, A-F, a-f). For example, string \"Boot####\" in the Name field of the policy entry will make it so that the policy entry will target variables named \"Boot0001\", \"Boot0002\", etc. Given the above two types of wildcards, one variable can be targeted by more than one policy entry, thus there is a need to establish the precedence rule: a more specific match is applied. When a variable access operation is performed, Variable Policy Engine should first check the variable being accessed against the policy entries without wildcards, then with 1 wildcard, then with 2 wildcards, etc., followed in the end by policy entries that match the whole namespace. One can still imagine a situation where two policy entries with the same number of wildcards match the same variable -- for example, policy entries with Names \"Boot00##\" and \"Boot##01\" will both match variable \"Boot0001\". Such situation can (and should) be avoided by designing mutually exclusive Name strings with wildcards, however, if it occurs, then the policy entry that was registered first will be used. After the most specific match is selected, all other policies are ignored. Available Testing \u00b6 This functionality is current supported by two kinds of tests: there is a host-based unit test for the core business logic (this test accompanies the UefiVariablePolicyLib implementation that lives in MdeModulePkg/Library ) and there is a functional test for the protocol and its interfaces (this test lives in the Mu Plus repo in the UefiTestingPkg , but will shortly move to Mu Basecore). Host-Based Unit Test \u00b6 This test: MdeModulePkg\\Library\\UefiVariablePolicyLib\\UefiVariablePolicyUnitTest\\UefiVariablePolicyUnitTest.inf can be run as part of the Host-Based Unit Testing infrastructure provided by EDK2 PyTools (documented elsewhere). It will test all internal guarantees and is where you will find test cases for most of the policy matching and security of the Variable Policy Engine. Shell-Based Functional Test \u00b6 This test -- Variable Policy Functional Unit Test -- can be built as a UEFI Shell application and run to validate that the Variable Policy Engine is correctly installed and enforcing policies on the target system. NOTE: This test must be run prior to calling DisableVariablePolicy for all test cases to pass. For this reason, it is recommended to run this on a test-built FW for complete results, and then again on a production-built FW for release results. Use Cases \u00b6 The below examples are hypothetical scenarios based on real-world requirements that demonstrate how Variable Policies could be constructed to solve various problems. UEFI Setup Variables (Example 1) \u00b6 Variables containing values of the setup options exposed via UEFI menu (setup variables). These would be locked based on a state of another variable, \"ReadyToBoot\", which would be set to 1 at the ReadyToBoot event. Thus, the policy for the setup variables would be of type LockOnVarState , with the \"ReadyToBoot\" listed as the name of the variable, appropriate GUID listed as the namespace, and 1 as value. Entry into the trusted UEFI menu app doesn't signal ReadyToBoot, but booting to any device does, and the setup variables are write-protected. The \"ReadyToBoot\" variable would need to be locked-on-create. (THIS IS ESSENTIALLY LOCK ON EVENT, BUT SINCE THE POLICY ENGINE IS NOT IN THE UEFI ENVIRONMENT VARIABLES ARE USED) For example, \"AllowPXEBoot\" variable locked by \"ReadyToBoot\" variable. (NOTE: In the below example, the emphasized fields ('Namespace', 'Value', and 'Name') are members of the VARIABLE_LOCK_ON_VAR_STATE_POLICY structure.) Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE Namespace ... Value 1 Name \"ReadyToBoot\" //Name \"AllowPXEBoot\" Manufacturing VPD (Example 2) \u00b6 Manufacturing Variable Provisioning Data (VPD) is stored in variables and is created while in Manufacturing (MFG) Mode. In MFG Mode Variable Policy Engine is disabled, thus these VPD variables can be created. These variables are locked with lock policy type LockNow , so that these variables can't be tampered with in Customer Mode. To overwrite or clear VPD, the device would need to MFG mode, which is standard practice for refurbishing/remanufacturing scenarios. Example: \"DisplayPanelCalibration\" variable... Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_NOW // Name \"DisplayPanelCalibration\" 3 rd Party Calibration Data (Example 3) \u00b6 Bluetooth pre-pairing variables are locked-on-create because these get created by an OS application when Variable Policy is in effect. Example: \"KeyboardBTPairing\" variable Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_ON_CREATE // Name \"KeyboardBTPairing\" Software-based Variable Policy (Example 4) \u00b6 Example: \"Boot####\" variables (a name string with wildcards that will match variables \"Boot0000\" to \"BootFFFF\") locked by \"LockBootOrder\" variable. Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE Namespace ... Value 1 Name \"LockBootOrder\" //Name \"Boot####\"","title":"Uefi Variable Policy"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#uefi-variable-policy","text":"","title":"UEFI Variable Policy"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#summary","text":"UEFI Variable Policy spec aims to describe the DXE protocol interface which allows enforcing certain rules on certain UEFI variables. The protocol allows communication with the Variable Policy Engine which performs the policy enforcement. The Variable Policy is comprised of a set of policy entries which describe, per UEFI variable (identified by namespace GUID and variable name) the following rules: Required variable attributes Prohibited variable attributes Minimum variable size Maximum variable size Locking: Locking \"immediately\" Locking on creation Locking based on a state of another variable The spec assumes that the Variable Policy Engine runs in a trusted enclave, potentially off the main CPU that runs UEFI. For that reason, it is assumed that the Variable Policy Engine has no concept of UEFI events, and that the communication from the DXE driver to the trusted enclave is proprietary. At power-on, the Variable Policy Engine is: Enabled -- present policy entries are evaluated on variable access calls. Unlocked -- new policy entries can be registered. Policy is expected to be clear on power-on. Policy is volatile and not preserved across system reset.","title":"Summary"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#dxe-protocol","text":"typedef struct { UINT64 Revision; DISABLE_VARIABLE_POLICY DisableVariablePolicy; IS_VARIABLE_POLICY_ENABLED IsVariablePolicyEnabled; REGISTER_VARIABLE_POLICY RegisterVariablePolicy; DUMP_VARIABLE_POLICY DumpVariablePolicy; LOCK_VARIABLE_POLICY LockVariablePolicy; } _VARIABLE_POLICY_PROTOCOL; typedef _VARIABLE_POLICY_PROTOCOL VARIABLE_POLICY_PROTOCOL; extern EFI_GUID gVariablePolicyProtocolGuid; ## Include/Protocol/VariablePolicy.h gVariablePolicyProtocolGuid = { 0x81D1675C, 0x86F6, 0x48DF, { 0xBD, 0x95, 0x9A, 0x6E, 0x4F, 0x09, 0x25, 0xC3 } }","title":"DXE Protocol"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#disablevariablepolicy","text":"Function prototype: EFI_STATUS EFIAPI DisableVariablePolicy ( VOID ); DisableVariablePolicy call disables the Variable Policy Engine, so that the present policy entries are no longer taken into account on variable access calls. This call effectively turns off the variable policy verification for this boot. This also disables UEFI Authenticated Variable protections including Secure Boot. DisableVariablePolicy can only be called once during boot. If called more than once, it will return EFI_ALREADY_STARTED . Note, this process is irreversible until the next system reset -- there is no \"EnablePolicy\" protocol function.","title":"DisableVariablePolicy"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#isvariablepolicyenabled","text":"Function prototype: EFI_STATUS EFIAPI IsVariablePolicyEnabled ( OUT BOOLEAN * State ); IsVariablePolicyEnabled accepts a pointer to a Boolean in which it will store TRUE if Variable Policy Engine is enabled, or FALSE if Variable Policy Engine is disabled. The function returns EFI_SUCCESS .","title":"IsVariablePolicyEnabled"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#registervariablepolicy","text":"Function prototype: EFI_STATUS EFIAPI RegisterVariablePolicy ( IN CONST VARIABLE_POLICY_ENTRY * PolicyEntry ); RegisterVariablePolicy call accepts a pointer to a policy entry structure and returns the status of policy registration. If the Variable Policy Engine is not locked and the policy structures are valid, the function will return EFI_SUCCESS . If the Variable Policy Engine is locked, RegisterVariablePolicy call will return EFI_WRITE_PROTECTED and will not register the policy entry. Bulk registration is not supported at this time due to the requirements around error handling on each policy registration. Upon successful registration of a policy entry, Variable Policy Engine will then evaluate this entry on subsequent variable access calls (as long as Variable Policy Engine hasn't been disabled).","title":"RegisterVariablePolicy"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#dumpvariablepolicy","text":"Function prototype: EFI_STATUS EFIAPI DumpVariablePolicy ( OUT UINT8 * Policy , IN OUT UINT32 * Size ); DumpVariablePolicy call accepts a pointer to a buffer and a pointer to the size of the buffer as parameters and returns the status of placing the policy into the buffer. On first call to DumpVariablePolicy one should pass NULL as the buffer and a pointer to 0 as the Size variable and DumpVariablePolicy will return EFI_BUFFER_TOO_SMALL and will populate the Size parameter with the size of the needed buffer to store the policy. This way, the caller can allocate the buffer of correct size and call DumpVariablePolicy again. The function will populate the buffer with policy and return EFI_SUCCESS .","title":"DumpVariablePolicy"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#lockvariablepolicy","text":"Function prototype: EFI_STATUS EFIAPI LockVariablePolicy ( VOID ); LockVariablePolicy locks the Variable Policy Engine, i.e. prevents any new policy entries from getting registered in this boot ( RegisterVariablePolicy calls will fail with EFI_WRITE_PROTECTED status code returned).","title":"LockVariablePolicy"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#policy-structure","text":"The structure below is meant for the DXE protocol calling interface, when communicating to the Variable Policy Engine, thus the pragma pack directive. How these policies are stored in memory is up to the implementation. #pragma pack(1) typedef struct { UINT32 Version ; UINT16 Size ; UINT16 OffsetToName ; EFI_GUID Namespace ; UINT32 MinSize ; UINT32 MaxSize ; UINT32 AttributesMustHave ; UINT32 AttributesCantHave ; UINT8 LockPolicyType ; UINT8 Reserved [ 3 ]; // UINT8 LockPolicy[]; // Variable Length Field // CHAR16 Name[]; // Variable Length Field } VARIABLE_POLICY_ENTRY ; The struct VARIABLE_POLICY_ENTRY above describes the layout for a policy entry. The first element, Size , is the size of the policy entry, then followed by OffsetToName -- the number of bytes from the beginning of the struct to the name of the UEFI variable targeted by the policy entry. The name can contain wildcards to match more than one variable, more on this in the Wildcards section. The rest of the struct elements are self-explanatory. #define VARIABLE_POLICY_TYPE_NO_LOCK 0 #define VARIABLE_POLICY_TYPE_LOCK_NOW 1 #define VARIABLE_POLICY_TYPE_LOCK_ON_CREATE 2 #define VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE 3 LockPolicyType can have the following values: VARIABLE_POLICY_TYPE_NO_LOCK -- means that no variable locking is performed. However, the attribute and size constraints are still enforced. LockPolicy field is size 0. VARIABLE_POLICY_TYPE_LOCK_NOW -- means that the variable starts being locked immediately after policy entry registration. If the variable doesn't exist at this point, being LockedNow means it cannot be created on this boot. LockPolicy field is size 0. VARIABLE_POLICY_TYPE_LOCK_ON_CREATE -- means that the variable starts being locked after it is created. This allows for variable creation and protection after LockVariablePolicy() function has been called. The LockPolicy field is size 0. VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE -- means that the Variable Policy Engine will examine the state/contents of another variable to determine if the variable referenced in the policy entry is locked. typedef struct { EFI_GUID Namespace ; UINT8 Value ; UINT8 Reserved ; // CHAR16 Name[]; // Variable Length Field } VARIABLE_LOCK_ON_VAR_STATE_POLICY ; If LockPolicyType is VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE , then the final element in the policy entry struct is of type VARIABLE_LOCK_ON_VAR_STATE_POLICY , which lists the namespace GUID, name (no wildcards here), and value of the variable which state determines the locking of the variable referenced in the policy entry. The \"locking\" variable must be 1 byte in terms of payload size. If the Referenced variable contents match the Value of the VARIABLE_LOCK_ON_VAR_STATE_POLICY structure, the lock will be considered active and the target variable will be locked. If the Reference variable does not exist (ie. returns EFI_NOT_FOUND ), this policy will be considered inactive.","title":"Policy Structure"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#variable-name-wildcards","text":"Two types of wildcards can be used in the UEFI variable name field in a policy entry: If the Name is a zero-length array (easily checked by comparing fields Size and OffsetToName -- if they're the same, then the Name is zero-length), then all variables in the namespace specified by the provided GUID are targeted by the policy entry. Character \"#\" in the Name corresponds to one numeric character (0-9, A-F, a-f). For example, string \"Boot####\" in the Name field of the policy entry will make it so that the policy entry will target variables named \"Boot0001\", \"Boot0002\", etc. Given the above two types of wildcards, one variable can be targeted by more than one policy entry, thus there is a need to establish the precedence rule: a more specific match is applied. When a variable access operation is performed, Variable Policy Engine should first check the variable being accessed against the policy entries without wildcards, then with 1 wildcard, then with 2 wildcards, etc., followed in the end by policy entries that match the whole namespace. One can still imagine a situation where two policy entries with the same number of wildcards match the same variable -- for example, policy entries with Names \"Boot00##\" and \"Boot##01\" will both match variable \"Boot0001\". Such situation can (and should) be avoided by designing mutually exclusive Name strings with wildcards, however, if it occurs, then the policy entry that was registered first will be used. After the most specific match is selected, all other policies are ignored.","title":"Variable Name Wildcards"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#available-testing","text":"This functionality is current supported by two kinds of tests: there is a host-based unit test for the core business logic (this test accompanies the UefiVariablePolicyLib implementation that lives in MdeModulePkg/Library ) and there is a functional test for the protocol and its interfaces (this test lives in the Mu Plus repo in the UefiTestingPkg , but will shortly move to Mu Basecore).","title":"Available Testing"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#host-based-unit-test","text":"This test: MdeModulePkg\\Library\\UefiVariablePolicyLib\\UefiVariablePolicyUnitTest\\UefiVariablePolicyUnitTest.inf can be run as part of the Host-Based Unit Testing infrastructure provided by EDK2 PyTools (documented elsewhere). It will test all internal guarantees and is where you will find test cases for most of the policy matching and security of the Variable Policy Engine.","title":"Host-Based Unit Test"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#shell-based-functional-test","text":"This test -- Variable Policy Functional Unit Test -- can be built as a UEFI Shell application and run to validate that the Variable Policy Engine is correctly installed and enforcing policies on the target system. NOTE: This test must be run prior to calling DisableVariablePolicy for all test cases to pass. For this reason, it is recommended to run this on a test-built FW for complete results, and then again on a production-built FW for release results.","title":"Shell-Based Functional Test"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#use-cases","text":"The below examples are hypothetical scenarios based on real-world requirements that demonstrate how Variable Policies could be constructed to solve various problems.","title":"Use Cases"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#uefi-setup-variables-example-1","text":"Variables containing values of the setup options exposed via UEFI menu (setup variables). These would be locked based on a state of another variable, \"ReadyToBoot\", which would be set to 1 at the ReadyToBoot event. Thus, the policy for the setup variables would be of type LockOnVarState , with the \"ReadyToBoot\" listed as the name of the variable, appropriate GUID listed as the namespace, and 1 as value. Entry into the trusted UEFI menu app doesn't signal ReadyToBoot, but booting to any device does, and the setup variables are write-protected. The \"ReadyToBoot\" variable would need to be locked-on-create. (THIS IS ESSENTIALLY LOCK ON EVENT, BUT SINCE THE POLICY ENGINE IS NOT IN THE UEFI ENVIRONMENT VARIABLES ARE USED) For example, \"AllowPXEBoot\" variable locked by \"ReadyToBoot\" variable. (NOTE: In the below example, the emphasized fields ('Namespace', 'Value', and 'Name') are members of the VARIABLE_LOCK_ON_VAR_STATE_POLICY structure.) Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE Namespace ... Value 1 Name \"ReadyToBoot\" //Name \"AllowPXEBoot\"","title":"UEFI Setup Variables (Example 1)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#manufacturing-vpd-example-2","text":"Manufacturing Variable Provisioning Data (VPD) is stored in variables and is created while in Manufacturing (MFG) Mode. In MFG Mode Variable Policy Engine is disabled, thus these VPD variables can be created. These variables are locked with lock policy type LockNow , so that these variables can't be tampered with in Customer Mode. To overwrite or clear VPD, the device would need to MFG mode, which is standard practice for refurbishing/remanufacturing scenarios. Example: \"DisplayPanelCalibration\" variable... Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_NOW // Name \"DisplayPanelCalibration\"","title":"Manufacturing VPD (Example 2)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#3rd-party-calibration-data-example-3","text":"Bluetooth pre-pairing variables are locked-on-create because these get created by an OS application when Variable Policy is in effect. Example: \"KeyboardBTPairing\" variable Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_ON_CREATE // Name \"KeyboardBTPairing\"","title":"3rd Party Calibration Data (Example 3)"},{"location":"dyn/mu_basecore/MdeModulePkg/Universal/Variable/UefiVariablePolicy/ReadMe/#software-based-variable-policy-example-4","text":"Example: \"Boot####\" variables (a name string with wildcards that will match variables \"Boot0000\" to \"BootFFFF\") locked by \"LockBootOrder\" variable. Size ... OffsetToName ... NameSpace ... MinSize ... MaxSize ... AttributesMustHave ... AttributesCantHave ... LockPolicyType VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE Namespace ... Value 1 Name \"LockBootOrder\" //Name \"Boot####\"","title":"Software-based Variable Policy (Example 4)"},{"location":"dyn/mu_basecore/MdePkg/Library/FltUsedLib/Readme/","text":"FltUsedLib \u00b6 This library provides a global (fltused) that needs to be defined anywhere floating point operations are used. The C compiler produces the _fltused symbol by default, this is just to satisfy the linker. Using \u00b6 To use FltUsedLib, just include it in the INF of the module that uses floating point. [LibraryClasses] BaseLib BaseMemoryLib FltUsedLib","title":"Flt Used Lib"},{"location":"dyn/mu_basecore/MdePkg/Library/FltUsedLib/Readme/#fltusedlib","text":"This library provides a global (fltused) that needs to be defined anywhere floating point operations are used. The C compiler produces the _fltused symbol by default, this is just to satisfy the linker.","title":"FltUsedLib"},{"location":"dyn/mu_basecore/MdePkg/Library/FltUsedLib/Readme/#using","text":"To use FltUsedLib, just include it in the INF of the module that uses floating point. [LibraryClasses] BaseLib BaseMemoryLib FltUsedLib","title":"Using"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/","text":"SharedNetworking \u00b6 Similar to SharedCrypto ( https://github.com/microsoft/mu_plus/tree/release/201911/SharedCryptoPkg ), SharedNetworking is a collection of pre-built network binaries you can include in your platform or other EDK2 project. SharedNetworking requires SharedCrypto for BaseCryptLib functionality (this only applies to TlsLib and IScsiDxe) The build script for this (SharedNetworkSettings.py) pulls in MU_PLUS as it has a dependency on SharedCrypto (which currently resides in MU_PLUS). This is temporary and will not carry forward to 202002. It also doesn't apply to the remainder of Basecore or CI. The dependency is only pulled in when build SharedNetworking itself, which doesn't happen often. Advantages \u00b6 Faster Compile Times Potentially smaller binary sizes (depending on compression and a variety of other factors) Easier to update and service since network binaries are packaged in an FV. Including it in your project \u00b6 Just !include the SharedNetworking.fdf.inc as the example below shows: [FV.FVDXE] ... !include NetworkPkg/SharedNetworking.fdf.inc","title":"README"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/#sharednetworking","text":"Similar to SharedCrypto ( https://github.com/microsoft/mu_plus/tree/release/201911/SharedCryptoPkg ), SharedNetworking is a collection of pre-built network binaries you can include in your platform or other EDK2 project. SharedNetworking requires SharedCrypto for BaseCryptLib functionality (this only applies to TlsLib and IScsiDxe) The build script for this (SharedNetworkSettings.py) pulls in MU_PLUS as it has a dependency on SharedCrypto (which currently resides in MU_PLUS). This is temporary and will not carry forward to 202002. It also doesn't apply to the remainder of Basecore or CI. The dependency is only pulled in when build SharedNetworking itself, which doesn't happen often.","title":"SharedNetworking"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/#advantages","text":"Faster Compile Times Potentially smaller binary sizes (depending on compression and a variety of other factors) Easier to update and service since network binaries are packaged in an FV.","title":"Advantages"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/#including-it-in-your-project","text":"Just !include the SharedNetworking.fdf.inc as the example below shows: [FV.FVDXE] ... !include NetworkPkg/SharedNetworking.fdf.inc","title":"Including it in your project"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/","text":"Shared Networking DXE \u00b6 What is it \u00b6 Shared Networking is a packaged versions of networking components from EDK II. Similar to SharedCrypto (see the SharedCryptoPkg), it precompiles certain components and allows them to be included in a platform without having to build the underlying library. How it works \u00b6 Since many parts of the network simply publish a protocol (like TlsDxe), it was fairly trivial to compile that into an EFI. This EFI is then downloaded via a NuGet External Dependency (see SharedNetworking_ext_dep.json). Versions are modified in a similar way to SharedCrypto. Versioning \u00b6 A typical version consists of 4 numbers. The year, the month of the EDK II release, the revision number, and the build number. An example of this would be 2019.03.02.01 , which would translate to EDK II 1903 release, the second revision and the first build. This means that there were two code changes within 1903 (either in BaseCryptLib or OpenSSL). Release notes will be provided on the NuGet package page and on this repo. Build numbers are reved whenever there needs to be a recompiled binary due to a mistake on our part or a build flag is tweaked. How to use it \u00b6 There are two ways to use SharedNetworking. For first way is to use the FV, which contains all the networking components needed. The second is to replace individual components with INF's. DSC/INF way \u00b6 Including it in your platform is easy peezy lemon squeezy. In fact, you only need three changes. In the example below we show X64, which happens to correspond with DXE but that could easily be changed. Look at your platform for where Networking is already defined. One thing to note is that each binary is released for two targets, RELEASE and DEBUG. Make sure to include the right INF. DSC Changes \u00b6 Parts need to be replaced on a compoenent by component basis. For example, here is how to move over TlsDxe. You need to remove the reference to TLSLib since we no longer need it (the only consumer is TlsDxe). Then switch the component to the Shared version of TLS. It looks like this: [LibraryClasses.X64] #TlsLib|CryptoPkg/Library/TlsLib/TlsLib.inf # remove this line [Components.X64] NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf FDF Changes \u00b6 [FV.FVDXE] INF NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf # Shared_TLS instead of TlsDxe ... FV way \u00b6 This way is still under development, so it maybe subject to change. In your FDF, add these lines. [FV.FVDXE] FILE FV_IMAGE = {GUID} { SECTION FV_IMAGE = NetworkPkg/SharedNetworking/Mu-SharedNetworking_extdep/$(TARGET)/{ARCH of your platform}/FVDXE.fv # Shared_Networking SECTION UI = \"SharedNetworking\" } With {GUID} being a guid you generated. We use E205F779-07E3-4B64-A2E2-EEDE717B0F59. {Arch of your platform} being the platform you're using. We currently support IA32, X64, and AARCH64. as supposered values You'll also need to remove the networking components that were already in your FDF. Why to Use SharedNetworking \u00b6 Depending on your platform, it could net you some small space savings depending on your linker. The main advantage is that when used with SharedCrypto, you can remove the need to compile OpenSSL, reducing compile times. Questions \u00b6 If you have any questions about anything in this package or the universe in general, feel free to comment on our Github or contact the Project Mu team.","title":"Shared Networking"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#shared-networking-dxe","text":"","title":"Shared Networking DXE"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#what-is-it","text":"Shared Networking is a packaged versions of networking components from EDK II. Similar to SharedCrypto (see the SharedCryptoPkg), it precompiles certain components and allows them to be included in a platform without having to build the underlying library.","title":"What is it"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#how-it-works","text":"Since many parts of the network simply publish a protocol (like TlsDxe), it was fairly trivial to compile that into an EFI. This EFI is then downloaded via a NuGet External Dependency (see SharedNetworking_ext_dep.json). Versions are modified in a similar way to SharedCrypto.","title":"How it works"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#versioning","text":"A typical version consists of 4 numbers. The year, the month of the EDK II release, the revision number, and the build number. An example of this would be 2019.03.02.01 , which would translate to EDK II 1903 release, the second revision and the first build. This means that there were two code changes within 1903 (either in BaseCryptLib or OpenSSL). Release notes will be provided on the NuGet package page and on this repo. Build numbers are reved whenever there needs to be a recompiled binary due to a mistake on our part or a build flag is tweaked.","title":"Versioning"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#how-to-use-it","text":"There are two ways to use SharedNetworking. For first way is to use the FV, which contains all the networking components needed. The second is to replace individual components with INF's.","title":"How to use it"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#dscinf-way","text":"Including it in your platform is easy peezy lemon squeezy. In fact, you only need three changes. In the example below we show X64, which happens to correspond with DXE but that could easily be changed. Look at your platform for where Networking is already defined. One thing to note is that each binary is released for two targets, RELEASE and DEBUG. Make sure to include the right INF.","title":"DSC/INF way"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#dsc-changes","text":"Parts need to be replaced on a compoenent by component basis. For example, here is how to move over TlsDxe. You need to remove the reference to TLSLib since we no longer need it (the only consumer is TlsDxe). Then switch the component to the Shared version of TLS. It looks like this: [LibraryClasses.X64] #TlsLib|CryptoPkg/Library/TlsLib/TlsLib.inf # remove this line [Components.X64] NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf","title":"DSC Changes"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#fdf-changes","text":"[FV.FVDXE] INF NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf # Shared_TLS instead of TlsDxe ...","title":"FDF Changes"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#fv-way","text":"This way is still under development, so it maybe subject to change. In your FDF, add these lines. [FV.FVDXE] FILE FV_IMAGE = {GUID} { SECTION FV_IMAGE = NetworkPkg/SharedNetworking/Mu-SharedNetworking_extdep/$(TARGET)/{ARCH of your platform}/FVDXE.fv # Shared_Networking SECTION UI = \"SharedNetworking\" } With {GUID} being a guid you generated. We use E205F779-07E3-4B64-A2E2-EEDE717B0F59. {Arch of your platform} being the platform you're using. We currently support IA32, X64, and AARCH64. as supposered values You'll also need to remove the networking components that were already in your FDF.","title":"FV way"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#why-to-use-sharednetworking","text":"Depending on your platform, it could net you some small space savings depending on your linker. The main advantage is that when used with SharedCrypto, you can remove the need to compile OpenSSL, reducing compile times.","title":"Why to Use SharedNetworking"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#questions","text":"If you have any questions about anything in this package or the universe in general, feel free to comment on our Github or contact the Project Mu team.","title":"Questions"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/release_notes/","text":"Shared Network Release Notes \u00b6 This is the packaged version of NetworkPkg. Please see more documentation here: https://github.com/microsoft/mu_basecore/tree/release/201911/NetworkPkg/SharedNetworking","title":"release notes"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/release_notes/#shared-network-release-notes","text":"This is the packaged version of NetworkPkg. Please see more documentation here: https://github.com/microsoft/mu_basecore/tree/release/201911/NetworkPkg/SharedNetworking","title":"Shared Network Release Notes"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/","text":"Unit Test Framework Package \u00b6 About \u00b6 This package adds a unit test framework capable of building tests for multiple contexts including the UEFI shell environment and host-based environments. It allows for unit test development to focus on the tests and leave error logging, result formatting, context persistance, and test running to the framework. The unit test framework works well for low level unit tests as well as system level tests and fits easily in automation frameworks. UnitTestLib \u00b6 The main \"framework\" library. The core of the framework is the Framework object, which can have any number of test cases and test suites registered with it. The Framework object is also what drives test execution. The Framework also provides helper macros and functions for checking test conditions and reporting errors. Status and error info will be logged into the test context. There are a number of Assert macros that make the unit test code friendly to view and easy to understand. Finally, the Framework also supports logging strings during the test execution. This data is logged to the test context and will be available in the test reporting phase. This should be used for logging test details and helpful messages to resolve test failures. UnitTestPersistenceLib \u00b6 Persistence lib has the main job of saving and restoring test context to a storage medium so that for tests that require exiting the active process and then resuming state can be maintained. This is critical in supporting a system reboot in the middle of a test run. UnitTestResultReportLib \u00b6 Library provides function to run at the end of a framework test run and handles formatting the report. This is a common customization point and allows the unit test framework to fit its output reports into other test infrastructure. In this package a simple library instances has been supplied to output test results to the console as plain text. Samples \u00b6 There is a sample unit test provided as both an example of how to write a unit test and leverage many of the features of the framework. This sample can be found in the Test/UnitTest/Sample/SampleUnitTest directory. The sample is provided in PEI, SMM, DXE, and UEFI App flavors. It also has a flavor for the HOST_APPLICATION build type, which can be run on a host system without needing a target. Usage \u00b6 This section is built a lot like a \"Getting Started\". We'll go through some of the components that are needed when constructing a unit test and some of the decisions that are made by the test writer. We'll also describe how to check for expected conditions in test cases and a bit of the logging characteristics. Most of these examples will refer to the SampleUnitTestUefiShell app found in this package. Requirements - INF \u00b6 In our INF file, we'll need to bring in the UnitTestLib library. Conveniently, the interface header for the UnitTestLib is located in MdePkg , so you shouldn't need to depend on any other packages. As long as your DSC file knows where to find the lib implementation that you want to use, you should be good to go. See this example in 'SampleUnitTestUefiShell.inf'... [Packages] MdePkg/MdePkg.dec [LibraryClasses] UefiApplicationEntryPoint BaseLib DebugLib UnitTestLib PrintLib Also, if you want you test to automatically be picked up by the Test Runner plugin, you will need to make sure that the module BASE_NAME contains the word Test ... [Defines] BASE_NAME = SampleUnitTestUefiShell Requirements - Code \u00b6 Not to state the obvious, but let's make sure we have the following include before getting too far along... #include <Library/UnitTestLib.h> Now that we've got that squared away, let's look at our 'Main()'' routine (or DriverEntryPoint() or whatever). Configuring the Framework \u00b6 Everything in the UnitTestPkg framework is built around an object called -- conveniently -- the Framework. This Framework object will contain all the information about our test, the test suites and test cases associated with it, the current location within the test pass, and any results that have been recorded so far. To get started with a test, we must first create a Framework instance. The function for this is InitUnitTestFramework . It takes in CHAR8 strings for the long name, short name, and test version. The long name and version strings are just for user presentation and relatively flexible. The short name will be used to name any cache files and/or test results, so should be a name that makes sense in that context. These strings are copied internally to the Framework, so using stack-allocated or literal strings is fine. In the 'SampleUnitTestUefiShell' app, the module name is used as the short name, so the init looks like this. DEBUG (( DEBUG_INFO , \"%a v%a \\n \" , UNIT_TEST_APP_NAME , UNIT_TEST_APP_VERSION )); // // Start setting up the test framework for running the tests. // Status = InitUnitTestFramework ( & Framework , UNIT_TEST_APP_NAME , gEfiCallerBaseName , UNIT_TEST_APP_VERSION ); The &Framework returned here is the handle to the Framework. If it's successfully returned, we can start adding test suites and test cases. Test suites exist purely to help organize test cases and to differentiate the results in reports. If you're writing a small unit test, you can conceivably put all test cases into a single suite. However, if you end up with 20+ test cases, it may be beneficial to organize them according to purpose. You must have at least one test suite, even if it's just a catch-all. The function to create a test suite is CreateUnitTestSuite . It takes in a handle to the Framework object, a CHAR8 string for the suite title and package name, and optional function pointers for a setup function and a teardown function. The suite title is for user presentation. The package name is for xUnit type reporting and uses a '.'-separated hierarchical format (see 'SampleUnitTestApp' for example). If provided, the setup and teardown functions will be called once at the start of the suite (before any tests have run) and once at the end of the suite (after all tests have run), respectively. If either or both of these are unneeded, pass NULL . The function prototypes are UNIT_TEST_SUITE_SETUP and UNIT_TEST_SUITE_TEARDOWN . Looking at 'SampleUnitTestUefiShell' app, you can see that the first test suite is created as below... // // Populate the SimpleMathTests Unit Test Suite. // Status = CreateUnitTestSuite ( & SimpleMathTests , Fw , \"Simple Math Tests\" , \"Sample.Math\" , NULL , NULL ); This test suite has no setup or teardown functions. The &SimpleMathTests returned here is a handle to the suite and will be used when adding test cases. Great! Now we've finished some of the cruft, red tape, and busy work. We're ready to add some tests. Adding a test to a test suite is accomplished with the -- you guessed it -- AddTestCase function. It takes in the suite handle; a CHAR8 string for the description and class name; a function pointer for the test case itself; additional, optional function pointers for prerequisite check and cleanup routines; and and optional pointer to a context structure. Okay, that's a lot. Let's take it one piece at a time. The description and class name strings are very similar in usage to the suite title and package name strings in the test suites. The former is for user presentation and the latter is for xUnit parsing. The test case function pointer is what is actually executed as the \"test\" and the prototype should be UNIT_TEST_FUNCTION . The last three parameters require a little bit more explaining. The prerequisite check function has a prototype of UNIT_TEST_PREREQUISITE and -- if provided -- will be called immediately before the test case. If this function returns any error, the test case will not be run and will be recorded as UNIT_TEST_ERROR_PREREQUISITE_NOT_MET . The cleanup function (prototype UNIT_TEST_CLEANUP ) will be called immediately after the test case to provide an opportunity to reset any global state that may have been changed in the test case. In the event of a prerequisite failure, the cleanup function will also be skipped. If either of these functions is not needed, pass NULL . The context pointer is entirely case-specific. It will be passed to the test case upon execution. One of the purposes of the context pointer is to allow test case reuse with different input data. (Another use is for testing that wraps around a system reboot, but that's beyond the scope of this guide.) The test case must know how to interpret the context pointer, so it could be a simple value, or it could be a complex structure. If unneeded, pass NULL . In 'SampleUnitTestUefiShell' app, the first test case is added using the code below... AddTestCase ( SimpleMathTests , \"Adding 1 to 1 should produce 2\" , \"Addition\" , OnePlusOneShouldEqualTwo , NULL , NULL , NULL ); This test case calls the function OnePlusOneShouldEqualTwo and has no prerequisite, cleanup, or context. Once all the suites and cases are added, it's time to run the Framework. // // Execute the tests. // Status = RunAllTestSuites ( Framework ); A Simple Test Case \u00b6 We'll take a look at the below test case from 'SampleUnitTestApp'... UNIT_TEST_STATUS EFIAPI OnePlusOneShouldEqualTwo ( IN UNIT_TEST_FRAMEWORK_HANDLE Framework , IN UNIT_TEST_CONTEXT Context ) { UINTN A , B , C ; A = 1 ; B = 1 ; C = A + B ; UT_ASSERT_EQUAL ( C , 2 ); return UNIT_TEST_PASSED ; } // OnePlusOneShouldEqualTwo() The prototype for this function matches the UNIT_TEST_FUNCTION prototype. It takes in a handle to the Framework itself and the context pointer. The context pointer could be cast and interpreted as anything within this test case, which is why it's important to configure contexts carefully. The test case returns a value of UNIT_TEST_STATUS , which will be recorded in the Framework and reported at the end of all suites. In this test case, the UT_ASSERT_EQUAL assertion is being used to establish that the business logic has functioned correctly. There are several assertion macros, and you are encouraged to use one that matches as closely to your intended test criterium as possible, because the logging is specific to the macro and more specific macros have more detailed logs. When in doubt, there are always UT_ASSERT_TRUE and UT_ASSERT_FALSE . Assertion macros that fail their test criterium will immediately return from the test case with UNIT_TEST_ERROR_TEST_FAILED and log an error string. Note that this early return can have implications for memory leakage. At the end, if all test criteria pass, you should return UNIT_TEST_PASSED . More Complex Cases \u00b6 To write more advanced tests, first take a look at all the Assertion and Logging macros provided in the framework. Beyond that, if you're writing host-based tests and want to take a dependency on the UnitTestFrameworkPkg, you can leverage the cmocka.h interface and write tests with all the features of the Cmocka framework. Documentation for Cmocka can be found here: https://api.cmocka.org/ Development \u00b6 When using the EDK2 Pytools for CI testing, the host-based unit tests will be built and run on any build that includes the NOOPT build target. If you are trying to iterate on a single test, a convenient pattern is to build only that test module. For example, the following command will build only the SafeIntLib host-based test from the MdePkg... stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG = VS2017 -p MdePkg -t NOOPT BUILDMODULE = MdePkg/Test/UnitTest/Library/BaseSafeIntLib/TestBaseSafeIntLib.inf Known Limitations \u00b6 PEI, DXE, SMM \u00b6 While sample tests have been provided for these execution environments, only cursory build validation has been performed. Care has been taken while designing the frameworks to allow for execution during boot phases, but only UEFI Shell and host-based tests have been thoroughly evaluated. Full support for PEI, DXE, and SMM is forthcoming, but should be considered beta/staging for now. Host-Based Support vs Other Tests \u00b6 The host-based test framework is powered internally by the Cmocka framework. As such, it has abilities that the target-based tests don't (yet). It would be awesome if this meant that it was a super set of the target-based tests, and it worked just like the target-based tests but with more features. Unfortunately, this is not the case. While care has been taken to keep them as close a possible, there are a few known inconsistencies that we're still ironing out. For example, the logging messages in the target-based tests are cached internally and associated with the running test case. They can be saved later as part of the reporting lib. This isn't currently possible with host-based. Only the assertion failures are logged. We will continue trying to make these as similar as possible. Unit Test Location/Layout Rules \u00b6 Code/Test Location Host-Based Unit Tests for a Library/Protocol/PPI/GUID Interface If what's being tested is an interface (e.g. a library with a public header file, like DebugLib), the test should be scoped to the parent package. Example: MdePkg/Test/UnitTest/[Library/Protocol/Ppi/Guid]/ A real-world example of this is the BaseSafeIntLib test in MdePkg. MdePkg/Test/UnitTest/Library/BaseSafeIntLib/TestBaseSafeIntLibHost.inf Host-Based Unit Tests for a Library/Driver (PEI/DXE/SMM) implementation If what's being tested is a specific implementation (e.g. BaseDebugLibSerialPort for DebugLib), the test should be scoped to the implementation directory itself, in a UnitTest subdirectory. Module Example: MdeModulePkg/Universal/EsrtFmpDxe/UnitTest/ Library Example: MdePkg/Library/BaseMemoryLib/UnitTest/ Host-Based Tests for a Functionality or Feature If you're writing a functional test that operates at the module level (i.e. if it's more than a single file or library), the test should be located in the package-level Tests directory under the HostFuncTest subdirectory. For example, if you were writing a test for the entire FMP Device Framework, you might put your test in: FmpDevicePkg/Test/HostFuncTest/FmpDeviceFramework If the feature spans multiple packages, it's location should be determined by the package owners related to the feature. Non-Host-Based (PEI/DXE/SMM/Shell) Tests for a Functionality or Feature Similar to Host-Based, if the feature is in one package, should be located in the *Pkg/Test/[Shell/Dxe/Smm/Pei]Test directory. If the feature spans multiple packages, it's location should be determined by the package owners related to the feature. USAGE EXAMPLES PEI Example: MP_SERVICE_PPI. Or check MTRR configuration in a notification function. SMM Example: a test in a protocol callback function. (It is different with the solution that SmmAgent+ShellApp) DXE Example: a test in a UEFI event call back to check SPI/SMRAM status. Shell Example: the SMM handler audit test has a shell-based app that interacts with an SMM handler to get information. The SMM paging audit test gathers information about both DXE and SMM. And the SMM paging functional test actually forces errors into SMM via a DXE driver. Example Directory Tree \u00b6 <PackageName>Pkg/ ComponentY/ ComponentY.inf ComponentY.c UnitTest/ ComponentYHostUnitTest.inf # Host-Based Test for Driver Module ComponentYUnitTest.c Library/ GeneralPurposeLibBase/ ... GeneralPurposeLibSerial/ ... SpecificLibDxe/ SpecificLibDxe.c SpecificLibDxe.inf UnitTest/ # Host-Based Test for Specific Library Implementation SpecificLibDxeHostUnitTest.c SpecificLibDxeHostUnitTest.inf Test/ <Package>HostTest.dsc # Host-Based Test Apps UnitTest/ InterfaceX InterfaceXHostUnitTest.inf # Host-Based App (should be in Test/<Package>HostTest.dsc) InterfaceXPeiUnitTest.inf # PEIM Target-Based Test (if applicable) InterfaceXDxeUnitTest.inf # DXE Target-Based Test (if applicable) InterfaceXSmmUnitTest.inf # SMM Target-Based Test (if applicable) InterfaceXShellUnitTest.inf # Shell App Target-Based Test (if applicable) InterfaceXUnitTest.c # Test Logic GeneralPurposeLib/ # Host-Based Test for any implementation of GeneralPurposeLib GeneralPurposeLibTest.c GeneralPurposeLibHostUnitTest.inf <Package>Pkg.dsc # Standard Modules and any Target-Based Test Apps (including in Test/) Future Locations in Consideration \u00b6 We don't know if these types will exist or be applicable yet, but if you write a support library or module that matches the following, please make sure they live in the correct place. Code/Test Location Host-Based Library Implementations Host-Based Implementations of common libraries (eg. MemoryAllocationLibHost) should live in the same package that declares the library interface in its .DEC file in the *Pkg/HostLibrary directory. Should have 'Host' in the name. Host-Based Mocks and Stubs Mock and Stub libraries should live in the UefiTestFrameworkPkg/StubLibrary with either 'Mock' or 'Stub' in the library name. If still in doubt... \u00b6 Hop on GitHub and ask @corthon, @mdkinney, or @spbrogan. ;) Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Modules"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#unit-test-framework-package","text":"","title":"Unit Test Framework Package"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#about","text":"This package adds a unit test framework capable of building tests for multiple contexts including the UEFI shell environment and host-based environments. It allows for unit test development to focus on the tests and leave error logging, result formatting, context persistance, and test running to the framework. The unit test framework works well for low level unit tests as well as system level tests and fits easily in automation frameworks.","title":"About"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#unittestlib","text":"The main \"framework\" library. The core of the framework is the Framework object, which can have any number of test cases and test suites registered with it. The Framework object is also what drives test execution. The Framework also provides helper macros and functions for checking test conditions and reporting errors. Status and error info will be logged into the test context. There are a number of Assert macros that make the unit test code friendly to view and easy to understand. Finally, the Framework also supports logging strings during the test execution. This data is logged to the test context and will be available in the test reporting phase. This should be used for logging test details and helpful messages to resolve test failures.","title":"UnitTestLib"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#unittestpersistencelib","text":"Persistence lib has the main job of saving and restoring test context to a storage medium so that for tests that require exiting the active process and then resuming state can be maintained. This is critical in supporting a system reboot in the middle of a test run.","title":"UnitTestPersistenceLib"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#unittestresultreportlib","text":"Library provides function to run at the end of a framework test run and handles formatting the report. This is a common customization point and allows the unit test framework to fit its output reports into other test infrastructure. In this package a simple library instances has been supplied to output test results to the console as plain text.","title":"UnitTestResultReportLib"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#samples","text":"There is a sample unit test provided as both an example of how to write a unit test and leverage many of the features of the framework. This sample can be found in the Test/UnitTest/Sample/SampleUnitTest directory. The sample is provided in PEI, SMM, DXE, and UEFI App flavors. It also has a flavor for the HOST_APPLICATION build type, which can be run on a host system without needing a target.","title":"Samples"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#usage","text":"This section is built a lot like a \"Getting Started\". We'll go through some of the components that are needed when constructing a unit test and some of the decisions that are made by the test writer. We'll also describe how to check for expected conditions in test cases and a bit of the logging characteristics. Most of these examples will refer to the SampleUnitTestUefiShell app found in this package.","title":"Usage"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#requirements-inf","text":"In our INF file, we'll need to bring in the UnitTestLib library. Conveniently, the interface header for the UnitTestLib is located in MdePkg , so you shouldn't need to depend on any other packages. As long as your DSC file knows where to find the lib implementation that you want to use, you should be good to go. See this example in 'SampleUnitTestUefiShell.inf'... [Packages] MdePkg/MdePkg.dec [LibraryClasses] UefiApplicationEntryPoint BaseLib DebugLib UnitTestLib PrintLib Also, if you want you test to automatically be picked up by the Test Runner plugin, you will need to make sure that the module BASE_NAME contains the word Test ... [Defines] BASE_NAME = SampleUnitTestUefiShell","title":"Requirements - INF"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#requirements-code","text":"Not to state the obvious, but let's make sure we have the following include before getting too far along... #include <Library/UnitTestLib.h> Now that we've got that squared away, let's look at our 'Main()'' routine (or DriverEntryPoint() or whatever).","title":"Requirements - Code"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#configuring-the-framework","text":"Everything in the UnitTestPkg framework is built around an object called -- conveniently -- the Framework. This Framework object will contain all the information about our test, the test suites and test cases associated with it, the current location within the test pass, and any results that have been recorded so far. To get started with a test, we must first create a Framework instance. The function for this is InitUnitTestFramework . It takes in CHAR8 strings for the long name, short name, and test version. The long name and version strings are just for user presentation and relatively flexible. The short name will be used to name any cache files and/or test results, so should be a name that makes sense in that context. These strings are copied internally to the Framework, so using stack-allocated or literal strings is fine. In the 'SampleUnitTestUefiShell' app, the module name is used as the short name, so the init looks like this. DEBUG (( DEBUG_INFO , \"%a v%a \\n \" , UNIT_TEST_APP_NAME , UNIT_TEST_APP_VERSION )); // // Start setting up the test framework for running the tests. // Status = InitUnitTestFramework ( & Framework , UNIT_TEST_APP_NAME , gEfiCallerBaseName , UNIT_TEST_APP_VERSION ); The &Framework returned here is the handle to the Framework. If it's successfully returned, we can start adding test suites and test cases. Test suites exist purely to help organize test cases and to differentiate the results in reports. If you're writing a small unit test, you can conceivably put all test cases into a single suite. However, if you end up with 20+ test cases, it may be beneficial to organize them according to purpose. You must have at least one test suite, even if it's just a catch-all. The function to create a test suite is CreateUnitTestSuite . It takes in a handle to the Framework object, a CHAR8 string for the suite title and package name, and optional function pointers for a setup function and a teardown function. The suite title is for user presentation. The package name is for xUnit type reporting and uses a '.'-separated hierarchical format (see 'SampleUnitTestApp' for example). If provided, the setup and teardown functions will be called once at the start of the suite (before any tests have run) and once at the end of the suite (after all tests have run), respectively. If either or both of these are unneeded, pass NULL . The function prototypes are UNIT_TEST_SUITE_SETUP and UNIT_TEST_SUITE_TEARDOWN . Looking at 'SampleUnitTestUefiShell' app, you can see that the first test suite is created as below... // // Populate the SimpleMathTests Unit Test Suite. // Status = CreateUnitTestSuite ( & SimpleMathTests , Fw , \"Simple Math Tests\" , \"Sample.Math\" , NULL , NULL ); This test suite has no setup or teardown functions. The &SimpleMathTests returned here is a handle to the suite and will be used when adding test cases. Great! Now we've finished some of the cruft, red tape, and busy work. We're ready to add some tests. Adding a test to a test suite is accomplished with the -- you guessed it -- AddTestCase function. It takes in the suite handle; a CHAR8 string for the description and class name; a function pointer for the test case itself; additional, optional function pointers for prerequisite check and cleanup routines; and and optional pointer to a context structure. Okay, that's a lot. Let's take it one piece at a time. The description and class name strings are very similar in usage to the suite title and package name strings in the test suites. The former is for user presentation and the latter is for xUnit parsing. The test case function pointer is what is actually executed as the \"test\" and the prototype should be UNIT_TEST_FUNCTION . The last three parameters require a little bit more explaining. The prerequisite check function has a prototype of UNIT_TEST_PREREQUISITE and -- if provided -- will be called immediately before the test case. If this function returns any error, the test case will not be run and will be recorded as UNIT_TEST_ERROR_PREREQUISITE_NOT_MET . The cleanup function (prototype UNIT_TEST_CLEANUP ) will be called immediately after the test case to provide an opportunity to reset any global state that may have been changed in the test case. In the event of a prerequisite failure, the cleanup function will also be skipped. If either of these functions is not needed, pass NULL . The context pointer is entirely case-specific. It will be passed to the test case upon execution. One of the purposes of the context pointer is to allow test case reuse with different input data. (Another use is for testing that wraps around a system reboot, but that's beyond the scope of this guide.) The test case must know how to interpret the context pointer, so it could be a simple value, or it could be a complex structure. If unneeded, pass NULL . In 'SampleUnitTestUefiShell' app, the first test case is added using the code below... AddTestCase ( SimpleMathTests , \"Adding 1 to 1 should produce 2\" , \"Addition\" , OnePlusOneShouldEqualTwo , NULL , NULL , NULL ); This test case calls the function OnePlusOneShouldEqualTwo and has no prerequisite, cleanup, or context. Once all the suites and cases are added, it's time to run the Framework. // // Execute the tests. // Status = RunAllTestSuites ( Framework );","title":"Configuring the Framework"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#a-simple-test-case","text":"We'll take a look at the below test case from 'SampleUnitTestApp'... UNIT_TEST_STATUS EFIAPI OnePlusOneShouldEqualTwo ( IN UNIT_TEST_FRAMEWORK_HANDLE Framework , IN UNIT_TEST_CONTEXT Context ) { UINTN A , B , C ; A = 1 ; B = 1 ; C = A + B ; UT_ASSERT_EQUAL ( C , 2 ); return UNIT_TEST_PASSED ; } // OnePlusOneShouldEqualTwo() The prototype for this function matches the UNIT_TEST_FUNCTION prototype. It takes in a handle to the Framework itself and the context pointer. The context pointer could be cast and interpreted as anything within this test case, which is why it's important to configure contexts carefully. The test case returns a value of UNIT_TEST_STATUS , which will be recorded in the Framework and reported at the end of all suites. In this test case, the UT_ASSERT_EQUAL assertion is being used to establish that the business logic has functioned correctly. There are several assertion macros, and you are encouraged to use one that matches as closely to your intended test criterium as possible, because the logging is specific to the macro and more specific macros have more detailed logs. When in doubt, there are always UT_ASSERT_TRUE and UT_ASSERT_FALSE . Assertion macros that fail their test criterium will immediately return from the test case with UNIT_TEST_ERROR_TEST_FAILED and log an error string. Note that this early return can have implications for memory leakage. At the end, if all test criteria pass, you should return UNIT_TEST_PASSED .","title":"A Simple Test Case"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#more-complex-cases","text":"To write more advanced tests, first take a look at all the Assertion and Logging macros provided in the framework. Beyond that, if you're writing host-based tests and want to take a dependency on the UnitTestFrameworkPkg, you can leverage the cmocka.h interface and write tests with all the features of the Cmocka framework. Documentation for Cmocka can be found here: https://api.cmocka.org/","title":"More Complex Cases"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#development","text":"When using the EDK2 Pytools for CI testing, the host-based unit tests will be built and run on any build that includes the NOOPT build target. If you are trying to iterate on a single test, a convenient pattern is to build only that test module. For example, the following command will build only the SafeIntLib host-based test from the MdePkg... stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG = VS2017 -p MdePkg -t NOOPT BUILDMODULE = MdePkg/Test/UnitTest/Library/BaseSafeIntLib/TestBaseSafeIntLib.inf","title":"Development"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#known-limitations","text":"","title":"Known Limitations"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#pei-dxe-smm","text":"While sample tests have been provided for these execution environments, only cursory build validation has been performed. Care has been taken while designing the frameworks to allow for execution during boot phases, but only UEFI Shell and host-based tests have been thoroughly evaluated. Full support for PEI, DXE, and SMM is forthcoming, but should be considered beta/staging for now.","title":"PEI, DXE, SMM"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#host-based-support-vs-other-tests","text":"The host-based test framework is powered internally by the Cmocka framework. As such, it has abilities that the target-based tests don't (yet). It would be awesome if this meant that it was a super set of the target-based tests, and it worked just like the target-based tests but with more features. Unfortunately, this is not the case. While care has been taken to keep them as close a possible, there are a few known inconsistencies that we're still ironing out. For example, the logging messages in the target-based tests are cached internally and associated with the running test case. They can be saved later as part of the reporting lib. This isn't currently possible with host-based. Only the assertion failures are logged. We will continue trying to make these as similar as possible.","title":"Host-Based Support vs Other Tests"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#unit-test-locationlayout-rules","text":"Code/Test Location Host-Based Unit Tests for a Library/Protocol/PPI/GUID Interface If what's being tested is an interface (e.g. a library with a public header file, like DebugLib), the test should be scoped to the parent package. Example: MdePkg/Test/UnitTest/[Library/Protocol/Ppi/Guid]/ A real-world example of this is the BaseSafeIntLib test in MdePkg. MdePkg/Test/UnitTest/Library/BaseSafeIntLib/TestBaseSafeIntLibHost.inf Host-Based Unit Tests for a Library/Driver (PEI/DXE/SMM) implementation If what's being tested is a specific implementation (e.g. BaseDebugLibSerialPort for DebugLib), the test should be scoped to the implementation directory itself, in a UnitTest subdirectory. Module Example: MdeModulePkg/Universal/EsrtFmpDxe/UnitTest/ Library Example: MdePkg/Library/BaseMemoryLib/UnitTest/ Host-Based Tests for a Functionality or Feature If you're writing a functional test that operates at the module level (i.e. if it's more than a single file or library), the test should be located in the package-level Tests directory under the HostFuncTest subdirectory. For example, if you were writing a test for the entire FMP Device Framework, you might put your test in: FmpDevicePkg/Test/HostFuncTest/FmpDeviceFramework If the feature spans multiple packages, it's location should be determined by the package owners related to the feature. Non-Host-Based (PEI/DXE/SMM/Shell) Tests for a Functionality or Feature Similar to Host-Based, if the feature is in one package, should be located in the *Pkg/Test/[Shell/Dxe/Smm/Pei]Test directory. If the feature spans multiple packages, it's location should be determined by the package owners related to the feature. USAGE EXAMPLES PEI Example: MP_SERVICE_PPI. Or check MTRR configuration in a notification function. SMM Example: a test in a protocol callback function. (It is different with the solution that SmmAgent+ShellApp) DXE Example: a test in a UEFI event call back to check SPI/SMRAM status. Shell Example: the SMM handler audit test has a shell-based app that interacts with an SMM handler to get information. The SMM paging audit test gathers information about both DXE and SMM. And the SMM paging functional test actually forces errors into SMM via a DXE driver.","title":"Unit Test Location/Layout Rules"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#example-directory-tree","text":"<PackageName>Pkg/ ComponentY/ ComponentY.inf ComponentY.c UnitTest/ ComponentYHostUnitTest.inf # Host-Based Test for Driver Module ComponentYUnitTest.c Library/ GeneralPurposeLibBase/ ... GeneralPurposeLibSerial/ ... SpecificLibDxe/ SpecificLibDxe.c SpecificLibDxe.inf UnitTest/ # Host-Based Test for Specific Library Implementation SpecificLibDxeHostUnitTest.c SpecificLibDxeHostUnitTest.inf Test/ <Package>HostTest.dsc # Host-Based Test Apps UnitTest/ InterfaceX InterfaceXHostUnitTest.inf # Host-Based App (should be in Test/<Package>HostTest.dsc) InterfaceXPeiUnitTest.inf # PEIM Target-Based Test (if applicable) InterfaceXDxeUnitTest.inf # DXE Target-Based Test (if applicable) InterfaceXSmmUnitTest.inf # SMM Target-Based Test (if applicable) InterfaceXShellUnitTest.inf # Shell App Target-Based Test (if applicable) InterfaceXUnitTest.c # Test Logic GeneralPurposeLib/ # Host-Based Test for any implementation of GeneralPurposeLib GeneralPurposeLibTest.c GeneralPurposeLibHostUnitTest.inf <Package>Pkg.dsc # Standard Modules and any Target-Based Test Apps (including in Test/)","title":"Example Directory Tree"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#future-locations-in-consideration","text":"We don't know if these types will exist or be applicable yet, but if you write a support library or module that matches the following, please make sure they live in the correct place. Code/Test Location Host-Based Library Implementations Host-Based Implementations of common libraries (eg. MemoryAllocationLibHost) should live in the same package that declares the library interface in its .DEC file in the *Pkg/HostLibrary directory. Should have 'Host' in the name. Host-Based Mocks and Stubs Mock and Stub libraries should live in the UefiTestFrameworkPkg/StubLibrary with either 'Mock' or 'Stub' in the library name.","title":"Future Locations in Consideration"},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#if-still-in-doubt","text":"Hop on GitHub and ask @corthon, @mdkinney, or @spbrogan. ;)","title":"If still in doubt..."},{"location":"dyn/mu_basecore/UnitTestFrameworkPkg/ReadMe/#copyright","text":"Copyright \u00a9 Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/","text":"Azure DevOps Pipelines \u00b6 These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution. Core CI \u00b6 Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details Platform CI \u00b6 Focused on building a single target platform and confirming functionality on that platform. Conventions \u00b6 Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml Links \u00b6 Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library Lessons Learned \u00b6 Templates and parameters \u00b6 They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late. File matching patterns \u00b6 On Linux this can hang if there are too many files in the search list. Templates and file splitting \u00b6 Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs. Conditional steps \u00b6 If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Read Me"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#azure-devops-pipelines","text":"These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution.","title":"Azure DevOps Pipelines"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#core-ci","text":"Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details","title":"Core CI"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#platform-ci","text":"Focused on building a single target platform and confirming functionality on that platform.","title":"Platform CI"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#conventions","text":"Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml","title":"Conventions"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#links","text":"Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library","title":"Links"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#lessons-learned","text":"","title":"Lessons Learned"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#templates-and-parameters","text":"They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late.","title":"Templates and parameters"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#file-matching-patterns","text":"On Linux this can hang if there are too many files in the search list.","title":"File matching patterns"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#templates-and-file-splitting","text":"Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs.","title":"Templates and file splitting"},{"location":"dyn/mu_basecore/azurepipelines/ReadMe/#conditional-steps","text":"If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Conditional steps"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/","text":"CI Templates \u00b6 This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation. Common CI templates \u00b6 basetools-build-steps.yml \u00b6 This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds. Core CI templates \u00b6 pr-gate-build-job.yml \u00b6 This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI. pr-gate-steps.yml \u00b6 This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\" spell-check-prereq-steps.yml \u00b6 This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. Platform CI templates \u00b6 platform-build-run-steps.yml \u00b6 This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools Special Notes \u00b6 For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"templates"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#ci-templates","text":"This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation.","title":"CI Templates"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#common-ci-templates","text":"","title":"Common CI templates"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#basetools-build-stepsyml","text":"This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds.","title":"basetools-build-steps.yml"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#core-ci-templates","text":"","title":"Core CI templates"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#pr-gate-build-jobyml","text":"This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI.","title":"pr-gate-build-job.yml"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#pr-gate-stepsyml","text":"This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\"","title":"pr-gate-steps.yml"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#spell-check-prereq-stepsyml","text":"This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0.","title":"spell-check-prereq-steps.yml"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#platform-ci-templates","text":"","title":"Platform CI templates"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#platform-build-run-stepsyml","text":"This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools","title":"platform-build-run-steps.yml"},{"location":"dyn/mu_basecore/azurepipelines/templates/ReadMe/#special-notes","text":"For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"Special Notes"},{"location":"dyn/mu_basecore/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues MdeModulePkg DxeIpl dependency on ArmPkg, Depends on StandaloneMmPkg, Spell checking in audit mode MdePkg Spell checking in audit mode NetworkPkg Spell checking in audit mode PcAtChipsetPkg SecurityPkg Spell checking in audit mode StandaloneMmPkg UefiCpuPkg Spell checking in audit mode, 2 binary modules not being built by DSC UnitTestFrameworkPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment Host Module Inclusion Test - HostUnitTestDscCompleteCheck \u00b6 This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin \u00b6 A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way... Including Host-Based Tests in the Package YAML \u00b6 For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided. Configuring the HostBased DSC \u00b6 The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"Readme"},{"location":"dyn/mu_basecore/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_basecore/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues MdeModulePkg DxeIpl dependency on ArmPkg, Depends on StandaloneMmPkg, Spell checking in audit mode MdePkg Spell checking in audit mode NetworkPkg Spell checking in audit mode PcAtChipsetPkg SecurityPkg Spell checking in audit mode StandaloneMmPkg UefiCpuPkg Spell checking in audit mode, 2 binary modules not being built by DSC UnitTestFrameworkPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_basecore/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_basecore/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_basecore/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_basecore/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_basecore/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_basecore/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#host-module-inclusion-test-hostunittestdsccompletecheck","text":"This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment","title":"Host Module Inclusion Test - HostUnitTestDscCompleteCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_basecore/pytool/Readme/#host-unit-test-compilation-and-run-test-hostunittestcompilerplugin","text":"A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way...","title":"Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin"},{"location":"dyn/mu_basecore/pytool/Readme/#including-host-based-tests-in-the-package-yaml","text":"For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided.","title":"Including Host-Based Tests in the Package YAML"},{"location":"dyn/mu_basecore/pytool/Readme/#configuring-the-hostbased-dsc","text":"The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION.","title":"Configuring the HostBased DSC"},{"location":"dyn/mu_basecore/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_basecore/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows","title":"PyTool Scopes"},{"location":"dyn/mu_basecore/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"Future investments"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/","text":"Character Encoding Check Plugin \u00b6 This CiBuildPlugin scans all the files in a package to make sure each file is correctly encoded and all characters can be read. Improper encoding causes tools to fail in some situations especially in different locals. Configuration \u00b6 The plugin can be configured to ignore certain files. \"CharEncodingCheck\" : { \"IgnoreFiles\" : [] } IgnoreFiles \u00b6 OPTIONAL List of file to ignore.","title":"Char Encoding Check"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/#character-encoding-check-plugin","text":"This CiBuildPlugin scans all the files in a package to make sure each file is correctly encoded and all characters can be read. Improper encoding causes tools to fail in some situations especially in different locals.","title":"Character Encoding Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/#configuration","text":"The plugin can be configured to ignore certain files. \"CharEncodingCheck\" : { \"IgnoreFiles\" : [] }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/#ignorefiles","text":"OPTIONAL List of file to ignore.","title":"IgnoreFiles"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/","text":"Compiler Plugin \u00b6 This CiBuildPlugin compiles the package DSC from the package being tested. Configuration \u00b6 The package relative path of the DSC file to build. \"CompilerPlugin\" : { \"DscPath\" : \"<path to dsc from root of pkg>\" } DscPath \u00b6 Package relative path to the DSC file to build.","title":"Compiler Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/#compiler-plugin","text":"This CiBuildPlugin compiles the package DSC from the package being tested.","title":"Compiler Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/#configuration","text":"The package relative path of the DSC file to build. \"CompilerPlugin\" : { \"DscPath\" : \"<path to dsc from root of pkg>\" }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/#dscpath","text":"Package relative path to the DSC file to build.","title":"DscPath"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/","text":"Depdendency Check Plugin \u00b6 A CiBuildPlugin that finds all modules (inf files) in a package and reviews the packages used to confirm they are acceptable. This is to help enforce layering and identify improper dependencies between packages. Configuration \u00b6 The plugin must be configured with the acceptabe package dependencies for the package. \"DependencyCheck\" : { \"AcceptableDependencies\" : [], \"AcceptableDependencies-<MODULE_TYPE>\" : [], \"IgnoreInf\" : [] } AcceptableDependencies \u00b6 Package dec files that are allowed in all INFs. Example: MdePkg/MdePkg.dec AcceptableDependencies- \u00b6 OPTIONAL Package dependencies for INFs that have module type . Example: AcceptableDependencies-HOST_APPLICATION. IgnoreInf \u00b6 OPTIONAL list of INFs to ignore for this dependency check.","title":"Dependency Check"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#depdendency-check-plugin","text":"A CiBuildPlugin that finds all modules (inf files) in a package and reviews the packages used to confirm they are acceptable. This is to help enforce layering and identify improper dependencies between packages.","title":"Depdendency Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#configuration","text":"The plugin must be configured with the acceptabe package dependencies for the package. \"DependencyCheck\" : { \"AcceptableDependencies\" : [], \"AcceptableDependencies-<MODULE_TYPE>\" : [], \"IgnoreInf\" : [] }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#acceptabledependencies","text":"Package dec files that are allowed in all INFs. Example: MdePkg/MdePkg.dec","title":"AcceptableDependencies"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#acceptabledependencies-","text":"OPTIONAL Package dependencies for INFs that have module type . Example: AcceptableDependencies-HOST_APPLICATION.","title":"AcceptableDependencies-"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#ignoreinf","text":"OPTIONAL list of INFs to ignore for this dependency check.","title":"IgnoreInf"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/","text":"Dsc Complete Check Plugin \u00b6 This CiBuildPlugin scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"DscCompleteCheck\" : { \"DscPath\" : \"\" , # Path to dsc from root of package \"IgnoreInf\" : [] # Ignore INF if found in filesystem but not dsc } DscPath \u00b6 Path to DSC to consider platform dsc IgnoreInf \u00b6 Ignore error if Inf file is not listed in DSC file","title":"Dsc Complete Check"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#dsc-complete-check-plugin","text":"This CiBuildPlugin scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment","title":"Dsc Complete Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"DscCompleteCheck\" : { \"DscPath\" : \"\" , # Path to dsc from root of package \"IgnoreInf\" : [] # Ignore INF if found in filesystem but not dsc }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#dscpath","text":"Path to DSC to consider platform dsc","title":"DscPath"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#ignoreinf","text":"Ignore error if Inf file is not listed in DSC file","title":"IgnoreInf"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/","text":"Guid Check Plugin \u00b6 This CiBuildPlugin scans all the files in a code tree to find all the GUID definitions. After collection it will then look for duplication in the package under test. Uniqueness of all GUIDs are critical within the UEFI environment. Duplication can cause numerous issues including locating the wrong data structure, calling the wrong function, or decoding the wrong data members. Currently Scanned: INF files are scanned for there Module guid DEC files are scanned for all of their Protocols, PPIs, and Guids as well as the one package GUID. Any GUID value being equal to two names or even just defined in two files is considered an error unless in the ignore list. Any GUID name that is found more than once is an error unless all occurrences are Module GUIDs. Since the Module GUID is assigned to the Module name it is common to have numerous versions of the same module named the same. Configuration \u00b6 The plugin has numerous configuration options to support the UEFI codebase. \"GuidCheck\" : { \"IgnoreGuidName\" : [], \"IgnoreGuidValue\" : [], \"IgnoreFoldersAndFiles\" : [], \"IgnoreDuplicates\" : [] } IgnoreGuidName \u00b6 This list allows strings in two formats. GuidName This will remove any entry with this GuidName from the list of GUIDs therefore ignoring any error associated with this name. GuidName=GuidValue This will also ignore the GUID by name but only if the value equals the GuidValue. GuidValue should be in registry format. This is the suggested format to use as it will limit the ignore to only the defined case. IgnoreGuidValue \u00b6 This list allows strings in guid registry format GuidValue . This will remove any entry with this GuidValue from the list of GUIDs therefore ignoring any error associated with this value. GuidValue must be in registry format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx IgnoreFoldersAndFiles \u00b6 This supports .gitignore file and folder matching strings including wildcards Any folder or file ignored will not be parsed and therefore any GUID defined will be ignored. The plugin will always ignores the following [\"/Build\", \"/Conf\"] IgnoreDuplicates \u00b6 This supports strings in the format of GuidName = GuidName = GuidName For the error with the GuidNames to be ignored the list must match completely with what is found during the code scan. For example if there are two GUIDs that are by design equal within the code tree then it should be GuidName = GuidName If instead there are three GUIDs then it must be GuidName = GuidName = GuidName This is the best ignore list to use because it is the most strict and will catch new problems when new conflicts are introduced. There are numerous places in the UEFI specification in which two GUID names are assigned the same value. These names should be set in this ignore list so that they don't cause an error but any additional duplication would still be caught.","title":"Guid Check"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#guid-check-plugin","text":"This CiBuildPlugin scans all the files in a code tree to find all the GUID definitions. After collection it will then look for duplication in the package under test. Uniqueness of all GUIDs are critical within the UEFI environment. Duplication can cause numerous issues including locating the wrong data structure, calling the wrong function, or decoding the wrong data members. Currently Scanned: INF files are scanned for there Module guid DEC files are scanned for all of their Protocols, PPIs, and Guids as well as the one package GUID. Any GUID value being equal to two names or even just defined in two files is considered an error unless in the ignore list. Any GUID name that is found more than once is an error unless all occurrences are Module GUIDs. Since the Module GUID is assigned to the Module name it is common to have numerous versions of the same module named the same.","title":"Guid Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#configuration","text":"The plugin has numerous configuration options to support the UEFI codebase. \"GuidCheck\" : { \"IgnoreGuidName\" : [], \"IgnoreGuidValue\" : [], \"IgnoreFoldersAndFiles\" : [], \"IgnoreDuplicates\" : [] }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignoreguidname","text":"This list allows strings in two formats. GuidName This will remove any entry with this GuidName from the list of GUIDs therefore ignoring any error associated with this name. GuidName=GuidValue This will also ignore the GUID by name but only if the value equals the GuidValue. GuidValue should be in registry format. This is the suggested format to use as it will limit the ignore to only the defined case.","title":"IgnoreGuidName"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignoreguidvalue","text":"This list allows strings in guid registry format GuidValue . This will remove any entry with this GuidValue from the list of GUIDs therefore ignoring any error associated with this value. GuidValue must be in registry format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx","title":"IgnoreGuidValue"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignorefoldersandfiles","text":"This supports .gitignore file and folder matching strings including wildcards Any folder or file ignored will not be parsed and therefore any GUID defined will be ignored. The plugin will always ignores the following [\"/Build\", \"/Conf\"]","title":"IgnoreFoldersAndFiles"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignoreduplicates","text":"This supports strings in the format of GuidName = GuidName = GuidName For the error with the GuidNames to be ignored the list must match completely with what is found during the code scan. For example if there are two GUIDs that are by design equal within the code tree then it should be GuidName = GuidName If instead there are three GUIDs then it must be GuidName = GuidName = GuidName This is the best ignore list to use because it is the most strict and will catch new problems when new conflicts are introduced. There are numerous places in the UEFI specification in which two GUID names are assigned the same value. These names should be set in this ignore list so that they don't cause an error but any additional duplication would still be caught.","title":"IgnoreDuplicates"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestCompilerPlugin/Readme/","text":"Host UnitTest Compiler Plugin \u00b6 A CiBuildPlugin that compiles the dsc for host based unit test apps. An IUefiBuildPlugin may be attached to this plugin that will run the unit tests and collect the results after successful compilation. Configuration \u00b6 The package relative path of the DSC file to build. \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"<path to dsc from root of pkg>\" } DscPath \u00b6 Package relative path to the DSC file to build. Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Host Unit Test Compiler Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestCompilerPlugin/Readme/#host-unittest-compiler-plugin","text":"A CiBuildPlugin that compiles the dsc for host based unit test apps. An IUefiBuildPlugin may be attached to this plugin that will run the unit tests and collect the results after successful compilation.","title":"Host UnitTest Compiler Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestCompilerPlugin/Readme/#configuration","text":"The package relative path of the DSC file to build. \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"<path to dsc from root of pkg>\" }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestCompilerPlugin/Readme/#dscpath","text":"Package relative path to the DSC file to build.","title":"DscPath"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestCompilerPlugin/Readme/#copyright","text":"Copyright \u00a9 Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestDscCompleteCheck/Readme/","text":"Host Unit Test Dsc Complete Check Plugin \u00b6 This CiBuildPlugin scans all INF files from a package for those related to host based unit tests confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance supports the HOST_APPLICATION environment Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"HostUnitTestDscCompleteCheck\" : { \"DscPath\" : \"\" , # Path to Host based unit test DSC file \"IgnoreInf\" : [] # Ignore INF if found in filesystem but not dsc } DscPath \u00b6 Path to DSC to consider platform dsc IgnoreInf \u00b6 Ignore error if Inf file is not listed in DSC file","title":"Host Unit Test Dsc Complete Check"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestDscCompleteCheck/Readme/#host-unit-test-dsc-complete-check-plugin","text":"This CiBuildPlugin scans all INF files from a package for those related to host based unit tests confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance supports the HOST_APPLICATION environment","title":"Host Unit Test Dsc Complete Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestDscCompleteCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"HostUnitTestDscCompleteCheck\" : { \"DscPath\" : \"\" , # Path to Host based unit test DSC file \"IgnoreInf\" : [] # Ignore INF if found in filesystem but not dsc }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestDscCompleteCheck/Readme/#dscpath","text":"Path to DSC to consider platform dsc","title":"DscPath"},{"location":"dyn/mu_basecore/pytool/Plugin/HostUnitTestDscCompleteCheck/Readme/#ignoreinf","text":"Ignore error if Inf file is not listed in DSC file","title":"IgnoreInf"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/","text":"Library Class Check Plugin \u00b6 This CiBuildPlugin scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"LibraryClassCheck\" : { IgnoreHeaderFile : [], # Ignore a file found on disk IgnoreLibraryClass : [] # Ignore a declaration found in dec file } IgnoreHeaderFile \u00b6 Ignore a file found on disk IgnoreLibraryClass \u00b6 Ignore a declaration found in dec file","title":"Library Class Check"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#library-class-check-plugin","text":"This CiBuildPlugin scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Class Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"LibraryClassCheck\" : { IgnoreHeaderFile : [], # Ignore a file found on disk IgnoreLibraryClass : [] # Ignore a declaration found in dec file }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#ignoreheaderfile","text":"Ignore a file found on disk","title":"IgnoreHeaderFile"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#ignorelibraryclass","text":"Ignore a declaration found in dec file","title":"IgnoreLibraryClass"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/","text":"Spell Check Plugin \u00b6 This CiBuildPlugin scans all the files in a given package and checks for spelling errors. This plugin requires NodeJs and cspell. If the plugin doesn't find its required tools then it will mark the test as skipped. NodeJS: https://nodejs.org/en/ cspell: https://www.npmjs.com/package/cspell Src and doc available: https://github.com/streetsidesoftware/cspell Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"SpellCheck\" : { \"AuditOnly\" : False , # If True, log all errors and then mark as skipped \"IgnoreFiles\" : [], # use gitignore syntax to ignore errors in matching files \"ExtendWords\" : [], # words to extend to the dictionary for this package \"IgnoreStandardPaths\" : [], # Standard Plugin defined paths that should be ignore \"AdditionalIncludePaths\" : [] # Additional paths to spell check (wildcards supported) } AuditOnly \u00b6 Boolean - Default is False. If True run the test in an Audit only mode which will log all errors but instead of failing the build it will set the test as skipped. This allows visibility into the failures without breaking the build. IgnoreFiles \u00b6 This supports .gitignore file and folder matching strings including wildcards All files will be parsed regardless but then any spelling errors found within ignored files will not be reported as an error. Errors in ignored files will still be output to the test results as informational comments. ExtendWords \u00b6 This list allows words to be added to the dictionary for the spell checker when this package is tested. These follow the rules of the cspell config words field. IgnoreStandardPaths \u00b6 This plugin by default will check the below standard paths. If the package would like to ignore any of them list that here. [ # C source \"*.c\" , \"*.h\" , # Assembly files \"*.nasm\" , \"*.asm\" , \"*.masm\" , \"*.s\" , # ACPI source language \"*.asl\" , # Edk2 build files \"*.dsc\" , \"*.dec\" , \"*.fdf\" , \"*.inf\" , # Documentation files \"*.md\" , \"*.txt\" ] AdditionalIncludePaths \u00b6 If the package would to add additional path patterns to be included in spellchecking they can be defined here. Other configuration \u00b6 In the cspell.base.json there are numerous other settings configured. There is no support to override these on a per package basis but future features could make this available. One interesting configuration option is minWordLength . Currently it is set to 5 which means all 2,3, and 4 letter words will be ignored. This helps minimize the number of technical acronyms, register names, and other UEFI specific values that must be ignored. False positives \u00b6 The cspell dictionary is not perfect and there are cases where technical words or acronyms are not found in the dictionary. There are three ways to resolve false positives and the choice for which method should be based on how broadly the word should be accepted. CSpell Base Config file \u00b6 If the change should apply to all UEFI code and documentation then it should be added to the base config file words section. The base config file is adjacent to this file and titled cspell.base.json . This is a list of accepted words for all spell checking operations on all packages. Package Config \u00b6 In the package *.ci.yaml file there is a SpellCheck config section. This section allows files to be ignored as well as words that should be considered valid for all files within this package. Add the desired words to the \"ExtendedWords\" member. In-line File \u00b6 CSpell supports numerous methods to annotate your files to ignore words, sections, etc. This can be found in CSpell documentation. Suggestion here is to use a c-style comment at the top of the file to add words that should be ignored just for this file. Obviously this has the highest maintenance cost so it should only be used for file unique words. // spell-checker:ignore unenroll, word2, word3 or # spell-checker:ignore unenroll, word2, word3","title":"Spell Check"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#spell-check-plugin","text":"This CiBuildPlugin scans all the files in a given package and checks for spelling errors. This plugin requires NodeJs and cspell. If the plugin doesn't find its required tools then it will mark the test as skipped. NodeJS: https://nodejs.org/en/ cspell: https://www.npmjs.com/package/cspell Src and doc available: https://github.com/streetsidesoftware/cspell","title":"Spell Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"SpellCheck\" : { \"AuditOnly\" : False , # If True, log all errors and then mark as skipped \"IgnoreFiles\" : [], # use gitignore syntax to ignore errors in matching files \"ExtendWords\" : [], # words to extend to the dictionary for this package \"IgnoreStandardPaths\" : [], # Standard Plugin defined paths that should be ignore \"AdditionalIncludePaths\" : [] # Additional paths to spell check (wildcards supported) }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#auditonly","text":"Boolean - Default is False. If True run the test in an Audit only mode which will log all errors but instead of failing the build it will set the test as skipped. This allows visibility into the failures without breaking the build.","title":"AuditOnly"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#ignorefiles","text":"This supports .gitignore file and folder matching strings including wildcards All files will be parsed regardless but then any spelling errors found within ignored files will not be reported as an error. Errors in ignored files will still be output to the test results as informational comments.","title":"IgnoreFiles"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#extendwords","text":"This list allows words to be added to the dictionary for the spell checker when this package is tested. These follow the rules of the cspell config words field.","title":"ExtendWords"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#ignorestandardpaths","text":"This plugin by default will check the below standard paths. If the package would like to ignore any of them list that here. [ # C source \"*.c\" , \"*.h\" , # Assembly files \"*.nasm\" , \"*.asm\" , \"*.masm\" , \"*.s\" , # ACPI source language \"*.asl\" , # Edk2 build files \"*.dsc\" , \"*.dec\" , \"*.fdf\" , \"*.inf\" , # Documentation files \"*.md\" , \"*.txt\" ]","title":"IgnoreStandardPaths"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#additionalincludepaths","text":"If the package would to add additional path patterns to be included in spellchecking they can be defined here.","title":"AdditionalIncludePaths"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#other-configuration","text":"In the cspell.base.json there are numerous other settings configured. There is no support to override these on a per package basis but future features could make this available. One interesting configuration option is minWordLength . Currently it is set to 5 which means all 2,3, and 4 letter words will be ignored. This helps minimize the number of technical acronyms, register names, and other UEFI specific values that must be ignored.","title":"Other configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#false-positives","text":"The cspell dictionary is not perfect and there are cases where technical words or acronyms are not found in the dictionary. There are three ways to resolve false positives and the choice for which method should be based on how broadly the word should be accepted.","title":"False positives"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#cspell-base-config-file","text":"If the change should apply to all UEFI code and documentation then it should be added to the base config file words section. The base config file is adjacent to this file and titled cspell.base.json . This is a list of accepted words for all spell checking operations on all packages.","title":"CSpell Base Config file"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#package-config","text":"In the package *.ci.yaml file there is a SpellCheck config section. This section allows files to be ignored as well as words that should be considered valid for all files within this package. Add the desired words to the \"ExtendedWords\" member.","title":"Package Config"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#in-line-file","text":"CSpell supports numerous methods to annotate your files to ignore words, sections, etc. This can be found in CSpell documentation. Suggestion here is to use a c-style comment at the top of the file to add words that should be ignored just for this file. Obviously this has the highest maintenance cost so it should only be used for file unique words. // spell-checker:ignore unenroll, word2, word3 or # spell-checker:ignore unenroll, word2, word3","title":"In-line File"},{"location":"dyn/mu_oem_sample/RepoDetails/","text":"Project Mu Oem Sample Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_oem_sample.git Branch: release/202005 Commit: 4cfb521bfce50f1977d8fcbc6d58bf4eae91fa0e Commit Date: 2020-07-01 15:21:41 -0700 This repository is considered sample code for any entity building devices using Project Mu. It is likely that any device manufacturer will want to customize the device behavior by changing the modules in this package. Numerous libraries to support UEFI Boot Device Selection phase (BDS) Firmware Version information UI App / \"Frontpage\" application support as well as example More Info \u00b6 FrontpageDsc and FrontpageFdf that can be included so you don't have to unravel all of the libraries and protocols that are required to get started with FrontPage. A brief description of each MS component was added to FrontPageDsc to help explain how each piece of the puzzle fits in. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Per Platform Libraries \u00b6 MsPlatformDevicesLib, DfciDeviceIdSupportLib, PlatformThemeLib. These three libraries need to be implemented per platform. An example can be found in the NXP iMX8 platform . Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements PR-Gate Builds \u00b6 pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json Copyright & License \u00b6 Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_oem_sample/RepoDetails/#project-mu-oem-sample-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_oem_sample.git Branch: release/202005 Commit: 4cfb521bfce50f1977d8fcbc6d58bf4eae91fa0e Commit Date: 2020-07-01 15:21:41 -0700 This repository is considered sample code for any entity building devices using Project Mu. It is likely that any device manufacturer will want to customize the device behavior by changing the modules in this package. Numerous libraries to support UEFI Boot Device Selection phase (BDS) Firmware Version information UI App / \"Frontpage\" application support as well as example","title":"Project Mu Oem Sample Repository"},{"location":"dyn/mu_oem_sample/RepoDetails/#more-info","text":"FrontpageDsc and FrontpageFdf that can be included so you don't have to unravel all of the libraries and protocols that are required to get started with FrontPage. A brief description of each MS component was added to FrontPageDsc to help explain how each piece of the puzzle fits in. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_oem_sample/RepoDetails/#per-platform-libraries","text":"MsPlatformDevicesLib, DfciDeviceIdSupportLib, PlatformThemeLib. These three libraries need to be implemented per platform. An example can be found in the NXP iMX8 platform .","title":"Per Platform Libraries"},{"location":"dyn/mu_oem_sample/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_oem_sample/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_oem_sample/RepoDetails/#pr-gate-builds","text":"pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json","title":"PR-Gate Builds"},{"location":"dyn/mu_oem_sample/RepoDetails/#copyright-license","text":"Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Copyright &amp; License"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/","text":"FrontPage Password Support \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 OemPkg FrontPage application is a sample UEFI UI app that among other features supports password authentication. Password Architecture \u00b6 Below architecture diagram shows how the password support is integrated into the FrontPage application. Setting a Password \u00b6 FrontPage links to PasswordPolicyLib in order to validate the user-provided password and then to create a hash out of the password. Then, the Settings Access Protocol is used to save the hash. DfciPasswordProvider is linked to DfciPkg SettingsManagerDxe to register the password setting with the Settings Access Protocol. DfciPasswordProvider links to PasswordStoreLib to set the password via PasswordStoreSetPassword. Authenticating a Password \u00b6 FrontPage uses the DFCI Authentication Protocol to authenticate a password via the AuthWithPW interface and acquire an authentication token. DFCI Authentication Protocol is installed by DFCI Identity and Auth Manager, which uses PasswordStoreLib to check whether a password is set, and if set, then to authenticate the password.","title":"Front Page"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#frontpage-password-support","text":"","title":"FrontPage Password Support"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#about","text":"OemPkg FrontPage application is a sample UEFI UI app that among other features supports password authentication.","title":"About"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#password-architecture","text":"Below architecture diagram shows how the password support is integrated into the FrontPage application.","title":"Password Architecture"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#setting-a-password","text":"FrontPage links to PasswordPolicyLib in order to validate the user-provided password and then to create a hash out of the password. Then, the Settings Access Protocol is used to save the hash. DfciPasswordProvider is linked to DfciPkg SettingsManagerDxe to register the password setting with the Settings Access Protocol. DfciPasswordProvider links to PasswordStoreLib to set the password via PasswordStoreSetPassword.","title":"Setting a Password"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#authenticating-a-password","text":"FrontPage uses the DFCI Authentication Protocol to authenticate a password via the AuthWithPW interface and acquire an authentication token. DFCI Authentication Protocol is installed by DFCI Identity and Auth Manager, which uses PasswordStoreLib to check whether a password is set, and if set, then to authenticate the password.","title":"Authenticating a Password"},{"location":"dyn/mu_plus/RepoDetails/","text":"Project Mu Common Plus \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_plus.git Branch: release/202005 Commit: febadbea9b24b221dbffd410f0361bb7d73f13f0 Commit Date: 2020-08-08 05:35:51 +0000 About \u00b6 This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. For full documentation. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Repo Details"},{"location":"dyn/mu_plus/RepoDetails/#project-mu-common-plus","text":"Git Details Repository Url: https://github.com/Microsoft/mu_plus.git Branch: release/202005 Commit: febadbea9b24b221dbffd410f0361bb7d73f13f0 Commit Date: 2020-08-08 05:35:51 +0000","title":"Project Mu Common Plus"},{"location":"dyn/mu_plus/RepoDetails/#about","text":"This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. For full documentation. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"About"},{"location":"dyn/mu_plus/RepoDetails/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/","text":"AdvLoggerPkg - Advanced Logger Package \u00b6 About \u00b6 This package contains the various libraries to have in memory logging. Configuration \u00b6 The following configurations are supported: Phase Usage DXE Only Uses DxeCore, DxeRuntime, and Dxe AdvancedLoggerLib libraries for logging from start of DXE CORE through Exit Boot Services. Accepts the PEI Advanced Logger Hob if one is generated. Produces the AdvancedLogger protocol. DXE+SMM Requires DXE modules above, and adds the Smm AdvancedLoggerLib library. Collects SMM generated messages in the in memory log PEI Uses PeiCore and Pei AdvancedLoggerLib libraries. Creates the Advanced Logger Hob if PcdAdvancedLoggerPeiInRAM is set. SEC Uses the Sec Advanced Logger Library. SEC requires a fixed load address, so it piggy backs on the Temporary RAM PCD information. Produces a Fixed Address temporary RAM log. When memory is added, the Sec Advanced Logger library converts the Temporary RAM logging information to the PEI Advanced Logger Hob. PEI64 Uses Pei64 Advanced Logger Library. Requires the SEC fixed address temporary log information in order to log Pei64 bit DEBUG messages. PCD's used by Advanced Logger PCD Function of the PCD PcdAdvancedLoggerForceEnable The default operation is to check if a Logs directory is present in the root of the filesystem. If the logs directory is present, logging is enabled. When PcdAdvancedLoggerForceEnable is TRUE, and the device is not a USB device, a Logs directory will be created and logging is enabled. When logging is enabled, the proper log files will be created if not already preset. PcdAdvancedLoggerPeiInRAM For system that have memory at PeiCore entry. The full in memory log buffer if PcdAdvancedLoggerPages is allocated in the Pei Core constructor and PcdAdvancedLoggerPreMemPages is ignored. PcdAdvancedSerialLoggerDebugPrintErrorLevel The standard debug flags filter which log messages are produced. This PCD allow a subset of log messages to be forwarded to the Serial Port Lib. PcdAdvancedSerialLoggerDisable Specifies when to disable writing to the serial port. PcdAdvancedLoggerPreMemPages Amount of temporary RAM used for the debug log. PcdAdvancedLoggerPages Amount of system RAM used for the debug log PcdAdvancedLoggerLocator When enabled, the AdvLogger creates a variable \"AdvLoggerLocator\" with the address of the LoggerInfo buffer Libraries \u00b6 The following libraries are used with AdvancedLogger: Library Function of the Library AdvancedLoggerAccessLib Used to access the memory log - used by FileLogger and Serial/Dxe/Logger AdvancedLoggerLib One per module type - used to provide access to the in memory log buffer AdvLoggerSmmAccessLib Used to intercept GetVariable in order to provide an OS utility the ability to read the log BaseDebugLibAdvancedLogger Basic Dxe etc DebugLib DebugAgent Used to intercept SEC initialization PeiDebugLibAdvancedLogger Basic Pei DebugLib Platform note: \u00b6 The SEC version of the Advanced Logger uses the temporary RAM block. This block is fixed in size and location, and these need to be adjusted to make room for the Advanced Logger buffer. There may be cases where the processor cache size is too small to enable the Advanced Logger during SEC. The following changes are needed in the .dsc [ LibraryClasses.common ] DebugLib|AdvLoggerPkg/Library/BaseDebugLibAdvancedLogger/BaseDebugLibAdvancedLogger.inf [LibraryClasses.IA32.SEC] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Sec/AdvancedLoggerLib.inf DebugAgentLib|AdvLoggerPkg/Library/DebugAgent/Sec/AdvancedLoggerSecDebugAgent.inf [LibraryClasses.IA32.PEI_CORE] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/PeiCore/AdvancedLoggerLib.inf [LibraryClasses.IA32.PEIM] DebugLib|AdvLoggerPkg/Library/PeiDebugLibAdvancedLogger/PeiDebugLibAdvancedLogger.inf [LibraryClasses.X64.PEIM] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Pei64/AdvancedLoggerLib.inf [LibraryClasses.X64] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Dxe/AdvancedLoggerLib.inf AdvancedLoggerAccessLib|AdvLoggerPkg/Library/AdvancedLoggerAccessLib/AdvancedLoggerAccessLib.inf [LibraryClasses.X64.DXE_CORE] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/DxeCore/AdvancedLoggerLib.inf [LibraryClasses.X64.DXE_SMM_DRIVER] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Smm/AdvancedLoggerLib.inf [LibraryClasses.X64.SMM_CORE] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Smm/AdvancedLoggerLib.inf [LibraryClasses.X64.DXE_RUNTIME_DRIVER] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Runtime/AdvancedLoggerLib.inf [PcdsFeatureFlag] ## Build Example if you build environment differentiates customer builds from internal test builds !if $(SHIP_MODE) == FALSE gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerForceEnable|TRUE gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerLocator|TRUE !else gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerForceEnable|FALSE gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerLocator|TRUE !endif The following changes should be in the family .dsc where the processor specific changes are specified [ PcdsFixedAtBuild.common ] gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedLoggerPreMemPages|24 Advanced File Logger \u00b6 The Advanced File Logger monitors for file systems mounted during boot. When an eligible file system is detected, the log is flushed to the file system. The log is flushed if the system is reset during POST, and at Exit Boot Services. An eligible file system is one with a Logs directory in the root of the file system. If no log files are present, the Advanced File Logger will create a log index file which contains the index of the last log file written, and nine log files each PcdAdvancedLoggerPages in size. These files are pre allocated at one time to reduce interference with other users of the filesystem. To enable the Advanced File Logger, the following change is needed in the .dsc: [ Components.ArchOfDXE ] AdvLoggerPkg/AdvancedFileLogger/AdvancedFileLogger.inf and the follow change is needed in the .fdf: [ Components.FV.YourFvDXE ] INF AdvLoggerPkg/AdvancedFileLogger/AdvancedFileLogger.inf Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Read Me"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#advloggerpkg-advanced-logger-package","text":"","title":"AdvLoggerPkg - Advanced Logger Package"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#about","text":"This package contains the various libraries to have in memory logging.","title":"About"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#configuration","text":"The following configurations are supported: Phase Usage DXE Only Uses DxeCore, DxeRuntime, and Dxe AdvancedLoggerLib libraries for logging from start of DXE CORE through Exit Boot Services. Accepts the PEI Advanced Logger Hob if one is generated. Produces the AdvancedLogger protocol. DXE+SMM Requires DXE modules above, and adds the Smm AdvancedLoggerLib library. Collects SMM generated messages in the in memory log PEI Uses PeiCore and Pei AdvancedLoggerLib libraries. Creates the Advanced Logger Hob if PcdAdvancedLoggerPeiInRAM is set. SEC Uses the Sec Advanced Logger Library. SEC requires a fixed load address, so it piggy backs on the Temporary RAM PCD information. Produces a Fixed Address temporary RAM log. When memory is added, the Sec Advanced Logger library converts the Temporary RAM logging information to the PEI Advanced Logger Hob. PEI64 Uses Pei64 Advanced Logger Library. Requires the SEC fixed address temporary log information in order to log Pei64 bit DEBUG messages. PCD's used by Advanced Logger PCD Function of the PCD PcdAdvancedLoggerForceEnable The default operation is to check if a Logs directory is present in the root of the filesystem. If the logs directory is present, logging is enabled. When PcdAdvancedLoggerForceEnable is TRUE, and the device is not a USB device, a Logs directory will be created and logging is enabled. When logging is enabled, the proper log files will be created if not already preset. PcdAdvancedLoggerPeiInRAM For system that have memory at PeiCore entry. The full in memory log buffer if PcdAdvancedLoggerPages is allocated in the Pei Core constructor and PcdAdvancedLoggerPreMemPages is ignored. PcdAdvancedSerialLoggerDebugPrintErrorLevel The standard debug flags filter which log messages are produced. This PCD allow a subset of log messages to be forwarded to the Serial Port Lib. PcdAdvancedSerialLoggerDisable Specifies when to disable writing to the serial port. PcdAdvancedLoggerPreMemPages Amount of temporary RAM used for the debug log. PcdAdvancedLoggerPages Amount of system RAM used for the debug log PcdAdvancedLoggerLocator When enabled, the AdvLogger creates a variable \"AdvLoggerLocator\" with the address of the LoggerInfo buffer","title":"Configuration"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#libraries","text":"The following libraries are used with AdvancedLogger: Library Function of the Library AdvancedLoggerAccessLib Used to access the memory log - used by FileLogger and Serial/Dxe/Logger AdvancedLoggerLib One per module type - used to provide access to the in memory log buffer AdvLoggerSmmAccessLib Used to intercept GetVariable in order to provide an OS utility the ability to read the log BaseDebugLibAdvancedLogger Basic Dxe etc DebugLib DebugAgent Used to intercept SEC initialization PeiDebugLibAdvancedLogger Basic Pei DebugLib","title":"Libraries"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#platform-note","text":"The SEC version of the Advanced Logger uses the temporary RAM block. This block is fixed in size and location, and these need to be adjusted to make room for the Advanced Logger buffer. There may be cases where the processor cache size is too small to enable the Advanced Logger during SEC. The following changes are needed in the .dsc [ LibraryClasses.common ] DebugLib|AdvLoggerPkg/Library/BaseDebugLibAdvancedLogger/BaseDebugLibAdvancedLogger.inf [LibraryClasses.IA32.SEC] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Sec/AdvancedLoggerLib.inf DebugAgentLib|AdvLoggerPkg/Library/DebugAgent/Sec/AdvancedLoggerSecDebugAgent.inf [LibraryClasses.IA32.PEI_CORE] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/PeiCore/AdvancedLoggerLib.inf [LibraryClasses.IA32.PEIM] DebugLib|AdvLoggerPkg/Library/PeiDebugLibAdvancedLogger/PeiDebugLibAdvancedLogger.inf [LibraryClasses.X64.PEIM] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Pei64/AdvancedLoggerLib.inf [LibraryClasses.X64] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Dxe/AdvancedLoggerLib.inf AdvancedLoggerAccessLib|AdvLoggerPkg/Library/AdvancedLoggerAccessLib/AdvancedLoggerAccessLib.inf [LibraryClasses.X64.DXE_CORE] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/DxeCore/AdvancedLoggerLib.inf [LibraryClasses.X64.DXE_SMM_DRIVER] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Smm/AdvancedLoggerLib.inf [LibraryClasses.X64.SMM_CORE] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Smm/AdvancedLoggerLib.inf [LibraryClasses.X64.DXE_RUNTIME_DRIVER] AdvancedLoggerLib|AdvLoggerPkg/Library/AdvancedLoggerLib/Runtime/AdvancedLoggerLib.inf [PcdsFeatureFlag] ## Build Example if you build environment differentiates customer builds from internal test builds !if $(SHIP_MODE) == FALSE gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerForceEnable|TRUE gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerLocator|TRUE !else gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerForceEnable|FALSE gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedFileLoggerLocator|TRUE !endif The following changes should be in the family .dsc where the processor specific changes are specified [ PcdsFixedAtBuild.common ] gAdvLoggerPkgTokenSpaceGuid.PcdAdvancedLoggerPreMemPages|24","title":"Platform note:"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#advanced-file-logger","text":"The Advanced File Logger monitors for file systems mounted during boot. When an eligible file system is detected, the log is flushed to the file system. The log is flushed if the system is reset during POST, and at Exit Boot Services. An eligible file system is one with a Logs directory in the root of the file system. If no log files are present, the Advanced File Logger will create a log index file which contains the index of the last log file written, and nine log files each PcdAdvancedLoggerPages in size. These files are pre allocated at one time to reduce interference with other users of the filesystem. To enable the Advanced File Logger, the following change is needed in the .dsc: [ Components.ArchOfDXE ] AdvLoggerPkg/AdvancedFileLogger/AdvancedFileLogger.inf and the follow change is needed in the .fdf: [ Components.FV.YourFvDXE ] INF AdvLoggerPkg/AdvancedFileLogger/AdvancedFileLogger.inf","title":"Advanced File Logger"},{"location":"dyn/mu_plus/AdvLoggerPkg/ReadMe/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/AdvLoggerPkg/Application/DecodeUefiLog/ReadMe/","text":"AdvLoggerPkg - DecodeUefiLog \u00b6 DecodeUefiLog is used to get the Uefi debug log that is stored by UEFI. About \u00b6 Advanced Logger stores the debug log file in memory, with additional data for each line. At runtime, if the Advanced Logger is enabled, this in memory log is available through the UEFI Variable store. As the log in memory has additional metadata and alignment structure, DecodeUefiLog parses the on memory UefiLog to a text stream and writes the decoded log to a local file. Usage \u00b6 Copy the two files DecideUefiLog.py and UefiVariablesSupportLib.py to the system with Advanced Logger enabled. The simplest case: DecodeUefiLog -o NewLogFile.txt Decode lines after a starting line number and send them to the file specified by -o: DecodeUefiLog -s 5000 -o NewLogFile.txt Copy the raw in memory log to a binary file. DecodeUfieLog -r RawLog.bin Decode a raw file into a text file: DecodeUefiLog -l RawLog.bin -o NewLogFIle.txt","title":"Decode Uefi Log"},{"location":"dyn/mu_plus/AdvLoggerPkg/Application/DecodeUefiLog/ReadMe/#advloggerpkg-decodeuefilog","text":"DecodeUefiLog is used to get the Uefi debug log that is stored by UEFI.","title":"AdvLoggerPkg - DecodeUefiLog"},{"location":"dyn/mu_plus/AdvLoggerPkg/Application/DecodeUefiLog/ReadMe/#about","text":"Advanced Logger stores the debug log file in memory, with additional data for each line. At runtime, if the Advanced Logger is enabled, this in memory log is available through the UEFI Variable store. As the log in memory has additional metadata and alignment structure, DecodeUefiLog parses the on memory UefiLog to a text stream and writes the decoded log to a local file.","title":"About"},{"location":"dyn/mu_plus/AdvLoggerPkg/Application/DecodeUefiLog/ReadMe/#usage","text":"Copy the two files DecideUefiLog.py and UefiVariablesSupportLib.py to the system with Advanced Logger enabled. The simplest case: DecodeUefiLog -o NewLogFile.txt Decode lines after a starting line number and send them to the file specified by -o: DecodeUefiLog -s 5000 -o NewLogFile.txt Copy the raw in memory log to a binary file. DecodeUfieLog -r RawLog.bin Decode a raw file into a text file: DecodeUefiLog -l RawLog.bin -o NewLogFIle.txt","title":"Usage"},{"location":"dyn/mu_plus/AdvLoggerPkg/UnitTests/LineParser/readme/","text":"Verify Line Parser function for development \u00b6 This test is to model the Line Parser in the AdvancedFileLogger. The internal message log is in units of DEBUG(()). While most messages are a complete and end with a '\\n', there are some debug messages that are built with multiple DEBUG(()) operations. The line parser builds a debug line by copying one or more DEBUG(()) segments into a line, and prepends the time stamp. About \u00b6 These tests verify that the LineParser is functional. LineParserTestApp \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Line Parser"},{"location":"dyn/mu_plus/AdvLoggerPkg/UnitTests/LineParser/readme/#verify-line-parser-function-for-development","text":"This test is to model the Line Parser in the AdvancedFileLogger. The internal message log is in units of DEBUG(()). While most messages are a complete and end with a '\\n', there are some debug messages that are built with multiple DEBUG(()) operations. The line parser builds a debug line by copying one or more DEBUG(()) segments into a line, and prepends the time stamp.","title":"Verify Line Parser function for development"},{"location":"dyn/mu_plus/AdvLoggerPkg/UnitTests/LineParser/readme/#about","text":"These tests verify that the LineParser is functional.","title":"About"},{"location":"dyn/mu_plus/AdvLoggerPkg/UnitTests/LineParser/readme/#lineparsertestapp","text":"","title":"LineParserTestApp"},{"location":"dyn/mu_plus/AdvLoggerPkg/UnitTests/LineParser/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/","text":"AuthManagerNull \u00b6 Purposes \u00b6 Do not use in production! FrontPage during device bringup \u00b6 This driver can be a stand in for IdentityAndAuthManager, which requires RngLib, to allow FrontPage development if RngLib is not yet functional. Unit Testing \u00b6 With further development, this \"Null\" driver could be an effective stub for IdentityAndAuthManager, allowing detailed unit testing of DFCI.","title":"Auth Manager Null"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#authmanagernull","text":"","title":"AuthManagerNull"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#purposes","text":"Do not use in production!","title":"Purposes"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#frontpage-during-device-bringup","text":"This driver can be a stand in for IdentityAndAuthManager, which requires RngLib, to allow FrontPage development if RngLib is not yet functional.","title":"FrontPage during device bringup"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#unit-testing","text":"With further development, this \"Null\" driver could be an effective stub for IdentityAndAuthManager, allowing detailed unit testing of DFCI.","title":"Unit Testing"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/","text":"Device Firmware Configuration Interface (DFCI) Introduction \u00b6 Overview \u00b6 The Device Firmware Configuration Interface (DFCI) brings new levels of security and usability to PC configuration management. It is a new feature of UEFI that enables secure programmatic configuration of hardware settings that are typically configured within a BIOS menu by a human. High value configuration can be moved to UEFI BIOS where it is resilient against malware, rootkits, and non-persistent physical tampering. Whereas traditional UEFI security implementations required a physical touch, DFCI securely enables zero-touch remote configuration of these settings built upon Microsoft Intune and authorized by Windows Autopilot . DFCI can provide additional assurance by configuring and locking hardware security features before launching the OS (e.g. disabling microphones or radios). Note that for management of servers in a datacenter, DFCI does not presume to be the solution. Redfish may be a more suitable solution for the datacenter. Why Zero Touch \u00b6 Traditional UEFI management solutions were either not secure, allowing malware to control them, or not scalable, requiring a physical touch by IT or OEM for authentication. DFCI is zero touch, leveraging the existing Windows Autopilot device registration for DFCI authorization. Why should I configure my UEFI BIOS \u00b6 PC configuration is typically performed via Active Directory Group Policy, System Center Configuration Manager (SCCM), or Modern Device Management (MDM) such as Microsoft Intune . All of these solutions store their managed configuration in the OS disk partition. Unfortunately, this configuration can be bypassed by the PCs default ability to boot other operating system instances via external media (e.g. USB), network (e.g. PXE), & alternate disk partitions, or by simply re-installing the OS. Device Firmware Configuration Interface (DFCI) places high value configuration settings into PCs UEFI BIOS. UEFI DFCI storage is both visible to all OS instances, persistent, surviving OS reinstalls and disk reformats, and tamper-resistant, defending itself from malware and rootkits. UEFI executes before the OS and can disallow booting of specified devices, for example USB or network PXE. Further, DFCI can leverage hardware security to enforce some policies with higher assurance than typical OS configuration. For example, it could disable power to cameras or radios in a way that they could not be re-enabled by an OS, malware, or rootkit. Popular Usages \u00b6 Disabling cameras, microphones, and/or radios in manufacturing and other secure facilities Disabling boot to USB and network for single purpose and KIOSK devices Disabling local user access to all UEFI settings to maintain the out of box configuration OEM Enablement Summary \u00b6 DFCI enablement is comprised of: UEFI BIOS implementation Windows Autopilot participation If an OEM or its Partners already participate in the Windows Autopilot program, no additional Autopilot work is required, the only remaining work should be the UEFI BIOS implementation. Windows Autopilot Implementation \u00b6 The pre-existing Windows Autopilot device registration workflows remain unchanged for DFCI, no additional work is required. It should be noted that Autopilot self-registrations are not trusted for the purpose of DFCI management (e.g. from Intune, Microsoft Store for Business, & Business 365). UEFI BIOS Implementation \u00b6 DFCI enablement in UEFI BIOS requires implementation of DFCI interfaces and semantics, and inclusion of a public Microsoft certificate. There is precisely one (1) Microsoft zero-touch certificate that is shared by all DFCI-enabled systems to authenticate zero-touch provisioning requests. Thus there is no requirement to inject the certificate at manufacturing, it may simply be included in the UEFI BIOS image. The DFCI source code and public certificate are available on GitHub under a permissive open source license (SPDX-License-Identifier: BSD-2-Clause-Patent). https://github.com/microsoft/mu_plus/tree/dev/201908/DfciPkg https://github.com/microsoft/mu_plus/tree/dev/201908/ZeroTouchPkg There is also an example UEFI BIOS menu that demonstrates how to integrate DFCI: https://github.com/microsoft/mu_oem_sample/search?q=dfci&unscoped_q=dfci UEFI Implementation Details \u00b6 Scenarios: Building the Microsoft Scenarios with DFCI Integration: Integrating DFCI code into your platforms Architecture: DFCI Code Internals","title":"Dfci Feature"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#device-firmware-configuration-interface-dfci-introduction","text":"","title":"Device Firmware Configuration Interface (DFCI) Introduction"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#overview","text":"The Device Firmware Configuration Interface (DFCI) brings new levels of security and usability to PC configuration management. It is a new feature of UEFI that enables secure programmatic configuration of hardware settings that are typically configured within a BIOS menu by a human. High value configuration can be moved to UEFI BIOS where it is resilient against malware, rootkits, and non-persistent physical tampering. Whereas traditional UEFI security implementations required a physical touch, DFCI securely enables zero-touch remote configuration of these settings built upon Microsoft Intune and authorized by Windows Autopilot . DFCI can provide additional assurance by configuring and locking hardware security features before launching the OS (e.g. disabling microphones or radios). Note that for management of servers in a datacenter, DFCI does not presume to be the solution. Redfish may be a more suitable solution for the datacenter.","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#why-zero-touch","text":"Traditional UEFI management solutions were either not secure, allowing malware to control them, or not scalable, requiring a physical touch by IT or OEM for authentication. DFCI is zero touch, leveraging the existing Windows Autopilot device registration for DFCI authorization.","title":"Why Zero Touch"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#why-should-i-configure-my-uefi-bios","text":"PC configuration is typically performed via Active Directory Group Policy, System Center Configuration Manager (SCCM), or Modern Device Management (MDM) such as Microsoft Intune . All of these solutions store their managed configuration in the OS disk partition. Unfortunately, this configuration can be bypassed by the PCs default ability to boot other operating system instances via external media (e.g. USB), network (e.g. PXE), & alternate disk partitions, or by simply re-installing the OS. Device Firmware Configuration Interface (DFCI) places high value configuration settings into PCs UEFI BIOS. UEFI DFCI storage is both visible to all OS instances, persistent, surviving OS reinstalls and disk reformats, and tamper-resistant, defending itself from malware and rootkits. UEFI executes before the OS and can disallow booting of specified devices, for example USB or network PXE. Further, DFCI can leverage hardware security to enforce some policies with higher assurance than typical OS configuration. For example, it could disable power to cameras or radios in a way that they could not be re-enabled by an OS, malware, or rootkit.","title":"Why should I configure my UEFI BIOS"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#popular-usages","text":"Disabling cameras, microphones, and/or radios in manufacturing and other secure facilities Disabling boot to USB and network for single purpose and KIOSK devices Disabling local user access to all UEFI settings to maintain the out of box configuration","title":"Popular Usages"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#oem-enablement-summary","text":"DFCI enablement is comprised of: UEFI BIOS implementation Windows Autopilot participation If an OEM or its Partners already participate in the Windows Autopilot program, no additional Autopilot work is required, the only remaining work should be the UEFI BIOS implementation.","title":"OEM Enablement Summary"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#windows-autopilot-implementation","text":"The pre-existing Windows Autopilot device registration workflows remain unchanged for DFCI, no additional work is required. It should be noted that Autopilot self-registrations are not trusted for the purpose of DFCI management (e.g. from Intune, Microsoft Store for Business, & Business 365).","title":"Windows Autopilot Implementation"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#uefi-bios-implementation","text":"DFCI enablement in UEFI BIOS requires implementation of DFCI interfaces and semantics, and inclusion of a public Microsoft certificate. There is precisely one (1) Microsoft zero-touch certificate that is shared by all DFCI-enabled systems to authenticate zero-touch provisioning requests. Thus there is no requirement to inject the certificate at manufacturing, it may simply be included in the UEFI BIOS image. The DFCI source code and public certificate are available on GitHub under a permissive open source license (SPDX-License-Identifier: BSD-2-Clause-Patent). https://github.com/microsoft/mu_plus/tree/dev/201908/DfciPkg https://github.com/microsoft/mu_plus/tree/dev/201908/ZeroTouchPkg There is also an example UEFI BIOS menu that demonstrates how to integrate DFCI: https://github.com/microsoft/mu_oem_sample/search?q=dfci&unscoped_q=dfci","title":"UEFI BIOS Implementation"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#uefi-implementation-details","text":"Scenarios: Building the Microsoft Scenarios with DFCI Integration: Integrating DFCI code into your platforms Architecture: DFCI Code Internals","title":"UEFI Implementation Details"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/","text":"DFCI Internals \u00b6 This section describes the internal operations of DFCI. Communications with Provider \u00b6 DFCI communicates with a controlling identity. One of the controlling identities could be Microsoft Intune. The communications path from the controlling identity is though the use of UEFI variables. DFCI processes the mailbox variables during a system restart. Identity Manager \u00b6 In the source code, the Identity manager is implemented in IdentityAndAuthManager is defined in the DfciPkg located in the mu_plus repository https://github.com/microsoft/mu_plus/ . Identity and Auth Manager is responsible for managing the Identities. The initial state of the system has the Local User with full authentication to make changes to any of the available settings. There are six Identities known by DFCI: Identity Use of the Identity Owner The system owner. Used by a controlling agent - that authorizes Use to control some settings User A delegated user. Used by Microsoft Intune. User1 Not currently used User2 Not currently used Local User Not a certificate - just a known, default, user Zero Touch Limited use Identity to allow an Enroll from a controlling agent. The system has the Zero Touch Certificate installed during manufacturing. Zero Touch cannot be enrolled through the normal enroll operation. Zero Touch has no use when a system is enrolled. The Identity Manager reads the incoming mailbox to process a Identity enroll, Identity certificate update, and Identity unenroll operations. Except for the Local User, when an Identity is enrolled, it means adding a Certificate that will be used to validate incoming settings. The Identity Manager verifies that the incoming identity mailbox packet: Is signed by one of the Identities The signed identity has permission to update the target identity. Target information in the packet matches the system information The one exception is when an new Owner is being enrolled, no Identities validate the mailbox packet, and the Local User has permission to enroll an owner, DFCI will pause booting to prompt the Local User for permission to do the enroll. The user will be asked to validate the enrollment by entering the last two characters of the new owners certificate hash. When installed by the manufacturer of the system, the Zero Touch certificate will have permission to allow the Zero Touch owner packet to be enrolled without user intervention. Hence, the term Zero Touch enrollment. Permissions Manager \u00b6 The permission manager processes incoming permission mailbox packets. Permission packets must be signed by one of Owner, User, User1 or User2. When processing the incoming permissions XML, the signer permissions are used to enable adding or changing a permission. Settings Manager \u00b6 The settings manager processes incoming settings mailbox packets. Settings packets must be signed by one of Owner, User, User1 or User2. When processing the incoming settings XML, the signer permissions are used to change a setting. Identity Packet Formats \u00b6 An Identity packet consists of a binary header, a DER encoded certificate file, a test signature validating the signing capability, and the signature of the packet: The Test Signature is the detached signature of signing the public key certificate by the private key of the public key certificate. The Signature field of the packet is the detached signature of signing Header-PublicCert-TestSignature by: Operation Signing Key Enroll The private key of the matching Public Key Certificate Roll The private key matching the public cert of the Identity being rolled. Unenroll The private key matching the public cert of the Identity being unenrolled. Permission Packet Formats \u00b6 A Permission packet consists of a binary header, an XML payload, and a signature: Sample permission packet: <?xml version=\"1.0\" encoding=\"utf-8\"?> <PermissionsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Cloud Controller </CreatedBy> <CreatedOn> 2018-03-28 </CreatedOn> <Version> 1 </Version> <LowestSupportedVersion> 1 </LowestSupportedVersion> <Permissions Default= \"129\" Delegated= \"192\" Append= \"False\" > <!-- Sample DDS initial enroll permissions Permission Mask - 128 = Owner 64 = User 32 = User1 16 = User2 8 = ZTD 1 = Local User Owner keeps the following settings for itself --> <Permission> <Id> Dfci.OwnerKey.Enum </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Recovery.Enable </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <!-- Needs 128 (Owner Permission) to set the key, Needs 64 (User Permission) for User to roll the key --> <Id> Dfci.UserKey.Enum </Id> <PMask> 192 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Hwid.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> </Permissions> </PermissionsPacket> ``` ## Settings Packet Formats ![Setting packet signature](Images/SettingsPacket.jpg) ![Setting packet signature](Images/DataPacketSignature.jpg) Sample settings payload: ```xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <SettingsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Mike Turner </CreatedBy> <CreatedOn> 2019-03-06 10:10:00 </CreatedOn> <Version> 2 </Version> <!-- Make sure you edit DfciSettingsPattern.xml and then run BuildSettings.bat to generate the DfciSettings.xml --> <LowestSupportedVersion> 2 </LowestSupportedVersion> <Settings> <Setting> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <Value> http://some URL to access recovery cert updates/ </Value> </Setting> <Setting> <Id> Dfci.RecoveryUrl.String </Id> <Value> https://some URL to access recovery update packets/ </Value> </Setting> <Setting> <Id> Dfci.HttpsCert.Binary </Id> <Value> <!-- This is where a BASE64 encoded string of the certificate used for HTTPS operations is stored. --> wA== </Value> </Setting> <Setting> <Id> Dfci.RegistrationId.String </Id> <Value> 12345678-1234-5678-1234-012345674321 </Value> </Setting> <Setting> <Id> Dfci.TenantId.String </Id> <Value> 98765432-1234-5678-1234-012345674321 </Value> </Setting> </Settings> </SettingsPacket> Packet Processing \u00b6 In order to minimize rebooting when accepting packets from the owner, there are 6 mailboxes for DFCI. There are two for each Identity, Permission, and Settings. We call them: Identity Identity2 Permission Permission2 Settings Settings2 Only packets of the correct type are processed out of each mailbox. Packets are processed in the following order: Enroll Identity Enroll Identity2 Apply Permission Apply Permission2 At this point, if there is a severe error, the Identities and Permissions are reverted to what they were before processing the packets. The following are still processed, in order. Settings Settings2 Unenroll Identity 2 Unenroll Identity UEFI CSP \u00b6 Intune accesses the variables through the UEFI CSP provider. Out of band recovery \u00b6 Normally, the cloud provider would just send unenroll packets through the OS to the UEFICsp. However, if Windows is unable to boot, the UEFI front page application has a method to contact the owner via HTTPS.","title":"Internals"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#dfci-internals","text":"This section describes the internal operations of DFCI.","title":"DFCI Internals"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#communications-with-provider","text":"DFCI communicates with a controlling identity. One of the controlling identities could be Microsoft Intune. The communications path from the controlling identity is though the use of UEFI variables. DFCI processes the mailbox variables during a system restart.","title":"Communications with Provider"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#identity-manager","text":"In the source code, the Identity manager is implemented in IdentityAndAuthManager is defined in the DfciPkg located in the mu_plus repository https://github.com/microsoft/mu_plus/ . Identity and Auth Manager is responsible for managing the Identities. The initial state of the system has the Local User with full authentication to make changes to any of the available settings. There are six Identities known by DFCI: Identity Use of the Identity Owner The system owner. Used by a controlling agent - that authorizes Use to control some settings User A delegated user. Used by Microsoft Intune. User1 Not currently used User2 Not currently used Local User Not a certificate - just a known, default, user Zero Touch Limited use Identity to allow an Enroll from a controlling agent. The system has the Zero Touch Certificate installed during manufacturing. Zero Touch cannot be enrolled through the normal enroll operation. Zero Touch has no use when a system is enrolled. The Identity Manager reads the incoming mailbox to process a Identity enroll, Identity certificate update, and Identity unenroll operations. Except for the Local User, when an Identity is enrolled, it means adding a Certificate that will be used to validate incoming settings. The Identity Manager verifies that the incoming identity mailbox packet: Is signed by one of the Identities The signed identity has permission to update the target identity. Target information in the packet matches the system information The one exception is when an new Owner is being enrolled, no Identities validate the mailbox packet, and the Local User has permission to enroll an owner, DFCI will pause booting to prompt the Local User for permission to do the enroll. The user will be asked to validate the enrollment by entering the last two characters of the new owners certificate hash. When installed by the manufacturer of the system, the Zero Touch certificate will have permission to allow the Zero Touch owner packet to be enrolled without user intervention. Hence, the term Zero Touch enrollment.","title":"Identity Manager"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#permissions-manager","text":"The permission manager processes incoming permission mailbox packets. Permission packets must be signed by one of Owner, User, User1 or User2. When processing the incoming permissions XML, the signer permissions are used to enable adding or changing a permission.","title":"Permissions Manager"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#settings-manager","text":"The settings manager processes incoming settings mailbox packets. Settings packets must be signed by one of Owner, User, User1 or User2. When processing the incoming settings XML, the signer permissions are used to change a setting.","title":"Settings Manager"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#identity-packet-formats","text":"An Identity packet consists of a binary header, a DER encoded certificate file, a test signature validating the signing capability, and the signature of the packet: The Test Signature is the detached signature of signing the public key certificate by the private key of the public key certificate. The Signature field of the packet is the detached signature of signing Header-PublicCert-TestSignature by: Operation Signing Key Enroll The private key of the matching Public Key Certificate Roll The private key matching the public cert of the Identity being rolled. Unenroll The private key matching the public cert of the Identity being unenrolled.","title":"Identity Packet Formats"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#permission-packet-formats","text":"A Permission packet consists of a binary header, an XML payload, and a signature: Sample permission packet: <?xml version=\"1.0\" encoding=\"utf-8\"?> <PermissionsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Cloud Controller </CreatedBy> <CreatedOn> 2018-03-28 </CreatedOn> <Version> 1 </Version> <LowestSupportedVersion> 1 </LowestSupportedVersion> <Permissions Default= \"129\" Delegated= \"192\" Append= \"False\" > <!-- Sample DDS initial enroll permissions Permission Mask - 128 = Owner 64 = User 32 = User1 16 = User2 8 = ZTD 1 = Local User Owner keeps the following settings for itself --> <Permission> <Id> Dfci.OwnerKey.Enum </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Recovery.Enable </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <!-- Needs 128 (Owner Permission) to set the key, Needs 64 (User Permission) for User to roll the key --> <Id> Dfci.UserKey.Enum </Id> <PMask> 192 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Hwid.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> </Permissions> </PermissionsPacket> ``` ## Settings Packet Formats ![Setting packet signature](Images/SettingsPacket.jpg) ![Setting packet signature](Images/DataPacketSignature.jpg) Sample settings payload: ```xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <SettingsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Mike Turner </CreatedBy> <CreatedOn> 2019-03-06 10:10:00 </CreatedOn> <Version> 2 </Version> <!-- Make sure you edit DfciSettingsPattern.xml and then run BuildSettings.bat to generate the DfciSettings.xml --> <LowestSupportedVersion> 2 </LowestSupportedVersion> <Settings> <Setting> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <Value> http://some URL to access recovery cert updates/ </Value> </Setting> <Setting> <Id> Dfci.RecoveryUrl.String </Id> <Value> https://some URL to access recovery update packets/ </Value> </Setting> <Setting> <Id> Dfci.HttpsCert.Binary </Id> <Value> <!-- This is where a BASE64 encoded string of the certificate used for HTTPS operations is stored. --> wA== </Value> </Setting> <Setting> <Id> Dfci.RegistrationId.String </Id> <Value> 12345678-1234-5678-1234-012345674321 </Value> </Setting> <Setting> <Id> Dfci.TenantId.String </Id> <Value> 98765432-1234-5678-1234-012345674321 </Value> </Setting> </Settings> </SettingsPacket>","title":"Permission Packet Formats"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#packet-processing","text":"In order to minimize rebooting when accepting packets from the owner, there are 6 mailboxes for DFCI. There are two for each Identity, Permission, and Settings. We call them: Identity Identity2 Permission Permission2 Settings Settings2 Only packets of the correct type are processed out of each mailbox. Packets are processed in the following order: Enroll Identity Enroll Identity2 Apply Permission Apply Permission2 At this point, if there is a severe error, the Identities and Permissions are reverted to what they were before processing the packets. The following are still processed, in order. Settings Settings2 Unenroll Identity 2 Unenroll Identity","title":"Packet Processing"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#uefi-csp","text":"Intune accesses the variables through the UEFI CSP provider.","title":"UEFI CSP"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#out-of-band-recovery","text":"Normally, the cloud provider would just send unenroll packets through the OS to the UEFICsp. However, if Windows is unable to boot, the UEFI front page application has a method to contact the owner via HTTPS.","title":"Out of band recovery"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/","text":"DfciDeviceIdSupportLib \u00b6 DfciDeviceIdSupportLib provides DFCI with three platform strings: Manufacturer name Product name Serial number Restrictions on Device Identifier strings \u00b6 Null terminated CHAR8 strings Maximum of 64 CHAR8 values plus a NULL terminator, for a maximum size of 65 bytes. The following five characters are not allowed & ' \" < > UTF-8, as per Wikipedia UTF-8 , are allowed within the 64 CHAR limit and the character set limitations. Interfaces \u00b6 Interface Function DfciIdSupportV1GetSerialNumber Return the system serial number as a UINTN if possible. Otherwise, return 0. DfciIdSupportGetManufacturer Returns an allocated buffer with the system manufacturer name. DfciIdSupportGetProductName Return an allocated buffer with the system product name. DfciIdSupportGetSerialNumber Return an allocated buffer with the system serial number. Additional Details \u00b6 These fields and their values are critical to the security of DFCI. These values should not be user configurable and should be protected from tampering.","title":"Dfci Device Id Support Lib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#dfcideviceidsupportlib","text":"DfciDeviceIdSupportLib provides DFCI with three platform strings: Manufacturer name Product name Serial number","title":"DfciDeviceIdSupportLib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#restrictions-on-device-identifier-strings","text":"Null terminated CHAR8 strings Maximum of 64 CHAR8 values plus a NULL terminator, for a maximum size of 65 bytes. The following five characters are not allowed & ' \" < > UTF-8, as per Wikipedia UTF-8 , are allowed within the 64 CHAR limit and the character set limitations.","title":"Restrictions on Device Identifier strings"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#interfaces","text":"Interface Function DfciIdSupportV1GetSerialNumber Return the system serial number as a UINTN if possible. Otherwise, return 0. DfciIdSupportGetManufacturer Returns an allocated buffer with the system manufacturer name. DfciIdSupportGetProductName Return an allocated buffer with the system product name. DfciIdSupportGetSerialNumber Return an allocated buffer with the system serial number.","title":"Interfaces"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#additional-details","text":"These fields and their values are critical to the security of DFCI. These values should not be user configurable and should be protected from tampering.","title":"Additional Details"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/","text":"DFCI Groups \u00b6 DFCI Groups allow numerous like typed settings to be managed together. Groups can also be used to provide multiple names for the same setting. This allows actual device settings to be mapped into a different namespaces for settings. One such example from the Microsoft scenario is Dfci.OnboardCameras.Enable . This group setting is used to manage the state of all onboard cameras and gives the management entity an ability to control all cameras regardless of how many each platform has. A platform may have Device.FrontCamera.Enable and Device.RearCamera.Enable settings. Adding those settings to the group Dfci.OnboardCameras.Enable allows a general purpose management entity to control both. Handling State Reporting \u00b6 If all of the settings of the group have the same value then that value will be returned (ie Enabled or Disabled for an Enable type setting). If they are not the same then Inconsistent will be returned. Unknown will be returned if there are no members in a group. Restrictions on group names \u00b6 Group names and settings names are in the same name space and duplicate names are not allowed. Like settings names, group names are limited to 96 characters in length, and are null terminated CHAR8 strings. DfciGroupLib \u00b6 The DfciGroupLib is how groups are managed. This library separates the grouping configuration from the setting providers to allow better flexibility and better maintainability. DfciGroupLib is defined in the DfciPkg located in mu_plus repository https://github.com/microsoft/mu_plus/ Library Interfaces \u00b6 Interfaces Usage DfciGetGroupEntries DfciGetGroupEntries returns an array of groups, and each group points to a list of settings that are members of the group. DFCI Standard Groups for OEM extension \u00b6 Group Setting String Description \"Dfci.OnboardCameras.Enable\" Enable/Disable all built-in cameras \"Dfci.OnboardAudio.Enable\" Enable/Disable all built-in microphones & speakers \"Dfci.OnboardRadios.Enable\" Enable/Disable all built-in radios (e.g. Wi-Fi, BlueTooth, NFC, Mobile Broadband...) \"Dfci.BootExternalMedia.Enable\" Enable/disable boot from external media \"Dfci.BootOnboardNetwork.Enable\" Enable/disable boot from built-in network adapters \"Dfci.CpuAndIoVirtualization.Enable\" Enable/disable both CPU & IO Virtualization (i.e. prerequisite for Windows Virtualization Based Security (a.k.a. Device Guard, Core Isolation, Secured Core) Example \u00b6 Declare names for all individual BIOS settings that may be modified by the DFCI-standard settings. The DFCI-standard string values are prefixed with \" Dfci. \". A naming convention for device-specific setting strings is proposed as \" Device. \", as follows: // Cameras // // Group setting \"Dfci.OnboardCameras.Enable\" #define DEVICE_SETTING_ID__FRONT_CAMERA \"Device.FrontCamera.Enable\" #define DEVICE_SETTING_ID__REAR_CAMERA \"Device.RearCamera.Enable\" #define DEVICE_SETTING_ID__IR_CAMERA \"Device.IRCamera.Enable\" Map the individual settings to the DFCI groups, an example DfciGroups.c is as follows: STATIC DFCI_SETTING_ID_STRING mAllCameraSettings [] = { DEVICE_SETTING_ID__FRONT_CAMERA , DEVICE_SETTING_ID__REAR_CAMERA , DEVICE_SETTING_ID__IR_CAMERA , NULL }; STATIC DFCI_SETTING_ID_STRING mAllCpuAndIoVirtSettings [] = { DEVICE_SETTING_ID__ENABLE_VIRT_SETTINGS , NULL }; STATIC DFCI_GROUP_ENTRY mMyGroups [] = { { DFCI_SETTING_ID__ALL_CAMERAS , ( DFCI_SETTING_ID_STRING * ) & mAllCameraSettings }, { DFCI_SETTING_ID__ALL_AUDIO , ( DFCI_SETTING_ID_STRING * ) & mAllAudioSettings }, { DFCI_SETTING_ID__ALL_RADIOS , ( DFCI_SETTING_ID_STRING * ) & mAllRadiosSettings }, { DFCI_SETTING_ID__EXTERNAL_MEDIA , ( DFCI_SETTING_ID_STRING * ) & mExternalMediaSettings }, { DFCI_SETTING_ID__ENABLE_NETWORK , ( DFCI_SETTING_ID_STRING * ) & mOnboardNetworkSettings }, { DFCI_SETTING_ID__ALL_CPU_IO_VIRT , ( DFCI_SETTING_ID_STRING * ) & mAllCpuAndIoVirtSettings }, { NULL , NULL } }; /** * Return a pointer to the Group Array to DFCI * */ DFCI_GROUP_ENTRY * EFIAPI DfciGetGroupEntries ( VOID ) { return ( DFCI_GROUP_ENTRY * ) & mMyGroups ; }","title":"Dfci Groups"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#dfci-groups","text":"DFCI Groups allow numerous like typed settings to be managed together. Groups can also be used to provide multiple names for the same setting. This allows actual device settings to be mapped into a different namespaces for settings. One such example from the Microsoft scenario is Dfci.OnboardCameras.Enable . This group setting is used to manage the state of all onboard cameras and gives the management entity an ability to control all cameras regardless of how many each platform has. A platform may have Device.FrontCamera.Enable and Device.RearCamera.Enable settings. Adding those settings to the group Dfci.OnboardCameras.Enable allows a general purpose management entity to control both.","title":"DFCI Groups"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#handling-state-reporting","text":"If all of the settings of the group have the same value then that value will be returned (ie Enabled or Disabled for an Enable type setting). If they are not the same then Inconsistent will be returned. Unknown will be returned if there are no members in a group.","title":"Handling State Reporting"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#restrictions-on-group-names","text":"Group names and settings names are in the same name space and duplicate names are not allowed. Like settings names, group names are limited to 96 characters in length, and are null terminated CHAR8 strings.","title":"Restrictions on group names"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#dfcigrouplib","text":"The DfciGroupLib is how groups are managed. This library separates the grouping configuration from the setting providers to allow better flexibility and better maintainability. DfciGroupLib is defined in the DfciPkg located in mu_plus repository https://github.com/microsoft/mu_plus/","title":"DfciGroupLib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#library-interfaces","text":"Interfaces Usage DfciGetGroupEntries DfciGetGroupEntries returns an array of groups, and each group points to a list of settings that are members of the group.","title":"Library Interfaces"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#dfci-standard-groups-for-oem-extension","text":"Group Setting String Description \"Dfci.OnboardCameras.Enable\" Enable/Disable all built-in cameras \"Dfci.OnboardAudio.Enable\" Enable/Disable all built-in microphones & speakers \"Dfci.OnboardRadios.Enable\" Enable/Disable all built-in radios (e.g. Wi-Fi, BlueTooth, NFC, Mobile Broadband...) \"Dfci.BootExternalMedia.Enable\" Enable/disable boot from external media \"Dfci.BootOnboardNetwork.Enable\" Enable/disable boot from built-in network adapters \"Dfci.CpuAndIoVirtualization.Enable\" Enable/disable both CPU & IO Virtualization (i.e. prerequisite for Windows Virtualization Based Security (a.k.a. Device Guard, Core Isolation, Secured Core)","title":"DFCI Standard Groups for OEM extension"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#example","text":"Declare names for all individual BIOS settings that may be modified by the DFCI-standard settings. The DFCI-standard string values are prefixed with \" Dfci. \". A naming convention for device-specific setting strings is proposed as \" Device. \", as follows: // Cameras // // Group setting \"Dfci.OnboardCameras.Enable\" #define DEVICE_SETTING_ID__FRONT_CAMERA \"Device.FrontCamera.Enable\" #define DEVICE_SETTING_ID__REAR_CAMERA \"Device.RearCamera.Enable\" #define DEVICE_SETTING_ID__IR_CAMERA \"Device.IRCamera.Enable\" Map the individual settings to the DFCI groups, an example DfciGroups.c is as follows: STATIC DFCI_SETTING_ID_STRING mAllCameraSettings [] = { DEVICE_SETTING_ID__FRONT_CAMERA , DEVICE_SETTING_ID__REAR_CAMERA , DEVICE_SETTING_ID__IR_CAMERA , NULL }; STATIC DFCI_SETTING_ID_STRING mAllCpuAndIoVirtSettings [] = { DEVICE_SETTING_ID__ENABLE_VIRT_SETTINGS , NULL }; STATIC DFCI_GROUP_ENTRY mMyGroups [] = { { DFCI_SETTING_ID__ALL_CAMERAS , ( DFCI_SETTING_ID_STRING * ) & mAllCameraSettings }, { DFCI_SETTING_ID__ALL_AUDIO , ( DFCI_SETTING_ID_STRING * ) & mAllAudioSettings }, { DFCI_SETTING_ID__ALL_RADIOS , ( DFCI_SETTING_ID_STRING * ) & mAllRadiosSettings }, { DFCI_SETTING_ID__EXTERNAL_MEDIA , ( DFCI_SETTING_ID_STRING * ) & mExternalMediaSettings }, { DFCI_SETTING_ID__ENABLE_NETWORK , ( DFCI_SETTING_ID_STRING * ) & mOnboardNetworkSettings }, { DFCI_SETTING_ID__ALL_CPU_IO_VIRT , ( DFCI_SETTING_ID_STRING * ) & mAllCpuAndIoVirtSettings }, { NULL , NULL } }; /** * Return a pointer to the Group Array to DFCI * */ DFCI_GROUP_ENTRY * EFIAPI DfciGetGroupEntries ( VOID ) { return ( DFCI_GROUP_ENTRY * ) & mMyGroups ; }","title":"Example"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/","text":"DFCI Settings Providers \u00b6 Settings providers are the foundation of DFCI. Settings providers provide a common method to get and apply a setting. All of the setting providers are linked to the Settings Manager,which published the Setting Access Protocol. All updates to settings that are provided by an anonymous DFCI settings library should be through the Setting Access protocol. The Setting Access protocol will validate the permission of the setting before allowing the setting to be changed. Overview \u00b6 A setting provider may publish more than one settings. Multiple setting providers are aggregated and accessed through the DFCI Setting Access Protocol as shown below: Lets look at what is needed for a single setting in the Setting Provider environment. A setting provider is an anonymous library linked with the Settings Manager DXE driver. Here is how the DfciSampleProvider library is linked with the Settings Manager as an anonymous library: DfciPkg/SettingsManager/SettingsManagerDxe.inf { <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE <LibraryClasses> NULL|DfciPkg/Library/DfciSampleProvider/DfciSampleProviderLib.inf } Any number of anonymous libraries can be linked with the Settings Manager. Referring to the DfciSampleProvider code, a setting provider defines a setting as: DFCI_SETTING_PROVIDER mDfciSampleProviderProviderSetting1 = { MY_SETTING_ID__SETTING1 , DFCI_SETTING_TYPE_ENABLE , DFCI_SETTING_FLAGS_NO_PREBOOT_UI , // NO UI element for user to change DfciSampleProviderSet , DfciSampleProviderGet , DfciSampleProviderGetDefault , DfciSampleProviderSetDefault }; This particular setting is a ENABLE/DISABLE type of setting, and is telling DFCI that there is no UI element for this setting. When there is no UI element for a setting, DFCI will set the value to the setting Default Value when DFCI is unenrolled. Each setting provider library must have a constructor with code that checks the PCD gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider. When the constructor is called with the InstallProvider PCD set to TRUE, the setting provider needs to register for a notification of the Settings Provider Support Protocol. When that notification is called, the Settings Provider calls the RegisterProvider method with each setting that the setting provider provides. The constructor looks like: if ( FeaturePcdGet ( PcdSettingsManagerInstallProvider )) { //Install callback on the SettingsManager gDfciSettingsProviderSupportProtocolGuid protocol mDfciSampleProviderProviderSupportInstallEvent = EfiCreateProtocolNotifyEvent ( & gDfciSettingsProviderSupportProtocolGuid , TPL_CALLBACK , DfciSampleProviderProviderSupportProtocolNotify , NULL , & mDfciSampleProviderProviderSupportInstallEventRegistration ); DEBUG (( DEBUG_INFO , \"%a: Event Registered. \\n \" , __FUNCTION__ )); //Initialize the settings store Status = InitializeSettingStore (); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"%a: Initialize Store failed. %r. \\n \" , __FUNCTION__ , Status )); } } return EFI_SUCCESS ; The notify routine looks like: //locate protocol Status = gBS -> LocateProtocol ( & gDfciSettingsProviderSupportProtocolGuid , NULL , ( VOID ** ) & sp ); if ( EFI_ERROR ( Status )) { if (( CallCount ++ != 0 ) || ( Status != EFI_NOT_FOUND )) { DEBUG (( DEBUG_ERROR , \"%a() - Failed to locate gDfciSettingsProviderSupportProtocolGuid in notify. Status = %r \\n \" , __FUNCTION__ , Status )); } return ; } Status = sp -> RegisterProvider ( sp , & mDfciSampleProviderProviderSetting1 ); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"Failed to Register %a. Status = %r \\n \" , mDfciSampleProviderProviderSetting1 . Id , Status )); } //We got here, this means all protocols were installed and we didn't exit early. //close the event as we don't need to be signaled again. (shouldn't happen anyway) gBS -> CloseEvent ( Event ); Setting Provider \u00b6 When you are writing your setting provider, keep in mind that other, similarly written libraries, are linked together. Define each common routine as STATIC to avoid conflicts with other providers. Refer to the UEFI general timeline here: Quite a few settings are only needed in late DXE, BDS or FrontPage. You may need access to settings values in PEI or in DXE prior to the starting of SettingsManager. To do this, add private methods to your settings library. Doing this will keep a single piece of code that accesses the nonvolatile storage for the settings. Here is a sample local setting function in the Dfci Sample Provider: // Here is where you would have private interfaces to get and or set a settings value EFI_STATUS OEM_GetSampleSetting1 ( OUT UINT8 * LocalSetting ) { UINTN LocalSettingsSize ; EFI_STATUS Status ; LocalSettingSize = sizeof ( * LocalSetting ); Status = DfciSampleProviderGet ( & mDfciSampleProviderProviderSetting1 , & LocalSettingSize , & LocalSetting ); return Status ; }","title":"Dfci Setting Providers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/#dfci-settings-providers","text":"Settings providers are the foundation of DFCI. Settings providers provide a common method to get and apply a setting. All of the setting providers are linked to the Settings Manager,which published the Setting Access Protocol. All updates to settings that are provided by an anonymous DFCI settings library should be through the Setting Access protocol. The Setting Access protocol will validate the permission of the setting before allowing the setting to be changed.","title":"DFCI Settings Providers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/#overview","text":"A setting provider may publish more than one settings. Multiple setting providers are aggregated and accessed through the DFCI Setting Access Protocol as shown below: Lets look at what is needed for a single setting in the Setting Provider environment. A setting provider is an anonymous library linked with the Settings Manager DXE driver. Here is how the DfciSampleProvider library is linked with the Settings Manager as an anonymous library: DfciPkg/SettingsManager/SettingsManagerDxe.inf { <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE <LibraryClasses> NULL|DfciPkg/Library/DfciSampleProvider/DfciSampleProviderLib.inf } Any number of anonymous libraries can be linked with the Settings Manager. Referring to the DfciSampleProvider code, a setting provider defines a setting as: DFCI_SETTING_PROVIDER mDfciSampleProviderProviderSetting1 = { MY_SETTING_ID__SETTING1 , DFCI_SETTING_TYPE_ENABLE , DFCI_SETTING_FLAGS_NO_PREBOOT_UI , // NO UI element for user to change DfciSampleProviderSet , DfciSampleProviderGet , DfciSampleProviderGetDefault , DfciSampleProviderSetDefault }; This particular setting is a ENABLE/DISABLE type of setting, and is telling DFCI that there is no UI element for this setting. When there is no UI element for a setting, DFCI will set the value to the setting Default Value when DFCI is unenrolled. Each setting provider library must have a constructor with code that checks the PCD gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider. When the constructor is called with the InstallProvider PCD set to TRUE, the setting provider needs to register for a notification of the Settings Provider Support Protocol. When that notification is called, the Settings Provider calls the RegisterProvider method with each setting that the setting provider provides. The constructor looks like: if ( FeaturePcdGet ( PcdSettingsManagerInstallProvider )) { //Install callback on the SettingsManager gDfciSettingsProviderSupportProtocolGuid protocol mDfciSampleProviderProviderSupportInstallEvent = EfiCreateProtocolNotifyEvent ( & gDfciSettingsProviderSupportProtocolGuid , TPL_CALLBACK , DfciSampleProviderProviderSupportProtocolNotify , NULL , & mDfciSampleProviderProviderSupportInstallEventRegistration ); DEBUG (( DEBUG_INFO , \"%a: Event Registered. \\n \" , __FUNCTION__ )); //Initialize the settings store Status = InitializeSettingStore (); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"%a: Initialize Store failed. %r. \\n \" , __FUNCTION__ , Status )); } } return EFI_SUCCESS ; The notify routine looks like: //locate protocol Status = gBS -> LocateProtocol ( & gDfciSettingsProviderSupportProtocolGuid , NULL , ( VOID ** ) & sp ); if ( EFI_ERROR ( Status )) { if (( CallCount ++ != 0 ) || ( Status != EFI_NOT_FOUND )) { DEBUG (( DEBUG_ERROR , \"%a() - Failed to locate gDfciSettingsProviderSupportProtocolGuid in notify. Status = %r \\n \" , __FUNCTION__ , Status )); } return ; } Status = sp -> RegisterProvider ( sp , & mDfciSampleProviderProviderSetting1 ); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"Failed to Register %a. Status = %r \\n \" , mDfciSampleProviderProviderSetting1 . Id , Status )); } //We got here, this means all protocols were installed and we didn't exit early. //close the event as we don't need to be signaled again. (shouldn't happen anyway) gBS -> CloseEvent ( Event );","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/#setting-provider","text":"When you are writing your setting provider, keep in mind that other, similarly written libraries, are linked together. Define each common routine as STATIC to avoid conflicts with other providers. Refer to the UEFI general timeline here: Quite a few settings are only needed in late DXE, BDS or FrontPage. You may need access to settings values in PEI or in DXE prior to the starting of SettingsManager. To do this, add private methods to your settings library. Doing this will keep a single piece of code that accesses the nonvolatile storage for the settings. Here is a sample local setting function in the Dfci Sample Provider: // Here is where you would have private interfaces to get and or set a settings value EFI_STATUS OEM_GetSampleSetting1 ( OUT UINT8 * LocalSetting ) { UINTN LocalSettingsSize ; EFI_STATUS Status ; LocalSettingSize = sizeof ( * LocalSetting ); Status = DfciSampleProviderGet ( & mDfciSampleProviderProviderSetting1 , & LocalSettingSize , & LocalSetting ); return Status ; }","title":"Setting Provider"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciUiSupportLib/","text":"DfciUiSupportLib \u00b6 DfciUiSupportLib allows DFCI to communicate with the user during DFCI initialization, enrollment, or to indicate a non secure environment is available. Interfaces \u00b6 Interface Usage DfciUiIsManufacturingMode Returns TRUE or FALSE. Used to self OptIn a cert used for zero touch enrollment. If the device doesn't support a manufacturing mode, return FALSE. DfciUiIsAvailable For one touch enrollment, the user has to authorize the enrollment. Since DFCI normally runs before consoles are started, DFCI will wait until END_OF_DXE and then make sure the User Interface (UI) is available. If no UI is available at that time, the enrollment will fail. DfciUiDisplayMessageBox Displays a message box DfciUiDisplayPasswordDialog Displays a prompt for the ADMIN password DfciUiDisplayAuthDialog Displays a prompt for confirmation of an enrolling certificate. The response is the last two characters of the thumbprint. If there is an ADMIN password set, then this dialog will also request the ADMIN password. DfciUiExitSecurityBoundary The platform settings application usually runs in a secure state before variables are locked. The DFCI Menu application will call ExitSecurityBoundary before starting the network or performing USB operations to minimize the security risks associated with external accesses.","title":"Dfci Ui Support Lib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciUiSupportLib/#dfciuisupportlib","text":"DfciUiSupportLib allows DFCI to communicate with the user during DFCI initialization, enrollment, or to indicate a non secure environment is available.","title":"DfciUiSupportLib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciUiSupportLib/#interfaces","text":"Interface Usage DfciUiIsManufacturingMode Returns TRUE or FALSE. Used to self OptIn a cert used for zero touch enrollment. If the device doesn't support a manufacturing mode, return FALSE. DfciUiIsAvailable For one touch enrollment, the user has to authorize the enrollment. Since DFCI normally runs before consoles are started, DFCI will wait until END_OF_DXE and then make sure the User Interface (UI) is available. If no UI is available at that time, the enrollment will fail. DfciUiDisplayMessageBox Displays a message box DfciUiDisplayPasswordDialog Displays a prompt for the ADMIN password DfciUiDisplayAuthDialog Displays a prompt for confirmation of an enrolling certificate. The response is the last two characters of the thumbprint. If there is an ADMIN password set, then this dialog will also request the ADMIN password. DfciUiExitSecurityBoundary The platform settings application usually runs in a secure state before variables are locked. The DFCI Menu application will call ExitSecurityBoundary before starting the network or performing USB operations to minimize the security risks associated with external accesses.","title":"Interfaces"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/","text":"Platform Integration of DFCI \u00b6 This section of documentation is focused on UEFI firmware developers and helping them enable their platforms with the DFCI feature. DFCI consists mostly of a software feature that is written in the DXE phase of UEFI. It has numerous architecture and platform independent modules with only a few required platform libraries. It also requires the platform adhere to and use the DFCI components to ensure the DFCI features work as designed. Finally to enable an End-To-End management scenario there maybe custom requirements in adjacent UEFI firmware components. Dfci Menu application \u00b6 The DfciMenu application is optimized for mu_plus MsGraphicsPkg . It is VFR but since many platforms use custom layouts and graphical representation this area might need some adjustments. The DfciMenu application publishes a HII formset that should be located by your pre-boot UEFI menu application (e.g. \"FrontPage\") and displayed. Formset GUID: gDfciMenuFormsetGuid = {0x3b82283d, 0x7add, 0x4c6a, {0xad, 0x2b, 0x71, 0x9b, 0x8d, 0x7b, 0x77, 0xc9 }} Entry Form: #define DFCI_MENU_FORM_ID 0x2000 Source Location: DfciPkg\\Application\\DfciMenu DFCI DXE Drivers \u00b6 Dxe Driver Location DfciManager.efi DfciPkg/DfciManager/DfciManager.inf IdentityAndAuthManager.efi DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf SettingsManager.efi DfciPkg/SettingsManager/SettingsManagerDxe.inf DfciMenu.inf DfciPkg/Application/DfciMenu/DfciMenu.inf DFCI Core Libraries \u00b6 These DFCI Standard libraries are expected to be used as is for standard functionality. Library Location DfciRecoveryLib DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciXmlDeviceIdSchemaSupportLib DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlSettingSchemaSupportLib DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf ZeroTouchSettingsLib ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciSettingPermissionLib DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf DFCI Platform provided libraries \u00b6 The following libraries have to be provided by the platform: Library Documentation Function DfciDeviceIdSupportLib Documentation Provides SMBIOS information - Manufacturer, Product, and Serial number DfciGroupLib Documentation Provides lists of platform settings that are in the Dfci group settings. DfciUiSupportLib Documentation Provides UI for various user interactions DFCI Setting Providers \u00b6 Setting providers is how a platform provides a setting to DFCI Setting detailed overview Mu Changes \u00b6 DFCI Recovery service uses HTTPS certificates with Subject Alternative Names. This requires a source modification to NetworkPkg, removal of EFI_TLS_VERIFY_FLAG_NO_WILDCARDS from TlsConfigureSession() . Configure OpenSSL to support modern TLS ciphers Platform DSC statements \u00b6 Adding DFCI to your system consists of: Write your settings providers. Use DfciPkg/Library/DfciSampleProvider . Writing three library classes for the DfciDeviceIdSupportLib, DfciGroupLib, and DfciUiSupportLib. Adding the DSC sections below. Adding the FDF sections below. [ LibraryClasses.XXX ] DfciXmlSettingSchemaSupportLib|DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib|DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlDeviceIdSchemaSupportLib|DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib|DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf ZeroTouchSettingsLib|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciRecoveryLib|DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib|DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciDeviceIdSupportLib|YOURPLATFORMPKG/Library/DfciDeviceIdSupportLib/DfciDeviceIdSupportLib.inf DfciUiSupportLib|YOURPLATFORMPKG/Library/DfciUiSupportLib/DfciUiSupportLib.inf DfciGroupLib|YOURPLATFORMPKG/Library/DfciGroupLib/DfciGroups.inf [Components.XXX] DfciPkg/SettingsManager/SettingsManagerDxe.inf { #Platform should add all it settings libs here <LibraryClasses> NULL|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_FIRST_SETTING_PROVIDER.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_SECOND_SETTING_PROVIDER.inf NULL|DfciPkg/Library/DfciPasswordProvider/DfciPasswordProvider.inf NULL|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf NULL|DfciPkg/Library/DfciVirtualizationSettings/DfciVirtualizationSettings.inf DfciSettingPermissionLib|DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE } DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf DfciPkg/DfciManager/DfciManager.inf DfciPkg/Application/DfciMenu/DfciMenu.inf Platform FDF statements \u00b6 [ FV.YOUR_DXE_FV ] INF DfciPkg/SettingsManager/SettingsManagerDxe.inf INF DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf INF DfciPkg/Application/DfciMenu/DfciMenu.inf INF DfciPkg/DfciManager/DfciManager.inf","title":"Platform Integration Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#platform-integration-of-dfci","text":"This section of documentation is focused on UEFI firmware developers and helping them enable their platforms with the DFCI feature. DFCI consists mostly of a software feature that is written in the DXE phase of UEFI. It has numerous architecture and platform independent modules with only a few required platform libraries. It also requires the platform adhere to and use the DFCI components to ensure the DFCI features work as designed. Finally to enable an End-To-End management scenario there maybe custom requirements in adjacent UEFI firmware components.","title":"Platform Integration of DFCI"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-menu-application","text":"The DfciMenu application is optimized for mu_plus MsGraphicsPkg . It is VFR but since many platforms use custom layouts and graphical representation this area might need some adjustments. The DfciMenu application publishes a HII formset that should be located by your pre-boot UEFI menu application (e.g. \"FrontPage\") and displayed. Formset GUID: gDfciMenuFormsetGuid = {0x3b82283d, 0x7add, 0x4c6a, {0xad, 0x2b, 0x71, 0x9b, 0x8d, 0x7b, 0x77, 0xc9 }} Entry Form: #define DFCI_MENU_FORM_ID 0x2000 Source Location: DfciPkg\\Application\\DfciMenu","title":"Dfci Menu application"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-dxe-drivers","text":"Dxe Driver Location DfciManager.efi DfciPkg/DfciManager/DfciManager.inf IdentityAndAuthManager.efi DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf SettingsManager.efi DfciPkg/SettingsManager/SettingsManagerDxe.inf DfciMenu.inf DfciPkg/Application/DfciMenu/DfciMenu.inf","title":"DFCI DXE Drivers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-core-libraries","text":"These DFCI Standard libraries are expected to be used as is for standard functionality. Library Location DfciRecoveryLib DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciXmlDeviceIdSchemaSupportLib DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlSettingSchemaSupportLib DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf ZeroTouchSettingsLib ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciSettingPermissionLib DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf","title":"DFCI Core Libraries"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-platform-provided-libraries","text":"The following libraries have to be provided by the platform: Library Documentation Function DfciDeviceIdSupportLib Documentation Provides SMBIOS information - Manufacturer, Product, and Serial number DfciGroupLib Documentation Provides lists of platform settings that are in the Dfci group settings. DfciUiSupportLib Documentation Provides UI for various user interactions","title":"DFCI Platform provided libraries"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-setting-providers","text":"Setting providers is how a platform provides a setting to DFCI Setting detailed overview","title":"DFCI Setting Providers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#mu-changes","text":"DFCI Recovery service uses HTTPS certificates with Subject Alternative Names. This requires a source modification to NetworkPkg, removal of EFI_TLS_VERIFY_FLAG_NO_WILDCARDS from TlsConfigureSession() . Configure OpenSSL to support modern TLS ciphers","title":"Mu Changes"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#platform-dsc-statements","text":"Adding DFCI to your system consists of: Write your settings providers. Use DfciPkg/Library/DfciSampleProvider . Writing three library classes for the DfciDeviceIdSupportLib, DfciGroupLib, and DfciUiSupportLib. Adding the DSC sections below. Adding the FDF sections below. [ LibraryClasses.XXX ] DfciXmlSettingSchemaSupportLib|DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib|DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlDeviceIdSchemaSupportLib|DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib|DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf ZeroTouchSettingsLib|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciRecoveryLib|DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib|DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciDeviceIdSupportLib|YOURPLATFORMPKG/Library/DfciDeviceIdSupportLib/DfciDeviceIdSupportLib.inf DfciUiSupportLib|YOURPLATFORMPKG/Library/DfciUiSupportLib/DfciUiSupportLib.inf DfciGroupLib|YOURPLATFORMPKG/Library/DfciGroupLib/DfciGroups.inf [Components.XXX] DfciPkg/SettingsManager/SettingsManagerDxe.inf { #Platform should add all it settings libs here <LibraryClasses> NULL|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_FIRST_SETTING_PROVIDER.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_SECOND_SETTING_PROVIDER.inf NULL|DfciPkg/Library/DfciPasswordProvider/DfciPasswordProvider.inf NULL|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf NULL|DfciPkg/Library/DfciVirtualizationSettings/DfciVirtualizationSettings.inf DfciSettingPermissionLib|DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE } DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf DfciPkg/DfciManager/DfciManager.inf DfciPkg/Application/DfciMenu/DfciMenu.inf","title":"Platform DSC statements"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#platform-fdf-statements","text":"[ FV.YOUR_DXE_FV ] INF DfciPkg/SettingsManager/SettingsManagerDxe.inf INF DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf INF DfciPkg/Application/DfciMenu/DfciMenu.inf INF DfciPkg/DfciManager/DfciManager.inf","title":"Platform FDF statements"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/","text":"Microsoft DFCI Scenarios \u00b6 Overview \u00b6 Microsoft leverages DFCI to provide automated UEFI settings management via Microsoft Intune . Intune provides the IT manager with abstracted, easy button settings that are generally applicable to all platforms, for example, disable booting USB devices or disable all cameras. IT managers depend on the UEFI implementation to protect and enforce DFCI configurations such that they cannot be bypassed by an operating system or casual physical attacker. Windows Autopilot provides the trusted device ownership database, mapping devices to Azure Active Directory Tenants. The \"Microsoft Device Management Trust\" certificate must be included in UEFI to act as the root of trust for automated UEFI management. The Autopilot Service (APS) exposes a cloud endpoint to enable recovery from a BIOS menu in case the device can no longer boot due to misconfiguration or disk corruption. Microsoft DFCI Scenario Requirements \u00b6 PCs must include the DFCI feature in their UEFI PCs must be registered to the Windows Autopilot service by an OEM or Microsoft Cloud Solution Provider PCs must be managed with Microsoft Intune OEMs that support DFCI \u00b6 More are in the works... Lifecycle \u00b6 The DFCI lifecycle can be viewed as UEFI integration, device registration, profile creation, enrollment, management, retirement, & recovery. Stage Description UEFI Integration PCs must first include a UEFI BIOS that integrates the DFCI code and includes the Microsoft Device Management Trust certificate. Device Registration Device ownership must be registered via the Windows Autopilot program by an OEM or Microsoft Cloud Solution Provider Profile Creation An IT administrator leverages Intune to create DFCI Profiles for their devices. Enrollment The DFCI enrollment process is kicked off when a PC is enrolled into Intune and has a matching DFCI Profile. Enrollment includes Intune requesting enrollment packets from APS, sending the packets to the Windows UEFI configuration service provider (CSP) endpoints , the CSP writes the packets to UEFI variables, and triggers an OS reboot to allow UEFI firmware to process the DFCI packets. Management For day-to-day management, Intune creates device-specific packets, digitally signs them, and sends them through the same UEFI configuration service provider , UEFI variable, and reboot process. Retirement When a device is removed from Windows Autopilot, they are marked as unenrolled in APS. Intune will attempt to restore permissions (un-grey all settings) and remove its management authority from the device. Recovery Recovery shall be provided via a pre-boot UEFI menu, always available to a physically-present user, that can refresh DFCI configuration via web, USB, or other. Enrollment Flow \u00b6 Prior to the time of Enrollment, Microsoft Device Management Trust delegates management to the APS by signing a wildcard enrollment packet (targeting all manufacturer, model, & serial number) that authorizes enrollment of the APS certificate. At the request of Intune, the APS authorizes enrollment of a device, creates and signs per-device-targeted enrollment packets that enroll the Intune DFCI management certificate. The APS provides a level of indirection as well as an extra level of recovery via a web recovery service endpoint. The APS additionally configures recovery settings as well as permissions that deny Intune access to modify them. DEFINED: A device is considered \"enrolled\" when IS_OWNER_IDENTITY_ENROLLED(IdMask) returns TRUE, unenrolled when FALSE. Intune creates device-specific packets to provision the Intune authority and configure DFCI settings and permissions as specified in the matching Intune DFCI Profile. Intune delivers these packets via the Windows UEFI configuration service provider which writes them to UEFI mailboxes, to be processed by DFCI on the following boot. Device-specific packets include the SMBIOS manufacturer, model, & serial number, along with an anti-rollback count, to be leveraged by the UEFI DFCI code to determine applicability. Retirement & Recovery Flows \u00b6 Retirement \u00b6 An IT administrator leverages the Intune console to remove devices from Autopilot. Intune creates and sends device-specific packets that both restore DFCI permissions (effectively un-managing settings, making them available in the BIOS menu) and remove the Intune authority from DFCI. Note that this does not restore the settings to default values, they remain as is. Intune also notifies APS that the device is in the unenrolled state. If the device owner wants to further remove the APS authority and/or opt-out of DFCI management, they must leverage the Recovery flow. Recovery \u00b6 Recovery is essential because UEFI misconfiguration may prevent booting to an operating system, for example if USB and network boot are disabled and the hard disk becomes corrupted. When a device is enrolled, UEFI must provide alternative mechanisms for the physically-present user to place packets in the DFCI request mailboxes - this MUST NOT be blocked by a BIOS password or similar. Note that when a device is not enrolled, a BIOS password should prevent access to DFCI enrollment by a physically-present user until they have entered the correct credential. The APS keeps track of the enrollment state of devices. When an administrator removes a device from Autopilot, APS creates signed, device-specific un-enrollment packets and makes them available via a REST endpoint at DFCI_PRIVATE_SETTING_ID__DFCI_RECOVERY_URL. These packets should delete the Intune and APS certificates and provide the local user with access to all settings (they should no longer be greyed out in BIOS menus). Note that retirement does not restore visible settings to their default values. Standard Settings \u00b6 The following standard settings are defined for DFCI v1.0, will be exposed by Intune, and Settings Providers for them must be implemented by the UEFI provider. Note that for device management, the settings apply only to built-in devices, not externally attached devices. All cameras All audio (microphones & speakers) All radios (Wi-Fi, broadband, NFC, BT, ...) CPU & IO virtualization (exposed, enabled for use by the OS so that Virtualization Based Security may be enabled) Boot to external media (e.g. USB, SD, ...) Boot via on-board network devices Refer to the \"Group Settings\" section of DfciSettings.h Device Ownership \u00b6 The Windows Autopilot database is used for authorizing DFCI management. It includes a map of device identifiers an owner's Azure Active Directory tenant. It is populated as part of the Windows Autopilot program by OEMs and Microsoft-authorized Cloud Solution Providers . The DFCI UEFI code leverages the SMBIOS manufacturer, model, and serial number for packet targeting. DFCI is a Windows Autopilot feature, available from Microsoft Intune. Management Authorities \u00b6 DFCI supports UEFI settings and permission management by multiple entities. In the recommended configuration, systems are shipped with 1 Microsoft public certificate included, \"CN=Microsoft Device Management Trust\", which provides the root of trust for management. This certificate authorizes Microsoft to automatically enroll management delegates with varying permissions. \"Microsoft Device Management Trust\" is only used to delegate management to APS and to provide second-chance recovery. APS in turn delegates management to Intune and provides a REST endpoint for online recovery. After enrollment, Intune performs the day-to-day UEFI management, whereas APS and Device Management Trust authorities provide various recovery paths. Authority DFCI ID Usage \"CN=Microsoft Device Management Trust\" DFCI_IDENTITY_SIGNER_ZTD Allowed to enroll a management delegates without a physical presence prompt. Enrolls the Autopilot Service authority. Can act as a backup recovery service. Autopilot Service (APS) DFCI_IDENTITY_SIGNER_OWNER Enrolls Microsoft Intune as a delegated management provider. Provides an online recovery service in case the OS is disconnected from Intune. Microsoft Intune DFCI_IDENTITY_SIGNER_USER Performs day-to-day UEFI settings and permissions management Security & Privacy Considerations \u00b6 When a DFCI owner is enrolled, DFCI must take precedence over any other UEFI management solution. Physically-present user, including authenticated, may not bypass DFCI permissions on DFCI_IDENTITY_LOCAL . Protection: The hardware/firmware implementation of DFCI must protect DFCI configuration code and data such that they cannot by bypassed by an operating system or casual physical attacker. For example, non-volatile storage that is locked prior to Boot Device Selection. Enforcement: The hardware/firmware implementation of DFCI must enforce DFCI configurations such that they cannot by bypassed by an operating system or casual physical attacker. For example, power to devices is disabled or busses disabled and the configuration is locked prior to Boot Device Selection. It is recommended to include an \"Opt Out\" button that enables a physically-present user on an unenrolled device to eject the DFCI_IDENTITY_SIGNER_ZTD from DFCI. The effectively disables automated management - any enrollment attempt will display a red prompt at boot. A user could then prevent enrollment by configuring a BIOS password or enroll their own User certificate (proceeding through the red prompt). Online Recovery via the Autopilot Service \u00b6 The Recovery REST interface includes machine identities. Before transferring machine identities, the server's authenticity should be verified against DFCI_PRIVATE_SETTING_ID__DFCI_HTTPS_CERT. After authenticating the server, the network traffic, including machine identities, are kept private by HTTPS encryption. But wait, there's more... the server certificate is updated regularly, so UEFI must first ensure it has the up-to-date DFCI_PRIVATE_SETTING_ID__DFCI_HTTPS_CERT. DFCI_PRIVATE_SETTING_ID__DFCI_BOOTSTRAP_URL provides a REST API to download a signed settings packet containing DFCI_PRIVATE_SETTING_ID__DFCI_HTTPS_CERT. For this workflow, the server is not authenticated, but the payload will be authenticated prior to consumption. Unknown Certificate Enrollment \u00b6 This is not a Microsoft-supported scenario but might be encountered during development and testing. On an unenrolled system, if enrollment packets are supplied to the DFCI mailboxes that are signed by an unknown certificate, a red authorization prompt is displayed during boot. The prompt requests the physically-present user to authorize the enrollment of the unknown certificate by typing the last 2 characters of the certificate's SHA-1 thumbprint. If a BIOS password is configured, the password must be entered prior to authorizing the enrollment. This is designed to avoid accidental or nefarious enrollment while allowing for valid custom identity management.","title":"Scenarios"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#microsoft-dfci-scenarios","text":"","title":"Microsoft DFCI Scenarios"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#overview","text":"Microsoft leverages DFCI to provide automated UEFI settings management via Microsoft Intune . Intune provides the IT manager with abstracted, easy button settings that are generally applicable to all platforms, for example, disable booting USB devices or disable all cameras. IT managers depend on the UEFI implementation to protect and enforce DFCI configurations such that they cannot be bypassed by an operating system or casual physical attacker. Windows Autopilot provides the trusted device ownership database, mapping devices to Azure Active Directory Tenants. The \"Microsoft Device Management Trust\" certificate must be included in UEFI to act as the root of trust for automated UEFI management. The Autopilot Service (APS) exposes a cloud endpoint to enable recovery from a BIOS menu in case the device can no longer boot due to misconfiguration or disk corruption.","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#microsoft-dfci-scenario-requirements","text":"PCs must include the DFCI feature in their UEFI PCs must be registered to the Windows Autopilot service by an OEM or Microsoft Cloud Solution Provider PCs must be managed with Microsoft Intune","title":"Microsoft DFCI Scenario Requirements"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#oems-that-support-dfci","text":"More are in the works...","title":"OEMs that support DFCI"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#lifecycle","text":"The DFCI lifecycle can be viewed as UEFI integration, device registration, profile creation, enrollment, management, retirement, & recovery. Stage Description UEFI Integration PCs must first include a UEFI BIOS that integrates the DFCI code and includes the Microsoft Device Management Trust certificate. Device Registration Device ownership must be registered via the Windows Autopilot program by an OEM or Microsoft Cloud Solution Provider Profile Creation An IT administrator leverages Intune to create DFCI Profiles for their devices. Enrollment The DFCI enrollment process is kicked off when a PC is enrolled into Intune and has a matching DFCI Profile. Enrollment includes Intune requesting enrollment packets from APS, sending the packets to the Windows UEFI configuration service provider (CSP) endpoints , the CSP writes the packets to UEFI variables, and triggers an OS reboot to allow UEFI firmware to process the DFCI packets. Management For day-to-day management, Intune creates device-specific packets, digitally signs them, and sends them through the same UEFI configuration service provider , UEFI variable, and reboot process. Retirement When a device is removed from Windows Autopilot, they are marked as unenrolled in APS. Intune will attempt to restore permissions (un-grey all settings) and remove its management authority from the device. Recovery Recovery shall be provided via a pre-boot UEFI menu, always available to a physically-present user, that can refresh DFCI configuration via web, USB, or other.","title":"Lifecycle"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#enrollment-flow","text":"Prior to the time of Enrollment, Microsoft Device Management Trust delegates management to the APS by signing a wildcard enrollment packet (targeting all manufacturer, model, & serial number) that authorizes enrollment of the APS certificate. At the request of Intune, the APS authorizes enrollment of a device, creates and signs per-device-targeted enrollment packets that enroll the Intune DFCI management certificate. The APS provides a level of indirection as well as an extra level of recovery via a web recovery service endpoint. The APS additionally configures recovery settings as well as permissions that deny Intune access to modify them. DEFINED: A device is considered \"enrolled\" when IS_OWNER_IDENTITY_ENROLLED(IdMask) returns TRUE, unenrolled when FALSE. Intune creates device-specific packets to provision the Intune authority and configure DFCI settings and permissions as specified in the matching Intune DFCI Profile. Intune delivers these packets via the Windows UEFI configuration service provider which writes them to UEFI mailboxes, to be processed by DFCI on the following boot. Device-specific packets include the SMBIOS manufacturer, model, & serial number, along with an anti-rollback count, to be leveraged by the UEFI DFCI code to determine applicability.","title":"Enrollment Flow"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#retirement-recovery-flows","text":"","title":"Retirement &amp; Recovery Flows"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#retirement","text":"An IT administrator leverages the Intune console to remove devices from Autopilot. Intune creates and sends device-specific packets that both restore DFCI permissions (effectively un-managing settings, making them available in the BIOS menu) and remove the Intune authority from DFCI. Note that this does not restore the settings to default values, they remain as is. Intune also notifies APS that the device is in the unenrolled state. If the device owner wants to further remove the APS authority and/or opt-out of DFCI management, they must leverage the Recovery flow.","title":"Retirement"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#recovery","text":"Recovery is essential because UEFI misconfiguration may prevent booting to an operating system, for example if USB and network boot are disabled and the hard disk becomes corrupted. When a device is enrolled, UEFI must provide alternative mechanisms for the physically-present user to place packets in the DFCI request mailboxes - this MUST NOT be blocked by a BIOS password or similar. Note that when a device is not enrolled, a BIOS password should prevent access to DFCI enrollment by a physically-present user until they have entered the correct credential. The APS keeps track of the enrollment state of devices. When an administrator removes a device from Autopilot, APS creates signed, device-specific un-enrollment packets and makes them available via a REST endpoint at DFCI_PRIVATE_SETTING_ID__DFCI_RECOVERY_URL. These packets should delete the Intune and APS certificates and provide the local user with access to all settings (they should no longer be greyed out in BIOS menus). Note that retirement does not restore visible settings to their default values.","title":"Recovery"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#standard-settings","text":"The following standard settings are defined for DFCI v1.0, will be exposed by Intune, and Settings Providers for them must be implemented by the UEFI provider. Note that for device management, the settings apply only to built-in devices, not externally attached devices. All cameras All audio (microphones & speakers) All radios (Wi-Fi, broadband, NFC, BT, ...) CPU & IO virtualization (exposed, enabled for use by the OS so that Virtualization Based Security may be enabled) Boot to external media (e.g. USB, SD, ...) Boot via on-board network devices Refer to the \"Group Settings\" section of DfciSettings.h","title":"Standard Settings"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#device-ownership","text":"The Windows Autopilot database is used for authorizing DFCI management. It includes a map of device identifiers an owner's Azure Active Directory tenant. It is populated as part of the Windows Autopilot program by OEMs and Microsoft-authorized Cloud Solution Providers . The DFCI UEFI code leverages the SMBIOS manufacturer, model, and serial number for packet targeting. DFCI is a Windows Autopilot feature, available from Microsoft Intune.","title":"Device Ownership"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#management-authorities","text":"DFCI supports UEFI settings and permission management by multiple entities. In the recommended configuration, systems are shipped with 1 Microsoft public certificate included, \"CN=Microsoft Device Management Trust\", which provides the root of trust for management. This certificate authorizes Microsoft to automatically enroll management delegates with varying permissions. \"Microsoft Device Management Trust\" is only used to delegate management to APS and to provide second-chance recovery. APS in turn delegates management to Intune and provides a REST endpoint for online recovery. After enrollment, Intune performs the day-to-day UEFI management, whereas APS and Device Management Trust authorities provide various recovery paths. Authority DFCI ID Usage \"CN=Microsoft Device Management Trust\" DFCI_IDENTITY_SIGNER_ZTD Allowed to enroll a management delegates without a physical presence prompt. Enrolls the Autopilot Service authority. Can act as a backup recovery service. Autopilot Service (APS) DFCI_IDENTITY_SIGNER_OWNER Enrolls Microsoft Intune as a delegated management provider. Provides an online recovery service in case the OS is disconnected from Intune. Microsoft Intune DFCI_IDENTITY_SIGNER_USER Performs day-to-day UEFI settings and permissions management","title":"Management Authorities"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#security-privacy-considerations","text":"When a DFCI owner is enrolled, DFCI must take precedence over any other UEFI management solution. Physically-present user, including authenticated, may not bypass DFCI permissions on DFCI_IDENTITY_LOCAL . Protection: The hardware/firmware implementation of DFCI must protect DFCI configuration code and data such that they cannot by bypassed by an operating system or casual physical attacker. For example, non-volatile storage that is locked prior to Boot Device Selection. Enforcement: The hardware/firmware implementation of DFCI must enforce DFCI configurations such that they cannot by bypassed by an operating system or casual physical attacker. For example, power to devices is disabled or busses disabled and the configuration is locked prior to Boot Device Selection. It is recommended to include an \"Opt Out\" button that enables a physically-present user on an unenrolled device to eject the DFCI_IDENTITY_SIGNER_ZTD from DFCI. The effectively disables automated management - any enrollment attempt will display a red prompt at boot. A user could then prevent enrollment by configuring a BIOS password or enroll their own User certificate (proceeding through the red prompt).","title":"Security &amp; Privacy Considerations"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#online-recovery-via-the-autopilot-service","text":"The Recovery REST interface includes machine identities. Before transferring machine identities, the server's authenticity should be verified against DFCI_PRIVATE_SETTING_ID__DFCI_HTTPS_CERT. After authenticating the server, the network traffic, including machine identities, are kept private by HTTPS encryption. But wait, there's more... the server certificate is updated regularly, so UEFI must first ensure it has the up-to-date DFCI_PRIVATE_SETTING_ID__DFCI_HTTPS_CERT. DFCI_PRIVATE_SETTING_ID__DFCI_BOOTSTRAP_URL provides a REST API to download a signed settings packet containing DFCI_PRIVATE_SETTING_ID__DFCI_HTTPS_CERT. For this workflow, the server is not authenticated, but the payload will be authenticated prior to consumption.","title":"Online Recovery via the Autopilot Service"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#unknown-certificate-enrollment","text":"This is not a Microsoft-supported scenario but might be encountered during development and testing. On an unenrolled system, if enrollment packets are supplied to the DFCI mailboxes that are signed by an unknown certificate, a red authorization prompt is displayed during boot. The prompt requests the physically-present user to authorize the enrollment of the unknown certificate by typing the last 2 characters of the certificate's SHA-1 thumbprint. If a BIOS password is configured, the password must be entered prior to authorizing the enrollment. This is designed to avoid accidental or nefarious enrollment while allowing for valid custom identity management.","title":"Unknown Certificate Enrollment"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/","text":"Identity and Authentication Manager \u00b6 Basic overview of the IdentityAndAuthManager module. File Overview \u00b6 IdentityAndAuthManager.H \u00b6 Private header file defining private functions for use across module Define the internal structure that holds the auth handle to identity mapping IdentityAndAuthManagerDxe \u00b6 Implement the Dxe specific parts of this. Including: Event handling Protocol access Protocol installation AuthManager.C \u00b6 Provide the implementation for the auth protocol functions AuthManagerProvision.C \u00b6 Support using Variable to set, change, or remove the AuthManager Key based identities AuthManagerProvisionedData.C \u00b6 Support NV storage of Provisioned Data. This manages loading internal store and saving changes to internal store. This differs from the Provision.c file in that this has nothing to do with User input or applying user changes. This is internal to the module only. IdentityManager.C \u00b6 Support the get identity functionality Dispose Auth Handle Private Identity / auth token map management (Add, Free, Find) Add security TODO IdentityAndAuthManagerDxe.INF \u00b6 Dxe Module inf file DfciAuthentication.h PUBLIC HEADER FILE \u00b6 Defines the DXE protocol to access Identity and Auth management","title":"Identity And Auth Manager"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identity-and-authentication-manager","text":"Basic overview of the IdentityAndAuthManager module.","title":"Identity and Authentication Manager"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#file-overview","text":"","title":"File Overview"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identityandauthmanagerh","text":"Private header file defining private functions for use across module Define the internal structure that holds the auth handle to identity mapping","title":"IdentityAndAuthManager.H"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identityandauthmanagerdxe","text":"Implement the Dxe specific parts of this. Including: Event handling Protocol access Protocol installation","title":"IdentityAndAuthManagerDxe"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#authmanagerc","text":"Provide the implementation for the auth protocol functions","title":"AuthManager.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#authmanagerprovisionc","text":"Support using Variable to set, change, or remove the AuthManager Key based identities","title":"AuthManagerProvision.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#authmanagerprovisioneddatac","text":"Support NV storage of Provisioned Data. This manages loading internal store and saving changes to internal store. This differs from the Provision.c file in that this has nothing to do with User input or applying user changes. This is internal to the module only.","title":"AuthManagerProvisionedData.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identitymanagerc","text":"Support the get identity functionality Dispose Auth Handle Private Identity / auth token map management (Add, Free, Find) Add security TODO","title":"IdentityManager.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identityandauthmanagerdxeinf","text":"Dxe Module inf file","title":"IdentityAndAuthManagerDxe.INF"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#dfciauthenticationh-public-header-file","text":"Defines the DXE protocol to access Identity and Auth management","title":"DfciAuthentication.h  PUBLIC HEADER FILE"},{"location":"dyn/mu_plus/DfciPkg/Library/DfciSampleProvider/readme/","text":"Dfci Sample Provider \u00b6 This is a DXE driver that publishes the gDfciSettingsProviderSupportProtocolGuid protocol, which is a settings provider for DFCI. This is not to be used in production but is provided as a sample for reference when creating your own provider. For more information, please refer to the DFCI documentation here Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Dfci Sample Provider"},{"location":"dyn/mu_plus/DfciPkg/Library/DfciSampleProvider/readme/#dfci-sample-provider","text":"This is a DXE driver that publishes the gDfciSettingsProviderSupportProtocolGuid protocol, which is a settings provider for DFCI. This is not to be used in production but is provided as a sample for reference when creating your own provider. For more information, please refer to the DFCI documentation here","title":"Dfci Sample Provider"},{"location":"dyn/mu_plus/DfciPkg/Library/DfciSampleProvider/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/","text":"Verify DfciDevicedIdLib library functionality \u00b6 The library DfciDeviceIdLib provided Dfci with platform information that Dfci needs. This include the manufacturer name, product name, and serial number. Dfci has limit on the characters supported, and the length of the strings returned. Device Id Library rules: The following five characters are not allowed: \" ' < > & The maximum string length is 64 characters plus a terminating '\\0' '\\0' is a required terminator. The interfaces return the string and the size of the string (including the '\\0'). The string is a valid UTF-8 string (ie, no 8-bit ASCII) About \u00b6 These tests verify that the DeviceIdLib Library functions properly. DeviceIdIdTestApp \u00b6 This application consumes the DfciDeviceIdLib executed test cases for the verification of the Device Id Strings. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Devicd Id Test"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#verify-dfcidevicedidlib-library-functionality","text":"The library DfciDeviceIdLib provided Dfci with platform information that Dfci needs. This include the manufacturer name, product name, and serial number. Dfci has limit on the characters supported, and the length of the strings returned. Device Id Library rules: The following five characters are not allowed: \" ' < > & The maximum string length is 64 characters plus a terminating '\\0' '\\0' is a required terminator. The interfaces return the string and the size of the string (including the '\\0'). The string is a valid UTF-8 string (ie, no 8-bit ASCII)","title":"Verify DfciDevicedIdLib library functionality"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#about","text":"These tests verify that the DeviceIdLib Library functions properly.","title":"About"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#deviceididtestapp","text":"This application consumes the DfciDeviceIdLib executed test cases for the verification of the Device Id Strings.","title":"DeviceIdIdTestApp"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/","text":"Testing DFCI \u00b6 This describes the test structure for insuring DFCI operates properly. A Host System (HOST) to run the test cases. A Device Under Test (DUT) to be tested. with the new DFCI supported also running current Windows. Both systems on the same network. Overview \u00b6 The DFCI tests are a collection of Robot Framework test cases. Each Robot Framework test case collection is contained in a directory, and, as a minimum, contains a run.robot file. Each test case collection is run manually in a proscribed order, and its status is verified before running the next test case. The tests must be run in order, as they alter the system state, much like the real usage of DFCI. Equipment needed \u00b6 The following equipment is needed to run the DFCI tests: A system to run the test cases running a current version of Windows. A System Under Test also running current version of Windows. Both systems able to communicate with each other across the same network. Optional equipment is a mechanism to collect firmware logs from the system under test. Included is a serial port support function that looks for an FTDI serial connection. See the Platforms\\SimpleFTDI folder. Setting up the Device Under Test (DUT) \u00b6 Copy the files needed for the DUT. There is a script to help you do this. With a removable device at drive letter D for example, issue the command DeviceUnderTest\\CollectFilesForDut.cmd D:\\DfciSetup . This will create a directory on the USB key named DfciSetup with the required files for setting up the remote server. Mount the USB device on the DUT and run the SetupDUT.cmd . This will download and install Python 3.7.4, robotframework, robotremoteserver, and pypiwin32. In addition, the SetupDUT command will update the firewall for the robot framework testing, and a make a couple of configuration changes to Windows for a better test experience. Setting up the HOST system \u00b6 The HOST system requires the following software (NOTE - There are dependencies on x86-64 versions of Windows): A current version of Windows 10 x86-64. The current Windows SDK, available here Windows SDK . Python 3.7.4 x86-64 (the version tested), available here Python 3.7.4 . python -m pip install pip --upgrade python -m pip install robotframework NOTE: Dfci now requires RobotFramework 3.2. Until 3.2 is the default, install using pip install --pre --upgrade robotframework python -m pip install edk2-pytool-library Copy the DfciTests directory, including all of the contents of the subdirectories, onto the HOST system. Test Cases Collections \u00b6 Table of DFCI Test case collections: | Test Case Collection | Description of Test Case | | ----- | ----- | ----- | | DFCI_CertChainingTest | Verifies that a ZeroTouch enroll actually prompts for authorization to Enroll when the enroll package is not signed by the proper key.| | DFCI_InitialState | Verifies that the firmware indicates support for DFCI and that the system is not already enrolled into DFCI. | | DFCI_InTuneBadUpdate | Tries to apply a settings package signed with the wrong key | | DFCI_InTunePermissions | Applies multiple sets of permissions to an InTune Enrolled system. | | DFCI_InTuneEnroll | Applies a InTune Owner, an InTune Manager, and the appropriate permissions and settings. | | DFCI_InTuneRollCerts | Updates the Owner and Manager certificates. This test can be run multiple times as it just swaps between two sets of certificates. | | DFCI_InTuneSettings | Applies multiple sets of settings to a InTune Enrolled system. | | DFCI_InTuneUnenroll | Applies an InTune Owner unenroll package, that removes both the InTune Owner and the InTune Manager, resets the Permission Database, and restores settings marked No UI to their default state. | Note on the firmware for testing DFCI \u00b6 Most of DFCI functionality can be tested without regard of the Zero Touch certificate. To test functionality of the Zero Touch feature, the firmware needs to be built with the ZTD_Leaf.cer file instead of the ZtdRecovery.cer file. To do this, change your platform .fdf file from: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = ZeroTouchPkg / Certs / ZeroTouch / ZtdRecovery . cer } to: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = DfciPkg / UnitTests / DfciTests / ZTD_Leaf . cer } WARNING: Do not ship with the ZTD_Leaf.cer certificate in your firmware Running The First Test Case \u00b6 Run the first test as shown replacing 11.11.11.211 with the actual IP address of the DUT. You should expect to see similar output with all four tests passing. DfciTests>RunDfciTest.bat TestCases\\DFCI_InitialState 11.11.11.211 DfciTests>python.exe -m robot.run -L TRACE -x DFCI_InitialState.xml -A Platforms\\SimpleFTDI\\Args.txt -v IP_OF_DUT:11.11.11.211 -v TEST_OUTPUT_BASE:C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224 -d C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224 TestCases\\DFCI_InitialState\\run.robot ============================================================================== Run :: DFCI Initial State test - Verifies that there are no enrolled identi... ============================================================================== Ensure Mailboxes Are Clean .. .L:\\Common\\MU\\DfciPkg\\UnitTests\\DfciTests\\TestCases\\DFCI_InitialState\\run.robot Ensure Mailboxes Are Clean | PASS | ------------------------------------------------------------------------------ Get the starting DFCI Settings | PASS | ------------------------------------------------------------------------------ Obtain Target Parameters From Target | PASS | ------------------------------------------------------------------------------ Process Complete Testcase List ..Initializing testcases ..Running test Process Complete Testcase List | PASS | ------------------------------------------------------------------------------ Run :: DFCI Initial State test - Verifies that there are no enroll... | PASS | 4 critical tests, 4 passed, 0 failed 4 tests total, 4 passed, 0 failed ============================================================================== Output: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\output.xml XUnit: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\DFCI_InitialState.xml Log: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\log.html Report: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\report.html Standard Testing \u00b6 Starting with a DUT that is not enrolled in DFCI, run the tests in the following order: DFCI_InitialState DFCI_InTuneEnroll DFCI_InTuneRollCerts DFCI_InTunePermissions DFCI_InTuneSettings DFCI_InTuneBadUpdate DFCI_InTuneUnenroll Steps 3 through 6 can and should be repeated in any order. Extended Testing \u00b6 This tests also start with a DUT that is not enrolled in DFCI, and will leave the system not enrolled if it completes successfully. DFCI_CertChainingTest Recovering from errors \u00b6 Code issues an present issues with DFCI that may require deleting the Identity and Permission data bases. Using privileged access of a DUT that unlocks the varstore, you can delete the two master variables of DFCI. These variable are: _SPP _IPCVN USB Refresh Test \u00b6 The test cases DFCI_InTuneEnroll and DFCI_InTuneUnenroll have a GenUsb.bat file. The GenUsb.bat file will generate a .dfi file that UEFI management menu can read. GenUsb MFG_NAME PRODUCT_NAME SERIAL_NUMBER If there is a space or other special characters, add double quotes as in: GenUsb Fabrikam \"Fabrikam Spelunker Kit\" \"SN-47599011345\"","title":"Dfci Tests"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#testing-dfci","text":"This describes the test structure for insuring DFCI operates properly. A Host System (HOST) to run the test cases. A Device Under Test (DUT) to be tested. with the new DFCI supported also running current Windows. Both systems on the same network.","title":"Testing DFCI"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#overview","text":"The DFCI tests are a collection of Robot Framework test cases. Each Robot Framework test case collection is contained in a directory, and, as a minimum, contains a run.robot file. Each test case collection is run manually in a proscribed order, and its status is verified before running the next test case. The tests must be run in order, as they alter the system state, much like the real usage of DFCI.","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#equipment-needed","text":"The following equipment is needed to run the DFCI tests: A system to run the test cases running a current version of Windows. A System Under Test also running current version of Windows. Both systems able to communicate with each other across the same network. Optional equipment is a mechanism to collect firmware logs from the system under test. Included is a serial port support function that looks for an FTDI serial connection. See the Platforms\\SimpleFTDI folder.","title":"Equipment needed"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#setting-up-the-device-under-test-dut","text":"Copy the files needed for the DUT. There is a script to help you do this. With a removable device at drive letter D for example, issue the command DeviceUnderTest\\CollectFilesForDut.cmd D:\\DfciSetup . This will create a directory on the USB key named DfciSetup with the required files for setting up the remote server. Mount the USB device on the DUT and run the SetupDUT.cmd . This will download and install Python 3.7.4, robotframework, robotremoteserver, and pypiwin32. In addition, the SetupDUT command will update the firewall for the robot framework testing, and a make a couple of configuration changes to Windows for a better test experience.","title":"Setting up the Device Under Test (DUT)"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#setting-up-the-host-system","text":"The HOST system requires the following software (NOTE - There are dependencies on x86-64 versions of Windows): A current version of Windows 10 x86-64. The current Windows SDK, available here Windows SDK . Python 3.7.4 x86-64 (the version tested), available here Python 3.7.4 . python -m pip install pip --upgrade python -m pip install robotframework NOTE: Dfci now requires RobotFramework 3.2. Until 3.2 is the default, install using pip install --pre --upgrade robotframework python -m pip install edk2-pytool-library Copy the DfciTests directory, including all of the contents of the subdirectories, onto the HOST system.","title":"Setting up the HOST system"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#test-cases-collections","text":"Table of DFCI Test case collections: | Test Case Collection | Description of Test Case | | ----- | ----- | ----- | | DFCI_CertChainingTest | Verifies that a ZeroTouch enroll actually prompts for authorization to Enroll when the enroll package is not signed by the proper key.| | DFCI_InitialState | Verifies that the firmware indicates support for DFCI and that the system is not already enrolled into DFCI. | | DFCI_InTuneBadUpdate | Tries to apply a settings package signed with the wrong key | | DFCI_InTunePermissions | Applies multiple sets of permissions to an InTune Enrolled system. | | DFCI_InTuneEnroll | Applies a InTune Owner, an InTune Manager, and the appropriate permissions and settings. | | DFCI_InTuneRollCerts | Updates the Owner and Manager certificates. This test can be run multiple times as it just swaps between two sets of certificates. | | DFCI_InTuneSettings | Applies multiple sets of settings to a InTune Enrolled system. | | DFCI_InTuneUnenroll | Applies an InTune Owner unenroll package, that removes both the InTune Owner and the InTune Manager, resets the Permission Database, and restores settings marked No UI to their default state. |","title":"Test Cases Collections"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#note-on-the-firmware-for-testing-dfci","text":"Most of DFCI functionality can be tested without regard of the Zero Touch certificate. To test functionality of the Zero Touch feature, the firmware needs to be built with the ZTD_Leaf.cer file instead of the ZtdRecovery.cer file. To do this, change your platform .fdf file from: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = ZeroTouchPkg / Certs / ZeroTouch / ZtdRecovery . cer } to: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = DfciPkg / UnitTests / DfciTests / ZTD_Leaf . cer } WARNING: Do not ship with the ZTD_Leaf.cer certificate in your firmware","title":"Note on the firmware for testing DFCI"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#running-the-first-test-case","text":"Run the first test as shown replacing 11.11.11.211 with the actual IP address of the DUT. You should expect to see similar output with all four tests passing. DfciTests>RunDfciTest.bat TestCases\\DFCI_InitialState 11.11.11.211 DfciTests>python.exe -m robot.run -L TRACE -x DFCI_InitialState.xml -A Platforms\\SimpleFTDI\\Args.txt -v IP_OF_DUT:11.11.11.211 -v TEST_OUTPUT_BASE:C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224 -d C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224 TestCases\\DFCI_InitialState\\run.robot ============================================================================== Run :: DFCI Initial State test - Verifies that there are no enrolled identi... ============================================================================== Ensure Mailboxes Are Clean .. .L:\\Common\\MU\\DfciPkg\\UnitTests\\DfciTests\\TestCases\\DFCI_InitialState\\run.robot Ensure Mailboxes Are Clean | PASS | ------------------------------------------------------------------------------ Get the starting DFCI Settings | PASS | ------------------------------------------------------------------------------ Obtain Target Parameters From Target | PASS | ------------------------------------------------------------------------------ Process Complete Testcase List ..Initializing testcases ..Running test Process Complete Testcase List | PASS | ------------------------------------------------------------------------------ Run :: DFCI Initial State test - Verifies that there are no enroll... | PASS | 4 critical tests, 4 passed, 0 failed 4 tests total, 4 passed, 0 failed ============================================================================== Output: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\output.xml XUnit: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\DFCI_InitialState.xml Log: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\log.html Report: C:\\TestLogs\\robot\\DFCI_InitialState\\logs_20191113_121224\\report.html","title":"Running The First Test Case"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#standard-testing","text":"Starting with a DUT that is not enrolled in DFCI, run the tests in the following order: DFCI_InitialState DFCI_InTuneEnroll DFCI_InTuneRollCerts DFCI_InTunePermissions DFCI_InTuneSettings DFCI_InTuneBadUpdate DFCI_InTuneUnenroll Steps 3 through 6 can and should be repeated in any order.","title":"Standard Testing"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#extended-testing","text":"This tests also start with a DUT that is not enrolled in DFCI, and will leave the system not enrolled if it completes successfully. DFCI_CertChainingTest","title":"Extended Testing"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#recovering-from-errors","text":"Code issues an present issues with DFCI that may require deleting the Identity and Permission data bases. Using privileged access of a DUT that unlocks the varstore, you can delete the two master variables of DFCI. These variable are: _SPP _IPCVN","title":"Recovering from errors"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#usb-refresh-test","text":"The test cases DFCI_InTuneEnroll and DFCI_InTuneUnenroll have a GenUsb.bat file. The GenUsb.bat file will generate a .dfi file that UEFI management menu can read. GenUsb MFG_NAME PRODUCT_NAME SERIAL_NUMBER If there is a space or other special characters, add double quotes as in: GenUsb Fabrikam \"Fabrikam Spelunker Kit\" \"SN-47599011345\"","title":"USB Refresh Test"},{"location":"dyn/mu_plus/HidPkg/Readme/","text":"HID support package \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 The purpose of this package is to provide a generic Boot HID layer that can be used so that devices that implement the Boot Mouse and Boot Keyboard HID interfaces defined in the USB 1.1 spec can share a common HID processing layer. This avoids duplication of the HID parsing/processing code and provides a natural interface layer where various HID-supporting hardware can plug into the UEFI input stack. Adding support for new hardware that wants to plug into HID Keyboard or Mouse simply requires implementing HidKeyboardProtocol or HidMouseProtocol and installing instances of those protocols on the controller handle for the hardware. This greatly simplifies the process of supporting hardware that is already designed to support Boot Mouse or Boot Keyboard HID spec, since such hardware is already producing reports in the same format that the HidKeyboardProtocol or HidMouseProtocol expects. Integration Guide \u00b6 To use the HidPkg, include the following drivers in your build: FDF: #HID Support INF HidPkg/HidKeyboardDxe/HidKeyboardDxe.inf INF HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe.inf DSC: #HID Support HidPkg/HidKeyboardDxe/HidKeyboardDxe.inf HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe.inf Then, include low-level drivers that produce the HID protocols. This package includes two sample drivers for USB keyboard and Mouse that can be used in place of the standard USB keyboard and Mouse drivers. A platform may add additional drivers to support proprietary hardware such as keyboards/track pads/ touchscreens that support the HID protocol definitions in this package. To include the sample USB HID drivers, remove the MdeModulePkg versions and replace them with the sample drivers from this package. FDF: -INF MdeModulePkg/Bus/Usb/UsbKbDxe/UsbKbDxe.inf -INF MdeModulePkg/Bus/Usb/UsbMouseAbsolutePointerDxe/UsbMouseAbsolutePointerDxe.inf +INF HidPkg/UsbKbHidDxe/UsbKbHidDxe.inf +INF HidPkg/UsbMouseHidDxe/UsbMouseHidDxe.inf DSC: - MdeModulePkg/Bus/Usb/UsbKbDxe/UsbKbDxe.inf - MdeModulePkg/Bus/Usb/UsbMouseAbsolutePointerDxe/UsbMouseAbsolutePointerDxe.inf + HidPkg/UsbKbHidDxe/UsbKbHidDxe.inf + HidPkg/UsbMouseHidDxe/UsbMouseHidDxe.inf","title":"Readme"},{"location":"dyn/mu_plus/HidPkg/Readme/#hid-support-package","text":"","title":"HID support package"},{"location":"dyn/mu_plus/HidPkg/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/HidPkg/Readme/#about","text":"The purpose of this package is to provide a generic Boot HID layer that can be used so that devices that implement the Boot Mouse and Boot Keyboard HID interfaces defined in the USB 1.1 spec can share a common HID processing layer. This avoids duplication of the HID parsing/processing code and provides a natural interface layer where various HID-supporting hardware can plug into the UEFI input stack. Adding support for new hardware that wants to plug into HID Keyboard or Mouse simply requires implementing HidKeyboardProtocol or HidMouseProtocol and installing instances of those protocols on the controller handle for the hardware. This greatly simplifies the process of supporting hardware that is already designed to support Boot Mouse or Boot Keyboard HID spec, since such hardware is already producing reports in the same format that the HidKeyboardProtocol or HidMouseProtocol expects.","title":"About"},{"location":"dyn/mu_plus/HidPkg/Readme/#integration-guide","text":"To use the HidPkg, include the following drivers in your build: FDF: #HID Support INF HidPkg/HidKeyboardDxe/HidKeyboardDxe.inf INF HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe.inf DSC: #HID Support HidPkg/HidKeyboardDxe/HidKeyboardDxe.inf HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe.inf Then, include low-level drivers that produce the HID protocols. This package includes two sample drivers for USB keyboard and Mouse that can be used in place of the standard USB keyboard and Mouse drivers. A platform may add additional drivers to support proprietary hardware such as keyboards/track pads/ touchscreens that support the HID protocol definitions in this package. To include the sample USB HID drivers, remove the MdeModulePkg versions and replace them with the sample drivers from this package. FDF: -INF MdeModulePkg/Bus/Usb/UsbKbDxe/UsbKbDxe.inf -INF MdeModulePkg/Bus/Usb/UsbMouseAbsolutePointerDxe/UsbMouseAbsolutePointerDxe.inf +INF HidPkg/UsbKbHidDxe/UsbKbHidDxe.inf +INF HidPkg/UsbMouseHidDxe/UsbMouseHidDxe.inf DSC: - MdeModulePkg/Bus/Usb/UsbKbDxe/UsbKbDxe.inf - MdeModulePkg/Bus/Usb/UsbMouseAbsolutePointerDxe/UsbMouseAbsolutePointerDxe.inf + HidPkg/UsbKbHidDxe/UsbKbHidDxe.inf + HidPkg/UsbMouseHidDxe/UsbMouseHidDxe.inf","title":"Integration Guide"},{"location":"dyn/mu_plus/HidPkg/HidKeyboardDxe/HidKeyboardDxe/","text":"Description \u00b6 This driver produces an instances of SIMPLE_TEXT_INPUT/SIMPLE_TEXT_INPUT_EX for keyboard support in UEFI It registers a callback with devices exposing the HID_KEYBOARD_PROTOCOL to receive Keyboard HID reports, which are used to satisfy the contract of SIMPLE_TEXT_INPUT/SIMPLE_TEXT_INPUT_EX. Provides \u00b6 SIMPLE_TEXT_INPUT/SIMPLE_TEXT_INPUT_EX instance for consumption by UEFI console. Dependencies \u00b6 HID_KEYBOARD_PROTOCOL to register for and receive HID Keyboard Reports. Application \u00b6 Used to enable PreBoot Keyboard Support.","title":"Hid Keyboard Dxe"},{"location":"dyn/mu_plus/HidPkg/HidKeyboardDxe/HidKeyboardDxe/#description","text":"This driver produces an instances of SIMPLE_TEXT_INPUT/SIMPLE_TEXT_INPUT_EX for keyboard support in UEFI It registers a callback with devices exposing the HID_KEYBOARD_PROTOCOL to receive Keyboard HID reports, which are used to satisfy the contract of SIMPLE_TEXT_INPUT/SIMPLE_TEXT_INPUT_EX.","title":"Description"},{"location":"dyn/mu_plus/HidPkg/HidKeyboardDxe/HidKeyboardDxe/#provides","text":"SIMPLE_TEXT_INPUT/SIMPLE_TEXT_INPUT_EX instance for consumption by UEFI console.","title":"Provides"},{"location":"dyn/mu_plus/HidPkg/HidKeyboardDxe/HidKeyboardDxe/#dependencies","text":"HID_KEYBOARD_PROTOCOL to register for and receive HID Keyboard Reports.","title":"Dependencies"},{"location":"dyn/mu_plus/HidPkg/HidKeyboardDxe/HidKeyboardDxe/#application","text":"Used to enable PreBoot Keyboard Support.","title":"Application"},{"location":"dyn/mu_plus/HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe/","text":"Description \u00b6 This driver produces an instance of EFI_ABSOLUTE_POINTER_PROTOCOL for mouse support in UEFI It registers a callback with the devices exposing HID_POINTER_PROTOCOL to receive Mouse HID reports, which are used to satisfy the contract of EFI_ABSOLUTE_POINTER_PROTOCOL. Provides \u00b6 EFI_ABSOLUTE_POINTER_PROTOCOL instance for consumption by UEFI console. Dependencies \u00b6 HID_POINTER_PROTOCOL to register for and receive HID Mouse Reports. Application \u00b6 Used to enable PreBoot Mouse Support.","title":"Hid Mouse Absolute Pointer Dxe"},{"location":"dyn/mu_plus/HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe/#description","text":"This driver produces an instance of EFI_ABSOLUTE_POINTER_PROTOCOL for mouse support in UEFI It registers a callback with the devices exposing HID_POINTER_PROTOCOL to receive Mouse HID reports, which are used to satisfy the contract of EFI_ABSOLUTE_POINTER_PROTOCOL.","title":"Description"},{"location":"dyn/mu_plus/HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe/#provides","text":"EFI_ABSOLUTE_POINTER_PROTOCOL instance for consumption by UEFI console.","title":"Provides"},{"location":"dyn/mu_plus/HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe/#dependencies","text":"HID_POINTER_PROTOCOL to register for and receive HID Mouse Reports.","title":"Dependencies"},{"location":"dyn/mu_plus/HidPkg/HidMouseAbsolutePointerDxe/HidMouseAbsolutePointerDxe/#application","text":"Used to enable PreBoot Mouse Support.","title":"Application"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/","text":"DRAFT: Manufacturer Firmware Configuration Interface (MFCI) \u00b6 NOTE \u00b6 The following is prerelease documentation for an upcoming feature. Details are subject to change. Overview \u00b6 What Is It \u00b6 Manufacturer Firmware Configuration Interface (MFCI) provides a mechanism for an authenticated agent, such as an OEM or ODM, to modify firmware security properties of a specific device, typically for the purposes of remanufacturing or refurbishment. Background \u00b6 Manufacturing and remanufacturing (refurbishment) of devices requires configuration of settings related to safety (batteries, thermals), compliance (radio calibration, anti-theft), licensing (OA3, serial numbers), & security. These sensitive settings are typically secured by the OEM / ODM at end of manufacturing to ensure that they are not modified with malicious intent. During initial manufacturing, a device is typically not secured until the final provisioning station, after calibration and other secure data have been provisioned. After a device is secured, product demand may deviate from forecasts and a device may need to be reprovisioned for a different region, requiring modification of secured settings. Remanufacturing of devices requires executing tools and workflows that bypass this security, allowing modification of these sensitive settings, and performing potentially destructive diagnostics. The Windows 10 X (a.k.a. WCOS) Compatibility Requirements (formerly known as Logo) prohibit unauthorized execution of these dangerous tools. The MFCI feature provides a secure path to enable remanufacturing while maintaining the Windows security promise. Workflows \u00b6 Generic MFCI Policy Installation Workflow \u00b6 Read targeting information from a target device (UEFI variables) Determine the flavor of MFCI policy to be applied to the device based upon the type of remanufacturing to be performed Assemble the targeting information and policy flavor into the unsigned MFCI Policy packet format Digitally sign the packet using the specified digital signing format and trusted signing keys Write the signed MFCI Policy packet to the \"next\" policy mailbox (a UEFI variable) on the target device Reboot the target device to trigger an installation attempt Prior to OS launch, the firmware attempts to verify the digital signature and targeting information If verification fails, the policy is deleted from the next policy mailbox, and the device proceeds with boot to the OS If verification succeeds... Registered firmware handlers are notified Bits representing 1-shot actions are cleared from the policy, as they should have been handled in the prior notifications The next policy and nonce become \"active\" The device reboots so that early boot code can observe the new active policy One \"Real\" Installation Workflow \u00b6 Boot device to FFU mode Use Microsoft-supplied tool foo to read the targeting information from the device Use Microsoft Hardware Dev Center (web portal or REST APIs) to generate the signed policy Authenticate from an authorized account Supply the targeting and flavor parameters Receive the signed MFCI Policy for your device Use the Microsoft-supplied tool foo to set the signed policy to the next device policy mailbox Reboot, and the new policy should take effect OEM performs customer service or re-manufacturing as needed. Note that if the device make, model, serial number, or related UEFI variables change or are deleted, the policy may no longer be valid on subsequent boots. After re-manufacturing, remove the policy using MS-supplied tool foo Boot Workflow \u00b6 PEI This is not the place to perform MFCI policy verification. Simply get a pre-verified policy from prior boot for augmenting PEI security properties. DXE prior to StartOfBds Drivers that need security policy augmentation should both get the current pre-verified policy and register for notification of policy changes OEM-supplied code populates UEFI variables that communicate the Make, Model, & SN targeting information to MFCI policy driver StartOfBds event The MFCI Policy Driver will... Check for the presence of a policy. If present... Verify its signature, delete the policy on failure Verify its structure, delete the policy on failure If the resulting policy is different from the current policy, notify registered handlers, make the new policy active, reboot If a next policy is not present, and a pre-verified policy was used for this boot, roll nonces, delete the pre-verified policy, & notify registered handlers TODO \u00b6 Links to drill-down documentation, more on the APIs, usage, & data format Diagrams of the structure Integrating MfciPkg \u00b6 TODO: OEM/IBV must author code that populates the UEFI variables that this module needs for make/model/sn TODO: document DSC & FDF changes TODO: certificate PCD and/or FDF changes TODO: static versus shared crypto TODO: source versus nuget versions TODO: builds with production versus test certificates & EKUs Data Format \u00b6 The MFCI policy consists of device targeting information, a policy flavor, & a digital signature. Targeting Information \u00b6 Policies are targeted at specific devices via Make (OEM), Model, Serial Number, Nonce, and 2 optional, OEM-defined fields. The Nonce is a randomly-generated 64-bit integer, and the remaining fields are unescaped, WIDE NULL-terminated UTF-16LE strings. Policy Flavor Bitfield \u00b6 A 64-bit bitfield representing both persistent states and 1-time actions to be performed upon successful authentication and installation of a policy. The bitfield is split into a 32-bit Microsoft-defined region, and 32-bit OEM-defined region. Each of those regions are split again into a 16-bit region for persistent states and 16-bit region for 1-time actions. Digital Signature \u00b6 The PKCS7 digital signature format familiar to the UEFI ecosystem is used with 2 key differences. First, a full PKCS7 is used, not the SignedData subset used by authenticated variables. Second, the P7 is embedded, not detatched. The policy targeting and flavor information are embedded in a PKCS 7 Data inside the full PKCS 7 Signed object. Signed Packet Example \u00b6 certutil.exe -asn output for an example MFCI Policy packet (signed) is as follows: 0000: 30 82 0e 6c ; SEQUENCE (e6c Bytes) 0004: 06 09 ; OBJECT_ID (9 Bytes) 0006: | 2a 86 48 86 f7 0d 01 07 02 | ; 1.2.840.113549.1.7.2 PKCS 7 Signed 000f: a0 82 0e 5d ; OPTIONAL[0] (e5d Bytes) 0013: 30 82 0e 59 ; SEQUENCE (e59 Bytes) 0017: 02 01 ; INTEGER (1 Bytes) 0019: | 01 001a: 31 0f ; SET (f Bytes) 001c: | 30 0d ; SEQUENCE (d Bytes) 001e: | 06 09 ; OBJECT_ID (9 Bytes) 0020: | | 60 86 48 01 65 03 04 02 01 | | ; 2.16.840.1.101.3.4.2.1 sha256 (sha256NoSign) 0029: | 05 00 ; NULL (0 Bytes) 002b: 30 82 02 17 ; SEQUENCE (217 Bytes) 002f: | 06 09 ; OBJECT_ID (9 Bytes) 0031: | | 2a 86 48 86 f7 0d 01 07 01 | | ; 1.2.840.113549.1.7.1 PKCS 7 Data 003a: | a0 82 02 08 ; OPTIONAL[0] (208 Bytes) 003e: | 04 82 02 04 ; OCTET_STRING (204 Bytes) [policy payload starts here] 0042: | 02 00 01 00 00 00 08 f8 e6 5a 84 83 b9 4e a2 3a ; .........Z...N.: 0052: | 0c cc 10 93 e3 dd 00 00 00 00 00 00 00 00 07 00 ; ................ 0062: | 00 00 10 ef 00 00 00 00 0e 00 00 00 28 00 00 00 ; ............(... 0072: | 00 00 10 ef 58 00 00 00 66 00 00 00 76 00 00 00 ; ....X...f...v... 0082: | 00 00 10 ef 8e 00 00 00 9c 00 00 00 b6 00 00 00 ; ................ 0092: | 00 00 10 ef e0 00 00 00 ee 00 00 00 fc 00 00 00 ; ................ 00a2: | 00 00 10 ef 0e 01 00 00 1c 01 00 00 2a 01 00 00 ; ............*... 00b2: | 00 00 10 ef 2e 01 00 00 3c 01 00 00 48 01 00 00 ; ........<...H... 00c2: | 00 00 10 ef 52 01 00 00 5c 01 00 00 6a 01 00 00 ; ....R...\\...j... 00d2: | 0c 00 54 00 61 00 72 00 67 00 65 00 74 00 18 00 ; ..T.a.r.g.e.t... 00e2: | 4d 00 61 00 6e 00 75 00 66 00 61 00 63 00 74 00 ; M.a.n.u.f.a.c.t. 00f2: | 75 00 72 00 65 00 72 00 00 00 2c 00 43 00 6f 00 ; u.r.e.r...,.C.o. 0102: | 6e 00 74 00 6f 00 73 00 6f 00 20 00 43 00 6f 00 ; n.t.o.s.o. .C.o. 0112: | 6d 00 70 00 75 00 74 00 65 00 72 00 73 00 2c 00 ; m.p.u.t.e.r.s.,. 0122: | 20 00 4c 00 4c 00 43 00 0c 00 54 00 61 00 72 00 ; .L.L.C...T.a.r. 0132: | 67 00 65 00 74 00 0e 00 50 00 72 00 6f 00 64 00 ; g.e.t...P.r.o.d. 0142: | 75 00 63 00 74 00 00 00 14 00 4c 00 61 00 70 00 ; u.c.t.....L.a.p. 0152: | 74 00 6f 00 70 00 20 00 46 00 6f 00 6f 00 0c 00 ; t.o.p. .F.o.o... 0162: | 54 00 61 00 72 00 67 00 65 00 74 00 18 00 53 00 ; T.a.r.g.e.t...S. 0172: | 65 00 72 00 69 00 61 00 6c 00 4e 00 75 00 6d 00 ; e.r.i.a.l.N.u.m. 0182: | 62 00 65 00 72 00 00 00 26 00 46 00 30 00 30 00 ; b.e.r...&.F.0.0. 0192: | 31 00 33 00 2d 00 30 00 30 00 30 00 32 00 34 00 ; 1.3.-.0.0.0.2.4. 01a2: | 33 00 35 00 34 00 36 00 2d 00 58 00 30 00 32 00 ; 3.5.4.6.-.X.0.2. 01b2: | 0c 00 54 00 61 00 72 00 67 00 65 00 74 00 0c 00 ; ..T.a.r.g.e.t... 01c2: | 4f 00 45 00 4d 00 5f 00 30 00 31 00 00 00 0e 00 ; O.E.M._.0.1..... 01d2: | 4f 00 44 00 4d 00 20 00 46 00 6f 00 6f 00 0c 00 ; O.D.M. .F.o.o... 01e2: | 54 00 61 00 72 00 67 00 65 00 74 00 0c 00 4f 00 ; T.a.r.g.e.t...O. 01f2: | 45 00 4d 00 5f 00 30 00 32 00 00 00 00 00 0c 00 ; E.M._.0.2....... 0202: | 54 00 61 00 72 00 67 00 65 00 74 00 0a 00 4e 00 ; T.a.r.g.e.t...N. 0212: | 6f 00 6e 00 63 00 65 00 05 00 ef cd ab 89 67 45 ; o.n.c.e.......gE 0222: | 23 01 08 00 55 00 45 00 46 00 49 00 0c 00 50 00 ; #...U.E.F.I...P. 0232: | 6f 00 6c 00 69 00 63 00 79 00 05 00 03 00 00 00 ; o.l.i.c.y....... 0242: | 00 00 00 00 ; .... [policy payload ends here] [digital signature continues but is not shown here]","title":"Package Overview"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#draft-manufacturer-firmware-configuration-interface-mfci","text":"","title":"DRAFT: Manufacturer Firmware Configuration Interface (MFCI)"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#note","text":"The following is prerelease documentation for an upcoming feature. Details are subject to change.","title":"NOTE"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#overview","text":"","title":"Overview"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#what-is-it","text":"Manufacturer Firmware Configuration Interface (MFCI) provides a mechanism for an authenticated agent, such as an OEM or ODM, to modify firmware security properties of a specific device, typically for the purposes of remanufacturing or refurbishment.","title":"What Is It"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#background","text":"Manufacturing and remanufacturing (refurbishment) of devices requires configuration of settings related to safety (batteries, thermals), compliance (radio calibration, anti-theft), licensing (OA3, serial numbers), & security. These sensitive settings are typically secured by the OEM / ODM at end of manufacturing to ensure that they are not modified with malicious intent. During initial manufacturing, a device is typically not secured until the final provisioning station, after calibration and other secure data have been provisioned. After a device is secured, product demand may deviate from forecasts and a device may need to be reprovisioned for a different region, requiring modification of secured settings. Remanufacturing of devices requires executing tools and workflows that bypass this security, allowing modification of these sensitive settings, and performing potentially destructive diagnostics. The Windows 10 X (a.k.a. WCOS) Compatibility Requirements (formerly known as Logo) prohibit unauthorized execution of these dangerous tools. The MFCI feature provides a secure path to enable remanufacturing while maintaining the Windows security promise.","title":"Background"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#workflows","text":"","title":"Workflows"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#generic-mfci-policy-installation-workflow","text":"Read targeting information from a target device (UEFI variables) Determine the flavor of MFCI policy to be applied to the device based upon the type of remanufacturing to be performed Assemble the targeting information and policy flavor into the unsigned MFCI Policy packet format Digitally sign the packet using the specified digital signing format and trusted signing keys Write the signed MFCI Policy packet to the \"next\" policy mailbox (a UEFI variable) on the target device Reboot the target device to trigger an installation attempt Prior to OS launch, the firmware attempts to verify the digital signature and targeting information If verification fails, the policy is deleted from the next policy mailbox, and the device proceeds with boot to the OS If verification succeeds... Registered firmware handlers are notified Bits representing 1-shot actions are cleared from the policy, as they should have been handled in the prior notifications The next policy and nonce become \"active\" The device reboots so that early boot code can observe the new active policy","title":"Generic MFCI Policy Installation Workflow"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#one-real-installation-workflow","text":"Boot device to FFU mode Use Microsoft-supplied tool foo to read the targeting information from the device Use Microsoft Hardware Dev Center (web portal or REST APIs) to generate the signed policy Authenticate from an authorized account Supply the targeting and flavor parameters Receive the signed MFCI Policy for your device Use the Microsoft-supplied tool foo to set the signed policy to the next device policy mailbox Reboot, and the new policy should take effect OEM performs customer service or re-manufacturing as needed. Note that if the device make, model, serial number, or related UEFI variables change or are deleted, the policy may no longer be valid on subsequent boots. After re-manufacturing, remove the policy using MS-supplied tool foo","title":"One \"Real\" Installation Workflow"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#boot-workflow","text":"PEI This is not the place to perform MFCI policy verification. Simply get a pre-verified policy from prior boot for augmenting PEI security properties. DXE prior to StartOfBds Drivers that need security policy augmentation should both get the current pre-verified policy and register for notification of policy changes OEM-supplied code populates UEFI variables that communicate the Make, Model, & SN targeting information to MFCI policy driver StartOfBds event The MFCI Policy Driver will... Check for the presence of a policy. If present... Verify its signature, delete the policy on failure Verify its structure, delete the policy on failure If the resulting policy is different from the current policy, notify registered handlers, make the new policy active, reboot If a next policy is not present, and a pre-verified policy was used for this boot, roll nonces, delete the pre-verified policy, & notify registered handlers","title":"Boot Workflow"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#todo","text":"Links to drill-down documentation, more on the APIs, usage, & data format Diagrams of the structure","title":"TODO"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#integrating-mfcipkg","text":"TODO: OEM/IBV must author code that populates the UEFI variables that this module needs for make/model/sn TODO: document DSC & FDF changes TODO: certificate PCD and/or FDF changes TODO: static versus shared crypto TODO: source versus nuget versions TODO: builds with production versus test certificates & EKUs","title":"Integrating MfciPkg"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#data-format","text":"The MFCI policy consists of device targeting information, a policy flavor, & a digital signature.","title":"Data Format"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#targeting-information","text":"Policies are targeted at specific devices via Make (OEM), Model, Serial Number, Nonce, and 2 optional, OEM-defined fields. The Nonce is a randomly-generated 64-bit integer, and the remaining fields are unescaped, WIDE NULL-terminated UTF-16LE strings.","title":"Targeting Information"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#policy-flavor-bitfield","text":"A 64-bit bitfield representing both persistent states and 1-time actions to be performed upon successful authentication and installation of a policy. The bitfield is split into a 32-bit Microsoft-defined region, and 32-bit OEM-defined region. Each of those regions are split again into a 16-bit region for persistent states and 16-bit region for 1-time actions.","title":"Policy Flavor Bitfield"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#digital-signature","text":"The PKCS7 digital signature format familiar to the UEFI ecosystem is used with 2 key differences. First, a full PKCS7 is used, not the SignedData subset used by authenticated variables. Second, the P7 is embedded, not detatched. The policy targeting and flavor information are embedded in a PKCS 7 Data inside the full PKCS 7 Signed object.","title":"Digital Signature"},{"location":"dyn/mu_plus/MfciPkg/Docs/Mfci_Feature/#signed-packet-example","text":"certutil.exe -asn output for an example MFCI Policy packet (signed) is as follows: 0000: 30 82 0e 6c ; SEQUENCE (e6c Bytes) 0004: 06 09 ; OBJECT_ID (9 Bytes) 0006: | 2a 86 48 86 f7 0d 01 07 02 | ; 1.2.840.113549.1.7.2 PKCS 7 Signed 000f: a0 82 0e 5d ; OPTIONAL[0] (e5d Bytes) 0013: 30 82 0e 59 ; SEQUENCE (e59 Bytes) 0017: 02 01 ; INTEGER (1 Bytes) 0019: | 01 001a: 31 0f ; SET (f Bytes) 001c: | 30 0d ; SEQUENCE (d Bytes) 001e: | 06 09 ; OBJECT_ID (9 Bytes) 0020: | | 60 86 48 01 65 03 04 02 01 | | ; 2.16.840.1.101.3.4.2.1 sha256 (sha256NoSign) 0029: | 05 00 ; NULL (0 Bytes) 002b: 30 82 02 17 ; SEQUENCE (217 Bytes) 002f: | 06 09 ; OBJECT_ID (9 Bytes) 0031: | | 2a 86 48 86 f7 0d 01 07 01 | | ; 1.2.840.113549.1.7.1 PKCS 7 Data 003a: | a0 82 02 08 ; OPTIONAL[0] (208 Bytes) 003e: | 04 82 02 04 ; OCTET_STRING (204 Bytes) [policy payload starts here] 0042: | 02 00 01 00 00 00 08 f8 e6 5a 84 83 b9 4e a2 3a ; .........Z...N.: 0052: | 0c cc 10 93 e3 dd 00 00 00 00 00 00 00 00 07 00 ; ................ 0062: | 00 00 10 ef 00 00 00 00 0e 00 00 00 28 00 00 00 ; ............(... 0072: | 00 00 10 ef 58 00 00 00 66 00 00 00 76 00 00 00 ; ....X...f...v... 0082: | 00 00 10 ef 8e 00 00 00 9c 00 00 00 b6 00 00 00 ; ................ 0092: | 00 00 10 ef e0 00 00 00 ee 00 00 00 fc 00 00 00 ; ................ 00a2: | 00 00 10 ef 0e 01 00 00 1c 01 00 00 2a 01 00 00 ; ............*... 00b2: | 00 00 10 ef 2e 01 00 00 3c 01 00 00 48 01 00 00 ; ........<...H... 00c2: | 00 00 10 ef 52 01 00 00 5c 01 00 00 6a 01 00 00 ; ....R...\\...j... 00d2: | 0c 00 54 00 61 00 72 00 67 00 65 00 74 00 18 00 ; ..T.a.r.g.e.t... 00e2: | 4d 00 61 00 6e 00 75 00 66 00 61 00 63 00 74 00 ; M.a.n.u.f.a.c.t. 00f2: | 75 00 72 00 65 00 72 00 00 00 2c 00 43 00 6f 00 ; u.r.e.r...,.C.o. 0102: | 6e 00 74 00 6f 00 73 00 6f 00 20 00 43 00 6f 00 ; n.t.o.s.o. .C.o. 0112: | 6d 00 70 00 75 00 74 00 65 00 72 00 73 00 2c 00 ; m.p.u.t.e.r.s.,. 0122: | 20 00 4c 00 4c 00 43 00 0c 00 54 00 61 00 72 00 ; .L.L.C...T.a.r. 0132: | 67 00 65 00 74 00 0e 00 50 00 72 00 6f 00 64 00 ; g.e.t...P.r.o.d. 0142: | 75 00 63 00 74 00 00 00 14 00 4c 00 61 00 70 00 ; u.c.t.....L.a.p. 0152: | 74 00 6f 00 70 00 20 00 46 00 6f 00 6f 00 0c 00 ; t.o.p. .F.o.o... 0162: | 54 00 61 00 72 00 67 00 65 00 74 00 18 00 53 00 ; T.a.r.g.e.t...S. 0172: | 65 00 72 00 69 00 61 00 6c 00 4e 00 75 00 6d 00 ; e.r.i.a.l.N.u.m. 0182: | 62 00 65 00 72 00 00 00 26 00 46 00 30 00 30 00 ; b.e.r...&.F.0.0. 0192: | 31 00 33 00 2d 00 30 00 30 00 30 00 32 00 34 00 ; 1.3.-.0.0.0.2.4. 01a2: | 33 00 35 00 34 00 36 00 2d 00 58 00 30 00 32 00 ; 3.5.4.6.-.X.0.2. 01b2: | 0c 00 54 00 61 00 72 00 67 00 65 00 74 00 0c 00 ; ..T.a.r.g.e.t... 01c2: | 4f 00 45 00 4d 00 5f 00 30 00 31 00 00 00 0e 00 ; O.E.M._.0.1..... 01d2: | 4f 00 44 00 4d 00 20 00 46 00 6f 00 6f 00 0c 00 ; O.D.M. .F.o.o... 01e2: | 54 00 61 00 72 00 67 00 65 00 74 00 0c 00 4f 00 ; T.a.r.g.e.t...O. 01f2: | 45 00 4d 00 5f 00 30 00 32 00 00 00 00 00 0c 00 ; E.M._.0.2....... 0202: | 54 00 61 00 72 00 67 00 65 00 74 00 0a 00 4e 00 ; T.a.r.g.e.t...N. 0212: | 6f 00 6e 00 63 00 65 00 05 00 ef cd ab 89 67 45 ; o.n.c.e.......gE 0222: | 23 01 08 00 55 00 45 00 46 00 49 00 0c 00 50 00 ; #...U.E.F.I...P. 0232: | 6f 00 6c 00 69 00 63 00 79 00 05 00 03 00 00 00 ; o.l.i.c.y....... 0242: | 00 00 00 00 ; .... [policy payload ends here] [digital signature continues but is not shown here]","title":"Signed Packet Example"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/","text":"Debugging with DxeDebugLibRouter \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 DxeDebugLibRouter \u00b6 The DxeDebugLibRouter is an implementation of DebugLib that routes the DebugPrint and DebugAssert messages depending on what the platform is capable of and what has been set-up. In the example below we show how to use DxeDebugLibRouter to route debug messages through either the serial interface or the report status code interface, depending on what protocols and libraries are being used. StatusCodeHandler \u00b6 If you wish to make use of the Report Status Code debugging feature you will need to set up a status code handler and install the gMsSerialStatusCodeHandlerDxeProtocolGuid tag GUID. The MsCorePkg version of StatusCodeHandler is setup to do this. DebugPortProtocolInstallLib \u00b6 The DebugPortProtocolInstallLib is a shim library whos only purpose is to install a protocol that points to the currently linked DebugLib being used by the module. You can see how this is used in the DSC example shown below. ReportStatusCodeRouter \u00b6 This library handles the routing of ReportStatusCode if the DxeDebugLibRouter is set-up to use the ReportStatusCode debug path. We have only implemented the serial output for report status code, but there are many ways you can implement a RSC observer including Serial Port Listener Save Debug To File System Save Debug To Memory ... How To Use \u00b6 To make full use of the DxeDebugLibRouter each Dxe Driver will need to use the DebugPort implementation of DebugLib to route their messages through the DxeDebugLibRouter. The Flowchart below shows how this would work. To set up DxeCore to work as the router you will need to set up the DSC as below: [LibraryClasses.X64] DebugLib|MdePkg/Library/UefiDebugLibDebugPortProtocol/UefiDebugLibDebugPortProtocol.inf [Components.X64] MdeModulePkg/Core/Dxe/DxeMain.inf { <LibraryClasses> NULL|MsCorePkg/Library/DebugPortProtocolInstallLib/DebugPortProtocolInstallLib.inf DebugLib|MsCorePkg/Library/DxeDebugLibRouter/DxeDebugLibRouter.inf } Debug Flow \u00b6 1: The NULL library responsible for publishing the DebugPort protocol is linked against DxeMain. This allows the DebugLib used by Dxe drivers to locate the DebugLib used by Dxe Main 2: Dxe Driver makes a DebugPrint which is routed to the DebugLib linked to DxeMain 3: DebugLib routes the DebugPrint through either Serial or Report Status Code depending on what is installed at the time A: This step can happen at any time. When the StatusCodeHandler is dispatched it installs a tag GUID letting the DebugLib know that Report Status Code is now available","title":"Feature Debug Routing Readme"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#debugging-with-dxedebuglibrouter","text":"","title":"Debugging with DxeDebugLibRouter"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#about","text":"","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#dxedebuglibrouter","text":"The DxeDebugLibRouter is an implementation of DebugLib that routes the DebugPrint and DebugAssert messages depending on what the platform is capable of and what has been set-up. In the example below we show how to use DxeDebugLibRouter to route debug messages through either the serial interface or the report status code interface, depending on what protocols and libraries are being used.","title":"DxeDebugLibRouter"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#statuscodehandler","text":"If you wish to make use of the Report Status Code debugging feature you will need to set up a status code handler and install the gMsSerialStatusCodeHandlerDxeProtocolGuid tag GUID. The MsCorePkg version of StatusCodeHandler is setup to do this.","title":"StatusCodeHandler"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#debugportprotocolinstalllib","text":"The DebugPortProtocolInstallLib is a shim library whos only purpose is to install a protocol that points to the currently linked DebugLib being used by the module. You can see how this is used in the DSC example shown below.","title":"DebugPortProtocolInstallLib"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#reportstatuscoderouter","text":"This library handles the routing of ReportStatusCode if the DxeDebugLibRouter is set-up to use the ReportStatusCode debug path. We have only implemented the serial output for report status code, but there are many ways you can implement a RSC observer including Serial Port Listener Save Debug To File System Save Debug To Memory ...","title":"ReportStatusCodeRouter"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#how-to-use","text":"To make full use of the DxeDebugLibRouter each Dxe Driver will need to use the DebugPort implementation of DebugLib to route their messages through the DxeDebugLibRouter. The Flowchart below shows how this would work. To set up DxeCore to work as the router you will need to set up the DSC as below: [LibraryClasses.X64] DebugLib|MdePkg/Library/UefiDebugLibDebugPortProtocol/UefiDebugLibDebugPortProtocol.inf [Components.X64] MdeModulePkg/Core/Dxe/DxeMain.inf { <LibraryClasses> NULL|MsCorePkg/Library/DebugPortProtocolInstallLib/DebugPortProtocolInstallLib.inf DebugLib|MsCorePkg/Library/DxeDebugLibRouter/DxeDebugLibRouter.inf }","title":"How To Use"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#debug-flow","text":"1: The NULL library responsible for publishing the DebugPort protocol is linked against DxeMain. This allows the DebugLib used by Dxe drivers to locate the DebugLib used by Dxe Main 2: Dxe Driver makes a DebugPrint which is routed to the DebugLib linked to DxeMain 3: DebugLib routes the DebugPrint through either Serial or Report Status Code depending on what is installed at the time A: This step can happen at any time. When the StatusCodeHandler is dispatched it installs a tag GUID letting the DebugLib know that Report Status Code is now available","title":"Debug Flow"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/","text":"MS Core Package \u00b6 About \u00b6 This package has shared drivers and libraries that are silicon and platform independent. Testing \u00b6 There are UEFI shell application based unit tests for each library. These tests attempt to verify basic functionality of public interfaces. Check the UntTests folder at the root of the package. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Read Me"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#ms-core-package","text":"","title":"MS Core Package"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#about","text":"This package has shared drivers and libraries that are silicon and platform independent.","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#testing","text":"There are UEFI shell application based unit tests for each library. These tests attempt to verify basic functionality of public interfaces. Check the UntTests folder at the root of the package.","title":"Testing"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/","text":"ACPI RGRT \u00b6 What is it? \u00b6 The Regulatory Graphics Resource Table is an entry into the ACPI table. It provides a way to publish a PNG with the regulatory information of a given product. This could include FCC id, UL, Model number, or CMIIT ID just to name a few. This driver publishes that entry in the ACPI table to be picked up by the OS later to display for regulatory reasons. The table definition \u00b6 The format of the table is defined by spec.png and consists of a standard ACPI header along with an RGRT payload. At time of writing, the only image format supported by RGRT is PNG. Using in your project \u00b6 Using this is as simple as including the INF in both your DSC and INF. You will also need to define the file for the PCD in your FDF. DSC: [Components] MsCorePkg/AcpiRGRT/AcpiRgrt.inf FDF: [FV.FVDXE] # Regulatory Graphic Driver INF MsCorePkg/AcpiRGRT/AcpiRgrt.inf FILE FREEFORM = PCD(gMsCorePkgTokenSpaceGuid.PcdRegulatoryGraphicFileGuid) { SECTION RAW = $(GRAPHICS_RESOURCES)/RGRT.png }","title":"Acpi RGRT"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#acpi-rgrt","text":"","title":"ACPI RGRT"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#what-is-it","text":"The Regulatory Graphics Resource Table is an entry into the ACPI table. It provides a way to publish a PNG with the regulatory information of a given product. This could include FCC id, UL, Model number, or CMIIT ID just to name a few. This driver publishes that entry in the ACPI table to be picked up by the OS later to display for regulatory reasons.","title":"What is it?"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#the-table-definition","text":"The format of the table is defined by spec.png and consists of a standard ACPI header along with an RGRT payload. At time of writing, the only image format supported by RGRT is PNG.","title":"The table definition"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#using-in-your-project","text":"Using this is as simple as including the INF in both your DSC and INF. You will also need to define the file for the PCD in your FDF. DSC: [Components] MsCorePkg/AcpiRGRT/AcpiRgrt.inf FDF: [FV.FVDXE] # Regulatory Graphic Driver INF MsCorePkg/AcpiRGRT/AcpiRgrt.inf FILE FREEFORM = PCD(gMsCorePkgTokenSpaceGuid.PcdRegulatoryGraphicFileGuid) { SECTION RAW = $(GRAPHICS_RESOURCES)/RGRT.png }","title":"Using in your project"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/","text":"CheckHardwareConnected \u00b6 About \u00b6 This driver determines at boot if the specified pci devices found in the DeviceSpecificBusInfoLib library are properly connected Usage \u00b6 To employ this driver, simply build it and supply the DeviceSpecificBusInfoLib which implements the DeviceSpecificBusInfoLib.h interface. The info for each PCI device will be contained within the DEVICE_PCI_INFO struct which contains fields: DeviceName: A friendly name for the device. This Ascii name will be contained within the AdditionalInfo2 field of the MU_TELEMETRY_CPER_SECTION_DATA telemetry struct. IsFatal: A boolean which if true states that the pci device being absent crashes the device upon OS boot SegmentNumber , BusNumber , DeviceNumber , FunctionNumber : Info required to locate the PCI device MinimumLinkSpeed The minimum link speed expected for the PCI device The library interface consists of two functions: GetPciCheckDevices() : Populates an array of pointers to DEVICE_PCI_INFO structs. The pointer to the unallocated array is passed in as an argument and should be allocated within the function. The DEVICE_PCI_INFO structs should be global variables in the library and the array should contain their addresses. Function returns the number of DEVICE_PCI_INFO struct pointers within the allocated array. ProcessPciDeviceResults() : Accepts an array of PCI device check results in the form of a pointer to a type of DEVICE_PCI_CHECK_RESULT and performs custom actions based upon the results. If there are specific cases when you do not want to check for certain PCI devices (such as when a device has been purposefully disabled), simply exclude the DEVICE_PCI_INFO associated with that device when allocating and returning the array. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Check Hardware Connected"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#checkhardwareconnected","text":"","title":"CheckHardwareConnected"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#about","text":"This driver determines at boot if the specified pci devices found in the DeviceSpecificBusInfoLib library are properly connected","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#usage","text":"To employ this driver, simply build it and supply the DeviceSpecificBusInfoLib which implements the DeviceSpecificBusInfoLib.h interface. The info for each PCI device will be contained within the DEVICE_PCI_INFO struct which contains fields: DeviceName: A friendly name for the device. This Ascii name will be contained within the AdditionalInfo2 field of the MU_TELEMETRY_CPER_SECTION_DATA telemetry struct. IsFatal: A boolean which if true states that the pci device being absent crashes the device upon OS boot SegmentNumber , BusNumber , DeviceNumber , FunctionNumber : Info required to locate the PCI device MinimumLinkSpeed The minimum link speed expected for the PCI device The library interface consists of two functions: GetPciCheckDevices() : Populates an array of pointers to DEVICE_PCI_INFO structs. The pointer to the unallocated array is passed in as an argument and should be allocated within the function. The DEVICE_PCI_INFO structs should be global variables in the library and the array should contain their addresses. Function returns the number of DEVICE_PCI_INFO struct pointers within the allocated array. ProcessPciDeviceResults() : Accepts an array of PCI device check results in the form of a pointer to a type of DEVICE_PCI_CHECK_RESULT and performs custom actions based upon the results. If there are specific cases when you do not want to check for certain PCI devices (such as when a device has been purposefully disabled), simply exclude the DEVICE_PCI_INFO associated with that device when allocating and returning the array.","title":"Usage"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/","text":"Debug File Logger II \u00b6 This is Debug File Logger II. Debug File Logger was tried a few times with varying degrees of success. It was thought that the not quite right USB stack was the issue with the Debug File Logger. When the Debug File Logger was enabled to write logs in the ESP (EFI System Partition) on the NVM/E drive, there would be systems that would no longer boot. Over time, we have found that there were issues in the Windows FAT file system driver across hibernate where the OS assumed no change to the file system metadata (The FAT and Directory Entries) causing irreparable damage to the ESP. Top design points. \u00b6 The debug log files are pre-allocated the first time they are needed on a particular device. Every SimpleFileSystem device that is connected during POST is eligible to be a log device. A USB device must have a directory in the root named 'Logs' to be considered a log device. The NVM/E device(s) will get a hidden directory named 'Logs' the first time is is mounted. The time to create the initial logs is about 2 seconds. The time to register the logs during subsequent time through POST is about 2ms. The logs can be obtained in the OS or by booting to the Shell. In the OS, mount the ESP using: mountvol P: /S The drive letter can be any available drive letter. Logs will be recorded at: When a registered device is connected When just prior to ExitBootServices to any previously registered devices When the system is reset (from TPL <= TPL_CALLBACK) to any previously registered devices If you want to collect logs on a USB device, you can insert the non-bootable USB drive with the Logs directory installed, then power on the system and hold VOL/- to attempt booting from USB. This will mount the USB filesystem, the Logs directory will be acknowledged, and the log written. The system will continue to boot and should append the rest of the log at ExitBootServices. Future items \u00b6 Let me know what future items are necessary. Some under considerations: Refactor DebugLib again to insure logs are collected during the whole boot. Add a new feature to DebugLib to capture the full UEFI log, but only send selected entries to the serial port (to handle devices with a slow serial port). Always connect the NVM/E drive at console connect. For 99.9% of the cases, the system is going to boot the NVM/E drive, so connecting it early would not significantly change the boot time, but would allow logs to be collected in more cases. When the log cannot be written (too high of TPL for the FileSystem to work), have a method to store the last 100 lines or so of the log somewhere (system dependent) that can be collected on the next boot. Timestamp the head of the log to assist in sorting logs Installation Instructions \u00b6 Replace the current Pei/DebugFileLoggerPei.inf and /Dxe/DebugFileLogger.inf with the new versions. Delete the DebugFileLoggerLib as it is no longer necessary. Remove these lines of the old Debug File Logger: DebugFileLoggerLib|XxxxxPkg/Library/DebugFileLoggerLib/DebugFileLoggerLib.inf XxxxxPkg/DebugFileLogger/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } XxxxxPkg/DebugFileLogger/Dxe/DebugFileLogger.inf Replace the old Debug file logger files with these new Debug File Logger II entries: MsCorePkg/DebugFileLoggerII/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } MsCorePkg/DebugFileLoggerII/Dxe/DebugFileLogger.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } There is no check for Manufacturing mode. The idea is for the File Logger to be robust enough to be on in all builds.","title":"Debug File Logger II"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#debug-file-logger-ii","text":"This is Debug File Logger II. Debug File Logger was tried a few times with varying degrees of success. It was thought that the not quite right USB stack was the issue with the Debug File Logger. When the Debug File Logger was enabled to write logs in the ESP (EFI System Partition) on the NVM/E drive, there would be systems that would no longer boot. Over time, we have found that there were issues in the Windows FAT file system driver across hibernate where the OS assumed no change to the file system metadata (The FAT and Directory Entries) causing irreparable damage to the ESP.","title":"Debug File Logger II"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#top-design-points","text":"The debug log files are pre-allocated the first time they are needed on a particular device. Every SimpleFileSystem device that is connected during POST is eligible to be a log device. A USB device must have a directory in the root named 'Logs' to be considered a log device. The NVM/E device(s) will get a hidden directory named 'Logs' the first time is is mounted. The time to create the initial logs is about 2 seconds. The time to register the logs during subsequent time through POST is about 2ms. The logs can be obtained in the OS or by booting to the Shell. In the OS, mount the ESP using: mountvol P: /S The drive letter can be any available drive letter. Logs will be recorded at: When a registered device is connected When just prior to ExitBootServices to any previously registered devices When the system is reset (from TPL <= TPL_CALLBACK) to any previously registered devices If you want to collect logs on a USB device, you can insert the non-bootable USB drive with the Logs directory installed, then power on the system and hold VOL/- to attempt booting from USB. This will mount the USB filesystem, the Logs directory will be acknowledged, and the log written. The system will continue to boot and should append the rest of the log at ExitBootServices.","title":"Top design points."},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#future-items","text":"Let me know what future items are necessary. Some under considerations: Refactor DebugLib again to insure logs are collected during the whole boot. Add a new feature to DebugLib to capture the full UEFI log, but only send selected entries to the serial port (to handle devices with a slow serial port). Always connect the NVM/E drive at console connect. For 99.9% of the cases, the system is going to boot the NVM/E drive, so connecting it early would not significantly change the boot time, but would allow logs to be collected in more cases. When the log cannot be written (too high of TPL for the FileSystem to work), have a method to store the last 100 lines or so of the log somewhere (system dependent) that can be collected on the next boot. Timestamp the head of the log to assist in sorting logs","title":"Future items"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#installation-instructions","text":"Replace the current Pei/DebugFileLoggerPei.inf and /Dxe/DebugFileLogger.inf with the new versions. Delete the DebugFileLoggerLib as it is no longer necessary. Remove these lines of the old Debug File Logger: DebugFileLoggerLib|XxxxxPkg/Library/DebugFileLoggerLib/DebugFileLoggerLib.inf XxxxxPkg/DebugFileLogger/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } XxxxxPkg/DebugFileLogger/Dxe/DebugFileLogger.inf Replace the old Debug file logger files with these new Debug File Logger II entries: MsCorePkg/DebugFileLoggerII/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } MsCorePkg/DebugFileLoggerII/Dxe/DebugFileLogger.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } There is no check for Manufacturing mode. The idea is for the File Logger to be robust enough to be on in all builds.","title":"Installation Instructions"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/","text":"MuCryptoDxe \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 MuCryptoDxe is a DXE_DRIVER you can include in your platform to have a protocol that can call Crypto functions without having to staticly link against the crypto library in many places Supported Architectures \u00b6 This package is not architecturally dependent. Methods supported \u00b6 There are two protocols exposed in this GUID _MU_PKCS5_PASSWORD_HASH_PROTOCOL \u00b6 HashPassword \u00b6 Hashes a password by passing through to the BaseCryptLib. Returns EFI_STATUS NOTE: DigestSize will be used to determine the hash algorithm and must correspond to a known hash digest size. Use standards. @retval EFI_SUCCESS Congratulations! Your hash is in the output buffer. @retval EFI_INVALID_PARAMETER One of the pointers was NULL or one of the sizes was too large. @retval EFI_INVALID_PARAMETER The hash algorithm could not be determined from the digest size. @retval EFI_ABORTED An error occurred in the OpenSSL subroutines. Inputs : IN CONST MU_PKCS5_PASSWORD_HASH_PROTOCOL IN UINTN PasswordSize IN CONST CHAR8 *Password IN UINTN SaltSize IN CONST UINT8 *Salt IN UINTN IterationCount IN UINTN DigestSize IN UINTN OutputSize OUT UINT8 *Output _MU_PKCS7_PROTOCOL \u00b6 Verify \u00b6 Verifies the validity of a PKCS#7 signed data as described in \"PKCS #7: Cryptographic Message Syntax Standard\". The input signed data could be wrapped in a ContentInfo structure. If P7Data, TrustedCert or InData is NULL, then return EFI_INVALID_PARAMETER. If P7Length, CertLength or DataLength overflow, then return EFI_INVALID_PARAMETER. If this interface is not supported, then return EFI_UNSUPPORTED. @retval EFI_SUCCESS The specified PKCS#7 signed data is valid. @retval EFI_SECURITY_VIOLATION Invalid PKCS#7 signed data. @retval EFI_UNSUPPORTED This interface is not supported. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *P7Data, IN UINTN P7DataLength, IN CONST UINT8 *TrustedCert, IN UINTN TrustedCertLength, IN CONST UINT8 *Data, IN UINTN DataLength (in bytes) VerifyEKU \u00b6 This function receives a PKCS7 formatted signature, and then verifies that the specified Enhanced or Extended Key Usages (EKU's) are present in the end-entity leaf signing certificate. Note that this function does not validate the certificate chain. Applications for custom EKU's are quite flexible. For example, a policy EKU may be present in an Issuing Certificate Authority (CA), and any sub-ordinate certificate issued might also contain this EKU, thus constraining the sub-ordinate certificate. Other applications might allow a certificate embedded in a device to specify that other Object Identifiers (OIDs) are present which contains binary data specifying custom capabilities that the device is able to do. @retval EFI_SUCCESS - The required EKUs were found in the signature. @retval EFI_INVALID_PARAMETER - A parameter was invalid. @retval EFI_NOT_FOUND - One or more EKU's were not found in the signature. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *Pkcs7Signature, IN CONST UINT32 SignatureSize, (in bytes) IN CONST CHAR8 *RequiredEKUs[], null-terminated strings listing OIDs of required EKUs IN CONST UINT32 RequiredEKUsSize, IN BOOLEAN RequireAllPresent Including in your platform \u00b6 Sample DSC change \u00b6 [Components.<arch>] ... ... MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf Sample FDF change \u00b6 [FV.<a DXE firmware volume>] ... ... INF MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf","title":"Mu Crypto Dxe"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#mucryptodxe","text":"","title":"MuCryptoDxe"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#about","text":"MuCryptoDxe is a DXE_DRIVER you can include in your platform to have a protocol that can call Crypto functions without having to staticly link against the crypto library in many places","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#supported-architectures","text":"This package is not architecturally dependent.","title":"Supported Architectures"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#methods-supported","text":"There are two protocols exposed in this GUID","title":"Methods supported"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#_mu_pkcs5_password_hash_protocol","text":"","title":"_MU_PKCS5_PASSWORD_HASH_PROTOCOL"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#hashpassword","text":"Hashes a password by passing through to the BaseCryptLib. Returns EFI_STATUS NOTE: DigestSize will be used to determine the hash algorithm and must correspond to a known hash digest size. Use standards. @retval EFI_SUCCESS Congratulations! Your hash is in the output buffer. @retval EFI_INVALID_PARAMETER One of the pointers was NULL or one of the sizes was too large. @retval EFI_INVALID_PARAMETER The hash algorithm could not be determined from the digest size. @retval EFI_ABORTED An error occurred in the OpenSSL subroutines. Inputs : IN CONST MU_PKCS5_PASSWORD_HASH_PROTOCOL IN UINTN PasswordSize IN CONST CHAR8 *Password IN UINTN SaltSize IN CONST UINT8 *Salt IN UINTN IterationCount IN UINTN DigestSize IN UINTN OutputSize OUT UINT8 *Output","title":"HashPassword"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#_mu_pkcs7_protocol","text":"","title":"_MU_PKCS7_PROTOCOL"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#verify","text":"Verifies the validity of a PKCS#7 signed data as described in \"PKCS #7: Cryptographic Message Syntax Standard\". The input signed data could be wrapped in a ContentInfo structure. If P7Data, TrustedCert or InData is NULL, then return EFI_INVALID_PARAMETER. If P7Length, CertLength or DataLength overflow, then return EFI_INVALID_PARAMETER. If this interface is not supported, then return EFI_UNSUPPORTED. @retval EFI_SUCCESS The specified PKCS#7 signed data is valid. @retval EFI_SECURITY_VIOLATION Invalid PKCS#7 signed data. @retval EFI_UNSUPPORTED This interface is not supported. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *P7Data, IN UINTN P7DataLength, IN CONST UINT8 *TrustedCert, IN UINTN TrustedCertLength, IN CONST UINT8 *Data, IN UINTN DataLength (in bytes)","title":"Verify"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#verifyeku","text":"This function receives a PKCS7 formatted signature, and then verifies that the specified Enhanced or Extended Key Usages (EKU's) are present in the end-entity leaf signing certificate. Note that this function does not validate the certificate chain. Applications for custom EKU's are quite flexible. For example, a policy EKU may be present in an Issuing Certificate Authority (CA), and any sub-ordinate certificate issued might also contain this EKU, thus constraining the sub-ordinate certificate. Other applications might allow a certificate embedded in a device to specify that other Object Identifiers (OIDs) are present which contains binary data specifying custom capabilities that the device is able to do. @retval EFI_SUCCESS - The required EKUs were found in the signature. @retval EFI_INVALID_PARAMETER - A parameter was invalid. @retval EFI_NOT_FOUND - One or more EKU's were not found in the signature. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *Pkcs7Signature, IN CONST UINT32 SignatureSize, (in bytes) IN CONST CHAR8 *RequiredEKUs[], null-terminated strings listing OIDs of required EKUs IN CONST UINT32 RequiredEKUsSize, IN BOOLEAN RequireAllPresent","title":"VerifyEKU"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#including-in-your-platform","text":"","title":"Including in your platform"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#sample-dsc-change","text":"[Components.<arch>] ... ... MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf","title":"Sample DSC change"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#sample-fdf-change","text":"[FV.<a DXE firmware volume>] ... ... INF MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf","title":"Sample FDF change"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/","text":"-# MuVarPolicyFoundationDxe Driver and Policies Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Overview \u00b6 This driver works in conjunction with the Variable Policy engine to create two policy-based concepts that can be leveraged by other drivers in the system and to produce an EDK2 Variable Locking Protocol. The two concepts are: Dxe Phase Indicators and Write Once State Variables. DXE Phase Indicators \u00b6 To support the DXE Phase Indicators, a new policy is installed that creates the gMuVarPolicyDxePhaseGuid variable namespace. By policy, all variables in this namespace are required to be the size of a PHASE_INDICATOR (which is a UINT8 ) and must be volatile and readable in both BootServices and Runtime. All variables in this namespace will also be made read-only immediately upon creation (as such, they are also Write-Once, but have a special purpose). Because these variables are required to be volatile, it will not be possible to create more after ExitBootServices. This driver will also register callbacks for EndOfDxe, ReadyToBoot, and ExitBootServices. At the time of the corresponding event, a new PHASE_INDICATOR variable will be created for the callback that has just been triggered. The purpose of these variables is two-fold: They can be queried by any driver or library that needs to know what phases of boot have already occurred. This is especially convenient for libraries that might be linked against any type of driver or application, but may not have been able to register callbacks for all the events because they don't know their execution order or time. They can be used as the delegated \"Variable State\" variables in other Variable Policy lock policies. As such, you could describe a variable that locks \"at ReadyToBoot\" or \"EndOfDxe\". Note that the PHASE_INDICATOR variables are intentionally named as short abbreviations, such as \"EOD\" and \"RTB\". This is to minimize the size of the policy entries for lock-on-state policies. Important Note on Timing \u00b6 The EndOfDxe and ReadyToBoot state variables will be created at the end of the Notify list for the respective event. As such, any variable that locks on those events will still be writeable in Notify callbacks for the event. However, the ExitBootServices state variable (due to architectural requirements) will be created at a non-deterministic time in the Notify list. Therefore, it is unpredictable whether any variable which locks \"on ExitBootServices\" would still be writeable in any given callback. Write-Once State Variables \u00b6 The Write-Once State Variables are actually quite similar to the DXE Phase Variables in terms of policy, but are kept distinct because of the different intentions for their use. This driver will also register a policy that creates the gMuVarPolicyWriteOnceStateVarGuid namespace. This policy will also limit these variables to being: volatile, BootServices and Runtime, read-only on create, and a fixed size -- sizeof(POLICY_LOCK_VAR) , which is also a UINT8 , given that that is the size of the Value field of the current VARIABLE_LOCK_ON_VAR_STATE_POLICY structure. The primary difference is that the DXE Phase Variables have a very clear purpose and meaning. While the gMuVarPolicyWriteOnceStateVarGuid namespace is designed to be general-purpose and used to easily describe delegated variables for VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE -type policies without requiring every driver to define two policies in order to employ this common pattern. Example \u00b6 Driver XYZ wants to define a policy to allow a boot application (that executes after the policy creation interface may be closed) to modify a collection of variables until the boot application chooses to lock them. One way to accomplish this is the create a policy that delegates the lock control of the target variables to the state of variable A. However, now the variable A is the linchpin for the protections of the target variables and should have some protections of its own. This might require another policy. Instead, the driver could create a policy that delegates lock control of the target variables to a variable in the gMuVarPolicyWriteOnceStateVarGuid . This would not require a second policy, as a general-purpose policy is already in place. Now the boot application only needs to create variable A in the gMuVarPolicyWriteOnceStateVarGuid namespace (with the value designated in the policy created by driver XYZ) in order to lock the target variables. Important Note on Naming \u00b6 Since the gMuVarPolicyWriteOnceStateVarGuid namespace is a single variable namespace, naming collisions are possible. Because of the write-once nature of the policy, it is not a concern that an existing lock may be overwritten, but there still may be policy confusion if there are two policies that delegate to the exact same name. It is up to the platform architect to ensure that naming collisions like this do not occur. EDK2 Variable Locking Protocol \u00b6 Finally, this driver installs an EDK2 Variable Locking Protocol instance. This implementation locks a variable by creating a policy entry and registering it via the Variable Policy Protocol. The policy entry is of type VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE and locks the variable based on state of the Phase Indicator variable \"EOD\" (meaning End Of Dxe) in gMuVarPolicyDxePhaseGuid namespace.","title":"Mu Var Policy Foundation Dxe"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#overview","text":"This driver works in conjunction with the Variable Policy engine to create two policy-based concepts that can be leveraged by other drivers in the system and to produce an EDK2 Variable Locking Protocol. The two concepts are: Dxe Phase Indicators and Write Once State Variables.","title":"Overview"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#dxe-phase-indicators","text":"To support the DXE Phase Indicators, a new policy is installed that creates the gMuVarPolicyDxePhaseGuid variable namespace. By policy, all variables in this namespace are required to be the size of a PHASE_INDICATOR (which is a UINT8 ) and must be volatile and readable in both BootServices and Runtime. All variables in this namespace will also be made read-only immediately upon creation (as such, they are also Write-Once, but have a special purpose). Because these variables are required to be volatile, it will not be possible to create more after ExitBootServices. This driver will also register callbacks for EndOfDxe, ReadyToBoot, and ExitBootServices. At the time of the corresponding event, a new PHASE_INDICATOR variable will be created for the callback that has just been triggered. The purpose of these variables is two-fold: They can be queried by any driver or library that needs to know what phases of boot have already occurred. This is especially convenient for libraries that might be linked against any type of driver or application, but may not have been able to register callbacks for all the events because they don't know their execution order or time. They can be used as the delegated \"Variable State\" variables in other Variable Policy lock policies. As such, you could describe a variable that locks \"at ReadyToBoot\" or \"EndOfDxe\". Note that the PHASE_INDICATOR variables are intentionally named as short abbreviations, such as \"EOD\" and \"RTB\". This is to minimize the size of the policy entries for lock-on-state policies.","title":"DXE Phase Indicators"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#important-note-on-timing","text":"The EndOfDxe and ReadyToBoot state variables will be created at the end of the Notify list for the respective event. As such, any variable that locks on those events will still be writeable in Notify callbacks for the event. However, the ExitBootServices state variable (due to architectural requirements) will be created at a non-deterministic time in the Notify list. Therefore, it is unpredictable whether any variable which locks \"on ExitBootServices\" would still be writeable in any given callback.","title":"Important Note on Timing"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#write-once-state-variables","text":"The Write-Once State Variables are actually quite similar to the DXE Phase Variables in terms of policy, but are kept distinct because of the different intentions for their use. This driver will also register a policy that creates the gMuVarPolicyWriteOnceStateVarGuid namespace. This policy will also limit these variables to being: volatile, BootServices and Runtime, read-only on create, and a fixed size -- sizeof(POLICY_LOCK_VAR) , which is also a UINT8 , given that that is the size of the Value field of the current VARIABLE_LOCK_ON_VAR_STATE_POLICY structure. The primary difference is that the DXE Phase Variables have a very clear purpose and meaning. While the gMuVarPolicyWriteOnceStateVarGuid namespace is designed to be general-purpose and used to easily describe delegated variables for VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE -type policies without requiring every driver to define two policies in order to employ this common pattern.","title":"Write-Once State Variables"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#example","text":"Driver XYZ wants to define a policy to allow a boot application (that executes after the policy creation interface may be closed) to modify a collection of variables until the boot application chooses to lock them. One way to accomplish this is the create a policy that delegates the lock control of the target variables to the state of variable A. However, now the variable A is the linchpin for the protections of the target variables and should have some protections of its own. This might require another policy. Instead, the driver could create a policy that delegates lock control of the target variables to a variable in the gMuVarPolicyWriteOnceStateVarGuid . This would not require a second policy, as a general-purpose policy is already in place. Now the boot application only needs to create variable A in the gMuVarPolicyWriteOnceStateVarGuid namespace (with the value designated in the policy created by driver XYZ) in order to lock the target variables.","title":"Example"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#important-note-on-naming","text":"Since the gMuVarPolicyWriteOnceStateVarGuid namespace is a single variable namespace, naming collisions are possible. Because of the write-once nature of the policy, it is not a concern that an existing lock may be overwritten, but there still may be policy confusion if there are two policies that delegate to the exact same name. It is up to the platform architect to ensure that naming collisions like this do not occur.","title":"Important Note on Naming"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#edk2-variable-locking-protocol","text":"Finally, this driver installs an EDK2 Variable Locking Protocol instance. This implementation locks a variable by creating a policy entry and registering it via the Variable Policy Protocol. The policy entry is of type VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE and locks the variable based on state of the Phase Indicator variable \"EOD\" (meaning End Of Dxe) in gMuVarPolicyDxePhaseGuid namespace.","title":"EDK2 Variable Locking Protocol"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/","text":"Verify Json Lite library functionality \u00b6 The Json Lite Library parses json strings in to tuples and encodes tuples into a json string. About \u00b6 These tests verify that the Json Lite Library functions properly. JsonTestApp \u00b6 This application consumes the UnitTestLib and implements various test cases for the verification of the Json Lite Library. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Json Test"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#verify-json-lite-library-functionality","text":"The Json Lite Library parses json strings in to tuples and encodes tuples into a json string.","title":"Verify Json Lite library functionality"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#about","text":"These tests verify that the Json Lite Library functions properly.","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#jsontestapp","text":"This application consumes the UnitTestLib and implements various test cases for the verification of the Json Lite Library.","title":"JsonTestApp"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/","text":"PrintScreenLogger \u00b6 About \u00b6 PrintScreenLogger is a DXE_DRIVER you can include in your platform to obtain Screen Captures during the preboot environment by pressing the Ctrl-PrtScn key combination. This action will creates a 24bbp (Bits Per Pixel) .BMP file of the screen's contents and write it to a enabled USB drive. Supported Architectures \u00b6 This package is not architecturally dependent. This package is dependent upon the Gop pixel format, and only supports these two pixel formats: 1. PixelRedGreenBlueReserved8BitPerColor 2. PixelBlueGreenRedReserved8BitPerColor PrintScreenLogger operation \u00b6 During initialization, the Print Screen Logger registers for notification of the Ctrl-PrtScn key combination is pressed. When a Print Screen callback occurs: Looks for a mounted USB drive that contains a file in the root directory called PrintScreenEnable.txt . This limits PrintScreenLogger to only write to enabled USB devices. Looks for the next available filename in the form PrtScreen####.bmp , starting with 0000. Creates the new PrtScreen####.bmp file. Call GraphicsOutput->Blt to obtain the complete screen. Converts the BLT buffer to a 24bbp BMP structure. Writes the BMP structure to the new PrtScreen####.bmp file. Including in your platform \u00b6 Sample DSC change \u00b6 [Components.<arch>] ... ... MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf Sample FDF change \u00b6 [FV.<a DXE firmware volume>] ... ... INF MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Print Screen Logger"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#printscreenlogger","text":"","title":"PrintScreenLogger"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#about","text":"PrintScreenLogger is a DXE_DRIVER you can include in your platform to obtain Screen Captures during the preboot environment by pressing the Ctrl-PrtScn key combination. This action will creates a 24bbp (Bits Per Pixel) .BMP file of the screen's contents and write it to a enabled USB drive.","title":"About"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#supported-architectures","text":"This package is not architecturally dependent. This package is dependent upon the Gop pixel format, and only supports these two pixel formats: 1. PixelRedGreenBlueReserved8BitPerColor 2. PixelBlueGreenRedReserved8BitPerColor","title":"Supported Architectures"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#printscreenlogger-operation","text":"During initialization, the Print Screen Logger registers for notification of the Ctrl-PrtScn key combination is pressed. When a Print Screen callback occurs: Looks for a mounted USB drive that contains a file in the root directory called PrintScreenEnable.txt . This limits PrintScreenLogger to only write to enabled USB devices. Looks for the next available filename in the form PrtScreen####.bmp , starting with 0000. Creates the new PrtScreen####.bmp file. Call GraphicsOutput->Blt to obtain the complete screen. Converts the BLT buffer to a 24bbp BMP structure. Writes the BMP structure to the new PrtScreen####.bmp file.","title":"PrintScreenLogger operation"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#including-in-your-platform","text":"","title":"Including in your platform"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#sample-dsc-change","text":"[Components.<arch>] ... ... MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf","title":"Sample DSC change"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#sample-fdf-change","text":"[FV.<a DXE firmware volume>] ... ... INF MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf","title":"Sample FDF change"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsWheaPkg/readme/","text":"MS WHEA Package \u00b6 About \u00b6 This package contains drivers and infrastructure for reporting errors and telemetry through the CPER (Common Platform Error Record) HwErrRecord interface, specifically targeting systems that also leverage WHEA (Windows Hardware Error Architecture). The MsWhea drivers provide the same functionality at different stages of UEFI by binding to the REPORT_STATUS_CODE interface. Together, they store hardware errors and corrected faults into non-volatile memory that is later picked up by Windows. In Windows, this can be emitted as telemetry which is then used to identify errors and patterns for devices (as of time of writing any event besides EFI_GENERIC_ERROR_INFO will be sent). Project MU has updated design which defines a specific section type under gMuTelemetrySectionTypeGuid to be used in CPER header for WHEA telemetry. All reported data will be formatted to this section. Please refer to MuTelemetryCperSection.h for field definitions. This package also contains optional solutions for persisting error records in the pre-memory space. In early boot stages, drivers need to emit events as critical in order for them to be logged. Detailed information about CPER can be found in Appendix N of the UEFI spec. How to include this driver \u00b6 This driver must be included via DSC by including the EarlyStorageLib: MsWheaEarlyStorageLib|MsWheaPkg/Library/MsWheaEarlyStorageLib/MsWheaEarlyStorageLib.inf Then the PEI stage driver will be included in the DSC Components.IA32 or PEI section: MsWheaPkg/MsWheaReport/Pei/MsWheaReportPei.inf Then the DXE stage driver will be included in the Components.X64 or DXE section: MsWheaPkg/MsWheaReport/Dxe/MsWheaReportDxe.inf Finally the SMM stage driver will be included in the Components.X64 or DXE_SMM_DRIVER section: MsWheaPkg/MsWheaReport/Smm/MsWheaReportSmm.inf Important Notes \u00b6 The PCD value of gMsWheaPkgTokenSpaceGuid.PcdDeviceIdentifierGuid must be overriden by each platform as this is later used in the CPER as the Platform ID (byte offset 32 in the record). In the DXE phase, errors will be picked up by MsWhea for you. In early phases of boot, the errors must be explicitly logged. To do so, first add the library into your INF: MsWheaPkg/MsWheaPkg.dec These headers must be included: - MsWheaErrorStatus.h - MuTelemetryCperSection.h - Library/BaseLib.h Once you're ready to log your error, you can fill the MU_TELEMETRY_CPER_SECTION_DATA and report the StatusCode. The failure type is of type EFI_STATUS_CODE_VALUE. Note that MsWhea drivers will only listen to report status code calls with (EFI_ERROR_MINOR | EFI_ERROR_CODE) or (EFI_ERROR_MAJOR | EFI_ERROR_CODE) at EFI_STATUS_CODE_TYPE. ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_FATAL, ); for error under EFI_GENERIC_ERROR_FATAL severity. Or: ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_INFO, ); will report errors under EFI_GENERIC_ERROR_INFO severity. Testing \u00b6 There is a UEFI shell application based unit test for WHEA reports. This test attempts to verify basic functionality of public interfaces. Check the UnitTests folder at the root of the package. There is also a feature flag that can inject reports on each boot during various uefi stages. This flag should be off in production. Helper Lib \u00b6 A helper lib to help integrate the MsWhea package has been provided. It is entirely optional and can be easily dropped in. It provides a few macros that are detailed here. LOG_INFO_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_INFO_EVENT ( ClassId ) LOG_CRITICAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_CRITICAL_EVENT ( ClassId ) LOG_FATAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_FATAL_EVENT ( ClassId ) Currently FATAL and CRITICAL map to the same level in the WHEA log but has been implemented to provide future functionality. The parameters are as follows. ClassId - An EFI_STATUS_CODE_VALUE representing the event that has occurred. This should be unique enough to identify a module or region of code. LibraryId - An optional EFI_GUID that should identify the library emitting this event IhvId - An optional EFI_GUID that should identify the Ihv that is most applicable to this. This will often be NULL ExtraData1 - A UINT64 that can be used to provide contextual or runtime data. It will be persisted and can be useful for debugging purposes. ExtraData2 - Another UINT64 that is also used for contextual and runtime data similar to ExtraData1. By default, gEfiCallerIdGuid is used as the module ID when using the macros. If you need complete control over the WHEA entry, you can use the LogTelemetry function to log a telemetry event. This is the function that the Macros use. More information on this function is in the public header for MuTelemetryHelperLib. Including the Helper Lib \u00b6 The helper lib can easily be included by including it in your DSC. [LibraryClasses] MuTelemetryHelperLib|MsWheaPkg/Library/MuTelemetryHelperLib/MuTelemetryHelperLib.inf Since it is a BASE library, it is available for all architectures. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Modules"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#ms-whea-package","text":"","title":"MS WHEA Package"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#about","text":"This package contains drivers and infrastructure for reporting errors and telemetry through the CPER (Common Platform Error Record) HwErrRecord interface, specifically targeting systems that also leverage WHEA (Windows Hardware Error Architecture). The MsWhea drivers provide the same functionality at different stages of UEFI by binding to the REPORT_STATUS_CODE interface. Together, they store hardware errors and corrected faults into non-volatile memory that is later picked up by Windows. In Windows, this can be emitted as telemetry which is then used to identify errors and patterns for devices (as of time of writing any event besides EFI_GENERIC_ERROR_INFO will be sent). Project MU has updated design which defines a specific section type under gMuTelemetrySectionTypeGuid to be used in CPER header for WHEA telemetry. All reported data will be formatted to this section. Please refer to MuTelemetryCperSection.h for field definitions. This package also contains optional solutions for persisting error records in the pre-memory space. In early boot stages, drivers need to emit events as critical in order for them to be logged. Detailed information about CPER can be found in Appendix N of the UEFI spec.","title":"About"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#how-to-include-this-driver","text":"This driver must be included via DSC by including the EarlyStorageLib: MsWheaEarlyStorageLib|MsWheaPkg/Library/MsWheaEarlyStorageLib/MsWheaEarlyStorageLib.inf Then the PEI stage driver will be included in the DSC Components.IA32 or PEI section: MsWheaPkg/MsWheaReport/Pei/MsWheaReportPei.inf Then the DXE stage driver will be included in the Components.X64 or DXE section: MsWheaPkg/MsWheaReport/Dxe/MsWheaReportDxe.inf Finally the SMM stage driver will be included in the Components.X64 or DXE_SMM_DRIVER section: MsWheaPkg/MsWheaReport/Smm/MsWheaReportSmm.inf","title":"How to include this driver"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#important-notes","text":"The PCD value of gMsWheaPkgTokenSpaceGuid.PcdDeviceIdentifierGuid must be overriden by each platform as this is later used in the CPER as the Platform ID (byte offset 32 in the record). In the DXE phase, errors will be picked up by MsWhea for you. In early phases of boot, the errors must be explicitly logged. To do so, first add the library into your INF: MsWheaPkg/MsWheaPkg.dec These headers must be included: - MsWheaErrorStatus.h - MuTelemetryCperSection.h - Library/BaseLib.h Once you're ready to log your error, you can fill the MU_TELEMETRY_CPER_SECTION_DATA and report the StatusCode. The failure type is of type EFI_STATUS_CODE_VALUE. Note that MsWhea drivers will only listen to report status code calls with (EFI_ERROR_MINOR | EFI_ERROR_CODE) or (EFI_ERROR_MAJOR | EFI_ERROR_CODE) at EFI_STATUS_CODE_TYPE. ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_FATAL, ); for error under EFI_GENERIC_ERROR_FATAL severity. Or: ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_INFO, ); will report errors under EFI_GENERIC_ERROR_INFO severity.","title":"Important Notes"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#testing","text":"There is a UEFI shell application based unit test for WHEA reports. This test attempts to verify basic functionality of public interfaces. Check the UnitTests folder at the root of the package. There is also a feature flag that can inject reports on each boot during various uefi stages. This flag should be off in production.","title":"Testing"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#helper-lib","text":"A helper lib to help integrate the MsWhea package has been provided. It is entirely optional and can be easily dropped in. It provides a few macros that are detailed here. LOG_INFO_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_INFO_EVENT ( ClassId ) LOG_CRITICAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_CRITICAL_EVENT ( ClassId ) LOG_FATAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_FATAL_EVENT ( ClassId ) Currently FATAL and CRITICAL map to the same level in the WHEA log but has been implemented to provide future functionality. The parameters are as follows. ClassId - An EFI_STATUS_CODE_VALUE representing the event that has occurred. This should be unique enough to identify a module or region of code. LibraryId - An optional EFI_GUID that should identify the library emitting this event IhvId - An optional EFI_GUID that should identify the Ihv that is most applicable to this. This will often be NULL ExtraData1 - A UINT64 that can be used to provide contextual or runtime data. It will be persisted and can be useful for debugging purposes. ExtraData2 - Another UINT64 that is also used for contextual and runtime data similar to ExtraData1. By default, gEfiCallerIdGuid is used as the module ID when using the macros. If you need complete control over the WHEA entry, you can use the LogTelemetry function to log a telemetry event. This is the function that the Macros use. More information on this function is in the public header for MuTelemetryHelperLib.","title":"Helper Lib"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#including-the-helper-lib","text":"The helper lib can easily be included by including it in your DSC. [LibraryClasses] MuTelemetryHelperLib|MsWheaPkg/Library/MuTelemetryHelperLib/MuTelemetryHelperLib.inf Since it is a BASE library, it is available for all architectures.","title":"Including the Helper Lib"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/","text":"MS Network DependencyLib and NetworkDelayLib \u00b6 About \u00b6 These libraries implement a method to disable the network stack unless there is a reason to use the network. Not starting the network improves boot performance and causes fewer issues during manufacturing. How to use these libraries \u00b6 Add the NetworkDelayLib as a NULL library reference. All this library does is introduce a [Depex] on a NetworkDelay protocol. MdeModulePkg/Universal/Network/SnpDxe/SnpDxe.inf { <LibraryClasses> NULL|PcBdsPkg/Library/MsNetworkDelayLib/MsNetworkDelayLib.inf } The NetworkDependencyLib implements a StartNetwork() interface that will publish the NetworkDelay protocol. The future goal of this functionality is to have the EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() function be overridden and insert a call to DeviceDependencyLib.StartNetwork() when a request is made to start the network class. You nay see other references to NetworkDependencyLib as the conversion to using EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() is not complete. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Ms Network Dependency Lib"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#ms-network-dependencylib-and-networkdelaylib","text":"","title":"MS Network DependencyLib and NetworkDelayLib"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#about","text":"These libraries implement a method to disable the network stack unless there is a reason to use the network. Not starting the network improves boot performance and causes fewer issues during manufacturing.","title":"About"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#how-to-use-these-libraries","text":"Add the NetworkDelayLib as a NULL library reference. All this library does is introduce a [Depex] on a NetworkDelay protocol. MdeModulePkg/Universal/Network/SnpDxe/SnpDxe.inf { <LibraryClasses> NULL|PcBdsPkg/Library/MsNetworkDelayLib/MsNetworkDelayLib.inf } The NetworkDependencyLib implements a StartNetwork() interface that will publish the NetworkDelay protocol. The future goal of this functionality is to have the EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() function be overridden and insert a call to DeviceDependencyLib.StartNetwork() when a request is made to start the network class. You nay see other references to NetworkDependencyLib as the conversion to using EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() is not complete.","title":"How to use these libraries"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/","text":"UEFI Testing Package \u00b6 About \u00b6 This package adds tests. System Functional tests \u00b6 Tests that invoke system functions and query system state for verification. MemmapAndMatTestApp \u00b6 This test compares the UEFI memory map and Memory Attributes Table against known requirements. The MAT has strict requirements to allow OS usage and page protections. MorLockTestApp \u00b6 This test verifies the UEFI variable store handling of MorLock v1 and v2 behavior. SmmPagingProtections \u00b6 This test verifies the SMM paging attributes by invoking operations that should cause cpu exceptions if the memory protections are in place. The SMM cpu exception handler needs to be configured to force reset on trap to allow automated testing. See UefiCpuPkg/Include/Protocol/SmmExceptionTestProtocol.h , gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionRebootInsteadOfHaltDefault , and gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionTestModeSupport . VarPolicyUnitTestApp \u00b6 This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries. System Audit tests \u00b6 UEFI applications that collect data from the system and then that data can be used to compare against known good values. UefiVarLockAudit \u00b6 Audit collection tool that gathers information about UEFI variables. This allows auditing the variables within a system, checking attributes, and confirming read/write status. This information is put into an XML file that allows for easy comparison and programmatic auditing. UEFI \u00b6 UEFI shell application that gets the current variable information from the UEFI shell and creates an XML file. Windows \u00b6 Python script that can be run from the Windows OS. It takes the UEFI created XML file as input and then queries all listed variables and updates the XML with access and status codes. This gives additional verification for variables that may employ late locking or other protections from OS access. TpmEventLogAudit \u00b6 Audit tool to collect the TPM Event Log from the system in standard format. It can then be programmatically compared against a known event log for the given system. Easy this that can be tested are the number of events in some PCRs, confirm that all PCRs should be capped, etc. SMMPagingAudit \u00b6 Audit tool creates a human readable description of the SMM page tables and memory environment. App \u00b6 UEFI shell application collects information from SMM and writes it to files. SMM \u00b6 SMM Library linked into SMM driver used to collect information about SMM environment. Activated by the shell app collects IDT, GDT, page tables, and loaded images. Windows \u00b6 Python scripts that process the files generated by the UEFI app and output a report for verification and analysis. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Readme"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#uefi-testing-package","text":"","title":"UEFI Testing Package"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#about","text":"This package adds tests.","title":"About"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#system-functional-tests","text":"Tests that invoke system functions and query system state for verification.","title":"System Functional tests"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#memmapandmattestapp","text":"This test compares the UEFI memory map and Memory Attributes Table against known requirements. The MAT has strict requirements to allow OS usage and page protections.","title":"MemmapAndMatTestApp"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#morlocktestapp","text":"This test verifies the UEFI variable store handling of MorLock v1 and v2 behavior.","title":"MorLockTestApp"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#smmpagingprotections","text":"This test verifies the SMM paging attributes by invoking operations that should cause cpu exceptions if the memory protections are in place. The SMM cpu exception handler needs to be configured to force reset on trap to allow automated testing. See UefiCpuPkg/Include/Protocol/SmmExceptionTestProtocol.h , gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionRebootInsteadOfHaltDefault , and gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionTestModeSupport .","title":"SmmPagingProtections"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#varpolicyunittestapp","text":"This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries.","title":"VarPolicyUnitTestApp"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#system-audit-tests","text":"UEFI applications that collect data from the system and then that data can be used to compare against known good values.","title":"System Audit tests"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#uefivarlockaudit","text":"Audit collection tool that gathers information about UEFI variables. This allows auditing the variables within a system, checking attributes, and confirming read/write status. This information is put into an XML file that allows for easy comparison and programmatic auditing.","title":"UefiVarLockAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#uefi","text":"UEFI shell application that gets the current variable information from the UEFI shell and creates an XML file.","title":"UEFI"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#windows","text":"Python script that can be run from the Windows OS. It takes the UEFI created XML file as input and then queries all listed variables and updates the XML with access and status codes. This gives additional verification for variables that may employ late locking or other protections from OS access.","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#tpmeventlogaudit","text":"Audit tool to collect the TPM Event Log from the system in standard format. It can then be programmatically compared against a known event log for the given system. Easy this that can be tested are the number of events in some PCRs, confirm that all PCRs should be capped, etc.","title":"TpmEventLogAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#smmpagingaudit","text":"Audit tool creates a human readable description of the SMM page tables and memory environment.","title":"SMMPagingAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#app","text":"UEFI shell application collects information from SMM and writes it to files.","title":"App"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#smm","text":"SMM Library linked into SMM driver used to collect information about SMM environment. Activated by the shell app collects IDT, GDT, page tables, and loaded images.","title":"SMM"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#windows_1","text":"Python scripts that process the files generated by the UEFI app and output a report for verification and analysis.","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/","text":"DMAR Table Audit \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 DMAProtectionUnitTestApp.c Shell based UEFI unit test based off of the UnitTestFrameworkPkg that test for: 1. IOMMU status register shows IOMMU enabled 2. All excluded regions are set as EfiReservedMemoryType(VTd)/EfiACPIMemoryNVS(IVRS) 3. Bus mastering enabled (BME) is disabled on ExitBootServices. Because we can no longer write to file after ExitBootServices a variable is used to store the test state and the machine. Note: this unit test requires a restart to finish its testing. If you plan to use this unit test in automation make sure to set up your startup.nsh script properly.","title":"UEFI"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/#dmar-table-audit","text":"","title":"DMAR Table Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/#about","text":"DMAProtectionUnitTestApp.c Shell based UEFI unit test based off of the UnitTestFrameworkPkg that test for: 1. IOMMU status register shows IOMMU enabled 2. All excluded regions are set as EfiReservedMemoryType(VTd)/EfiACPIMemoryNVS(IVRS) 3. Bus mastering enabled (BME) is disabled on ExitBootServices. Because we can no longer write to file after ExitBootServices a variable is used to store the test state and the machine. Note: this unit test requires a restart to finish its testing. If you plan to use this unit test in automation make sure to set up your startup.nsh script properly.","title":"About"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/","text":"DMAR Table Audit \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 DMARTableAudit.py Unit test that checks: 1. DMA remapping bit is enabled 2. No ANDD structures are included in DMAR table 3. RMRRs are limited to only the RMRRs specified in provided XML file (if no XML provided then verify no RMRRs exist) Software Requirements 1. Python3 2. Pywin32 ```pip install Pywin32``` Project Mu python library pip install mu-python-library","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/#dmar-table-audit","text":"","title":"DMAR Table Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/#about","text":"DMARTableAudit.py Unit test that checks: 1. DMA remapping bit is enabled 2. No ANDD structures are included in DMAR table 3. RMRRs are limited to only the RMRRs specified in provided XML file (if no XML provided then verify no RMRRs exist) Software Requirements 1. Python3 2. Pywin32 ```pip install Pywin32``` Project Mu python library pip install mu-python-library","title":"About"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/","text":"Paging Audit \u00b6 SmmPagingAudit \u00b6 Code Basics \u00b6 SMM is a privileged mode of the ia32/x64 cpu architecture. In this environment nearly all system state can be inspected including that of the operating system, kernel, and hypervisor. Due to it's capabilities SMM has become an area of interest for those searching to exploit the system. To help minimize the interest and impact of an exploit in SMM the SMI handlers should operate in a least privileged model. To do this standard paging can be leveraged to limit the SMI handlers access. Tianocore has a feature to enable paging within SMM and this tool helps confirm the configuration being used. This tool requires three parts to get a complete view. SMM \u00b6 The SMM driver must be included in your build and dispatched to SMM before the End Of Dxe. It is recommended that this driver should only be used on debug builds as it reports the entire SMM memory environment to the caller. The shell app will communicate to the SMM driver and request critical memory information including IDT, GDT, page tables, and loaded images. SMM Version App \u00b6 The UEFI shell application collects system information from the DXE environment and then communicates to the SMM driver/handler to collect necessary info from SMM. It then writes this data to files and then that content is used by the windows scripts. DxePagingAudit \u00b6 Code Basics \u00b6 The Dxe version of paging audit driver/shell app intends to inspect all 4 levels of page tables and their corresponding Read/Write/Executable permissions. The driver/shell app will collect necessary memory information from platform environment, then iterate through each page entries and log them on to available SimpleFileSystem. The collected *.dat files can be parsed using Windows\\PagingReportGenerator.py. DXE Driver \u00b6 The DXE Driver registers an event to be notified on Mu Pre Exit Boot Services (to change this, replace gMuEventPreExitBootServicesGuid with a different event GUID), which will then trigger the paging information collection. DXE Version App \u00b6 The DXE version of UEFI shell application collects necessary system and memory information from DXE when invoked from Shell environment. Windows \u00b6 The Windows script will look at the *.DAT files, parse their content, check for errors and then insert the formatted data into the Html report file. This report file is then double-clickable by the end user/developer to review the posture of the SMM environment. The Results tab applies our suggested rules for SMM to show if the environment passes or fails. If it fails the filters on the data tab can be configured to show where the problem exists. Usage / Enabling on EDK2 based system \u00b6 First, for the SMM driver and app you need to add them to your DSC file for your project so they get compiled. SMM Paging Audit \u00b6 [Components.X64] UefiTestingPkg/AuditTests/PagingAudit/UEFI/SmmPagingAuditDriver.inf UefiTestingPkg/AuditTests/PagingAudit/UEFI/SmmPagingAuditApp.inf Next, you must add the SMM driver to a firmware volume in your FDF that can dispatch SMM modules. INF UefiTestingPkg/AuditTests/PagingAudit/UEFI/SmmPagingAuditApp.inf Third, after compiling your new firmware you must: 1. flash that image on the system. 2. Copy the SmmPagingAuditApp.efi to a USB key Then, boot your system running the new firmware to the shell and run the app. The tool will create a set of *.dat files on the same USB key. On a Windows PC, run the Python script on the data found on your USB key. Finally, double-click the HTML output file and check your results. DXE Paging Audit \u00b6 DxePagingAuditDxe \u00b6 Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg/AuditTests/PagingAudit/UEFI/DxePagingAuditDriver.inf Add the driver to a firmware volume in your FDF that can dispatch it; INF UefiTestingPkg/AuditTests/PagingAudit/UEFI/DxePagingAuditDriver.inf After compiling your new firmware you must: a. flash that image on the system. Boot your system running the new firmware to the OS then reboot to UEFI shell with a USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: copy FS1:\\*.dat FS0:\\ On a Windows PC, run Windows\\PagingReportGenerator.py script with the data found on your USB key. Please use the following command for detailed script instruction: PagingReportGenerator.py -h Double-click the HTML output file and check your results; DxePagingAuditApp \u00b6 Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg/AuditTests/PagingAudit/UEFI/DxePagingAuditApp.inf Compile the newly added application and copy DxePagingAuditApp.efi to a USB key; Boot your system to the shell with the USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: FS0:\\ DxePagingAuditApp.efi copy FS1:\\*.dat FS0:\\ Follow step 5 - 6 from DxePagingAuditDxe section; Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#paging-audit","text":"","title":"Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smmpagingaudit","text":"","title":"SmmPagingAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#code-basics","text":"SMM is a privileged mode of the ia32/x64 cpu architecture. In this environment nearly all system state can be inspected including that of the operating system, kernel, and hypervisor. Due to it's capabilities SMM has become an area of interest for those searching to exploit the system. To help minimize the interest and impact of an exploit in SMM the SMI handlers should operate in a least privileged model. To do this standard paging can be leveraged to limit the SMI handlers access. Tianocore has a feature to enable paging within SMM and this tool helps confirm the configuration being used. This tool requires three parts to get a complete view.","title":"Code Basics"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smm","text":"The SMM driver must be included in your build and dispatched to SMM before the End Of Dxe. It is recommended that this driver should only be used on debug builds as it reports the entire SMM memory environment to the caller. The shell app will communicate to the SMM driver and request critical memory information including IDT, GDT, page tables, and loaded images.","title":"SMM"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smm-version-app","text":"The UEFI shell application collects system information from the DXE environment and then communicates to the SMM driver/handler to collect necessary info from SMM. It then writes this data to files and then that content is used by the windows scripts.","title":"SMM Version App"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxepagingaudit","text":"","title":"DxePagingAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#code-basics_1","text":"The Dxe version of paging audit driver/shell app intends to inspect all 4 levels of page tables and their corresponding Read/Write/Executable permissions. The driver/shell app will collect necessary memory information from platform environment, then iterate through each page entries and log them on to available SimpleFileSystem. The collected *.dat files can be parsed using Windows\\PagingReportGenerator.py.","title":"Code Basics"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxe-driver","text":"The DXE Driver registers an event to be notified on Mu Pre Exit Boot Services (to change this, replace gMuEventPreExitBootServicesGuid with a different event GUID), which will then trigger the paging information collection.","title":"DXE Driver"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxe-version-app","text":"The DXE version of UEFI shell application collects necessary system and memory information from DXE when invoked from Shell environment.","title":"DXE Version App"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#windows","text":"The Windows script will look at the *.DAT files, parse their content, check for errors and then insert the formatted data into the Html report file. This report file is then double-clickable by the end user/developer to review the posture of the SMM environment. The Results tab applies our suggested rules for SMM to show if the environment passes or fails. If it fails the filters on the data tab can be configured to show where the problem exists.","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#usage-enabling-on-edk2-based-system","text":"First, for the SMM driver and app you need to add them to your DSC file for your project so they get compiled.","title":"Usage / Enabling on EDK2 based system"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smm-paging-audit","text":"[Components.X64] UefiTestingPkg/AuditTests/PagingAudit/UEFI/SmmPagingAuditDriver.inf UefiTestingPkg/AuditTests/PagingAudit/UEFI/SmmPagingAuditApp.inf Next, you must add the SMM driver to a firmware volume in your FDF that can dispatch SMM modules. INF UefiTestingPkg/AuditTests/PagingAudit/UEFI/SmmPagingAuditApp.inf Third, after compiling your new firmware you must: 1. flash that image on the system. 2. Copy the SmmPagingAuditApp.efi to a USB key Then, boot your system running the new firmware to the shell and run the app. The tool will create a set of *.dat files on the same USB key. On a Windows PC, run the Python script on the data found on your USB key. Finally, double-click the HTML output file and check your results.","title":"SMM Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxe-paging-audit","text":"","title":"DXE Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxepagingauditdxe","text":"Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg/AuditTests/PagingAudit/UEFI/DxePagingAuditDriver.inf Add the driver to a firmware volume in your FDF that can dispatch it; INF UefiTestingPkg/AuditTests/PagingAudit/UEFI/DxePagingAuditDriver.inf After compiling your new firmware you must: a. flash that image on the system. Boot your system running the new firmware to the OS then reboot to UEFI shell with a USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: copy FS1:\\*.dat FS0:\\ On a Windows PC, run Windows\\PagingReportGenerator.py script with the data found on your USB key. Please use the following command for detailed script instruction: PagingReportGenerator.py -h Double-click the HTML output file and check your results;","title":"DxePagingAuditDxe"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxepagingauditapp","text":"Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg/AuditTests/PagingAudit/UEFI/DxePagingAuditApp.inf Compile the newly added application and copy DxePagingAuditApp.efi to a USB key; Boot your system to the shell with the USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: FS0:\\ DxePagingAuditApp.efi copy FS1:\\*.dat FS0:\\ Follow step 5 - 6 from DxePagingAuditDxe section;","title":"DxePagingAuditApp"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#copyright","text":"Copyright \u00a9 Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/","text":"Heap Guard Tests \u00b6 \ud83d\udd39 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About This Test \u00b6 This is set of tests to ensure that heap guard, stack guard, null pointer detection, and nx protections are working properly. It consists of: An SMM driver A Shell-based test app The Shell-based app may be built at any time and run from Shell. The app can use the SMM driver to preform SMM tests if the SMM driver is installed. It is not the intention of this test to include the driver in production systems. They should only be used for purpose-built test images.","title":"Heap Guard Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/#heap-guard-tests","text":"","title":"Heap Guard Tests"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"&#x1F539; Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/#about-this-test","text":"This is set of tests to ensure that heap guard, stack guard, null pointer detection, and nx protections are working properly. It consists of: An SMM driver A Shell-based test app The Shell-based app may be built at any time and run from Shell. The app can use the SMM driver to preform SMM tests if the SMM driver is installed. It is not the intention of this test to include the driver in production systems. They should only be used for purpose-built test images.","title":"About This Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/","text":"SMM Paging Protections Test \u00b6 \ud83d\udd39 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About This Test \u00b6 This is a [currently] small test to prove that certain SMM paging protections have been applied. It consists of: An SMM driver A DXE driver A Shell-based test app In order to use this test, the SMM and DXE drivers must be built and included in your FW image to be dispatched at boot time. The Shell-based app may be built at any time and run from Shell. The app will ask the DXE driver to pass a message to the SMM driver to invoke a particular test. It is not the intention of this test to include the two drivers in production systems. They should only be used for purpose-built test images.","title":"Smm Paging Protections Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/#smm-paging-protections-test","text":"","title":"SMM Paging Protections Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"&#x1F539; Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/#about-this-test","text":"This is a [currently] small test to prove that certain SMM paging protections have been applied. It consists of: An SMM driver A DXE driver A Shell-based test app In order to use this test, the SMM and DXE drivers must be built and included in your FW image to be dispatched at boot time. The Shell-based app may be built at any time and run from Shell. The app will ask the DXE driver to pass a message to the SMM driver to invoke a particular test. It is not the intention of this test to include the two drivers in production systems. They should only be used for purpose-built test images.","title":"About This Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/","text":"variable Policy Unit Tests \u00b6 \ud83d\udd39 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About This Test \u00b6 This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries. Only policies that are created as a part of this test will be tested. 1. Try getting test context, if empty then get VP protocol, confirm that VP is not disabled by calling IsVariablePolicyEnabled. Log VP revision. 2. \"No lock\" policies: * check minsize enforcement * check maxsize enforcement * check musthave attr enforcement * check canthave attr enforcement * check one of the above with empty string policy i.e. name wildcard * check another one of the above with a \"#\" containing policy string * check policy prioritization by having a namespace-wide policy, a policy with a # wildcard, and a one-var specific policy and testing which one is enforced 3. \"Lock now\" policies (means if the var doesn't exist, it won't be created; if one exists, it can't be updated): * test a policy for an already existing variable, verify we can't write into that variable * create a policy for a non-existing variable and attempt to register such var 4. \"Lock on create\" policies (means the var can still be created, but no updates later, existing vars can't be updated): * create a var, lock it with LockOnCreate, attempt to update its contents * create LockOnCreate VP, attempt to create var with invalid size, then invalid attr, then create valid var, attempt to update its contents 5. \"Lock on var state\" policies (means the var protected by this policy can't be created or updated once the trigger is set) * create VP, trigger lock with a valid var, attempt to create a locked var, then modify the trigger var, create locked var * create VP, create targeted var, modify it, trigger lock, attempt to modify var * create VP, trigger lock with invalid (larger than one byte) var, see if VPE allows creation of the locked var (it should allow) * create VP, set locking var with wrong value, see if VPE allows creation of the locked var (should allow) 6. Attempt registering invalid policy entries * invalid required and banned attributes * large min size - let's say 2GB * max size equal to 0 * invalid policy type 7. Exercise dumping policy. No need to check the validity of the dump blob. 8. Test registering a policy with a random version. 9. Lock VPE, make sure old policies are enforced, new ones can't be registered. * Register a LockOnCreate policy * Lock VPE * Test locking it again. * Verify one of the prior policies is enforced * Make sure we can create variables even if those are protected by LockOnCreate policy, after locking the VPE * Attempt to register new policies * Make sure can't disable VPE * Cleanup: save context and reboot 10. Disable variable policy and try some things * Locate Variable Policy Protocol * Make sure VP is enabled * Register a policy * Disable VPE * Call IsVariablePolicyEnabled to confirm it's disabled. * Make sure can't lock policy * Make sure the policy from a is no longer enforced * Final cleanup: delete vars that were created in some earlier test suites","title":"Var Policy Unit Test App"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/#variable-policy-unit-tests","text":"","title":"variable Policy Unit Tests"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"&#x1F539; Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/#about-this-test","text":"This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries. Only policies that are created as a part of this test will be tested. 1. Try getting test context, if empty then get VP protocol, confirm that VP is not disabled by calling IsVariablePolicyEnabled. Log VP revision. 2. \"No lock\" policies: * check minsize enforcement * check maxsize enforcement * check musthave attr enforcement * check canthave attr enforcement * check one of the above with empty string policy i.e. name wildcard * check another one of the above with a \"#\" containing policy string * check policy prioritization by having a namespace-wide policy, a policy with a # wildcard, and a one-var specific policy and testing which one is enforced 3. \"Lock now\" policies (means if the var doesn't exist, it won't be created; if one exists, it can't be updated): * test a policy for an already existing variable, verify we can't write into that variable * create a policy for a non-existing variable and attempt to register such var 4. \"Lock on create\" policies (means the var can still be created, but no updates later, existing vars can't be updated): * create a var, lock it with LockOnCreate, attempt to update its contents * create LockOnCreate VP, attempt to create var with invalid size, then invalid attr, then create valid var, attempt to update its contents 5. \"Lock on var state\" policies (means the var protected by this policy can't be created or updated once the trigger is set) * create VP, trigger lock with a valid var, attempt to create a locked var, then modify the trigger var, create locked var * create VP, create targeted var, modify it, trigger lock, attempt to modify var * create VP, trigger lock with invalid (larger than one byte) var, see if VPE allows creation of the locked var (it should allow) * create VP, set locking var with wrong value, see if VPE allows creation of the locked var (should allow) 6. Attempt registering invalid policy entries * invalid required and banned attributes * large min size - let's say 2GB * max size equal to 0 * invalid policy type 7. Exercise dumping policy. No need to check the validity of the dump blob. 8. Test registering a policy with a random version. 9. Lock VPE, make sure old policies are enforced, new ones can't be registered. * Register a LockOnCreate policy * Lock VPE * Test locking it again. * Verify one of the prior policies is enforced * Make sure we can create variables even if those are protected by LockOnCreate policy, after locking the VPE * Attempt to register new policies * Make sure can't disable VPE * Cleanup: save context and reboot 10. Disable variable policy and try some things * Locate Variable Policy Protocol * Make sure VP is enabled * Register a policy * Disable VPE * Call IsVariablePolicyEnabled to confirm it's disabled. * Make sure can't lock policy * Make sure the policy from a is no longer enforced * Final cleanup: delete vars that were created in some earlier test suites","title":"About This Test"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/","text":"Xml Support Package \u00b6 About \u00b6 This package adds some limited XML support to the UEFI environment. Xml brings value in that there are numerous, robust, readily available parsing solutions in nearly every environment, language, and operating system. The UEFI support is limited in that it only supports ASCII strings and does not support XSD, schema, namespaces, or other extensions to XML. XmlTreeLib \u00b6 The XmlTreeLib is the cornerstone of this package. It provides functions for: Reading and parsing XML strings into an XML node/tree structure Creating or altering xml nodes within a tree Writing xml nodes/trees to ASCII string Escaping and Un-Escaping strings XmlTreeQueryLib \u00b6 The XmlTreeQueryLib provides very basic and simple query functions allowing code to interact with the XmlTree to do things like: Find the first child element node with a name equal to the parameter Find the first attribute node of a given element with a name equal to the parameter UnitTestResultReportLib \u00b6 A UnitTestResultReportLib that formats the results in XML using the JUnit defined schema. This instance allows the UEFI Unit Test Framework to integrate results with existing tools and other frameworks. Testing \u00b6 There are UEFI shell and host based unit tests for each library. These tests attempt to verify basic functionality of publicly defined functions. Check the Test/UnitTest folder at the root of the package for more details. Developer Notes \u00b6 These libraries have known limitations and have not been fully vetted for un-trusted input. If used in such a situation it is suggested to validate the input before leveraging the XML libraries. With that said the ability to use xml in UEFI has been invaluable for building features and tests that interact with code running in other environments. The parser has been tuned to fail fast and when invalid XML encountered just return NULL. Copyright \u00b6 Copyright (C) Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Modules"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#xml-support-package","text":"","title":"Xml Support Package"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#about","text":"This package adds some limited XML support to the UEFI environment. Xml brings value in that there are numerous, robust, readily available parsing solutions in nearly every environment, language, and operating system. The UEFI support is limited in that it only supports ASCII strings and does not support XSD, schema, namespaces, or other extensions to XML.","title":"About"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#xmltreelib","text":"The XmlTreeLib is the cornerstone of this package. It provides functions for: Reading and parsing XML strings into an XML node/tree structure Creating or altering xml nodes within a tree Writing xml nodes/trees to ASCII string Escaping and Un-Escaping strings","title":"XmlTreeLib"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#xmltreequerylib","text":"The XmlTreeQueryLib provides very basic and simple query functions allowing code to interact with the XmlTree to do things like: Find the first child element node with a name equal to the parameter Find the first attribute node of a given element with a name equal to the parameter","title":"XmlTreeQueryLib"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#unittestresultreportlib","text":"A UnitTestResultReportLib that formats the results in XML using the JUnit defined schema. This instance allows the UEFI Unit Test Framework to integrate results with existing tools and other frameworks.","title":"UnitTestResultReportLib"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#testing","text":"There are UEFI shell and host based unit tests for each library. These tests attempt to verify basic functionality of publicly defined functions. Check the Test/UnitTest folder at the root of the package for more details.","title":"Testing"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#developer-notes","text":"These libraries have known limitations and have not been fully vetted for un-trusted input. If used in such a situation it is suggested to validate the input before leveraging the XML libraries. With that said the ability to use xml in UEFI has been invaluable for building features and tests that interact with code running in other environments. The parser has been tuned to fail fast and when invalid XML encountered just return NULL.","title":"Developer Notes"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#copyright","text":"Copyright (C) Microsoft Corporation. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/","text":"Project Mu Silicon Arm Tiano Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_silicon_arm_tiano.git Branch: release/202005 Commit: 093c3bbce64d2872acedfa859c4cc5b705c363af Commit Date: 2020-06-25 14:27:27 -0700 This repository contains Project Mu code based on TianoCore edk2 code for ARM silicon features and ARM based platforms. More Info \u00b6 This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements Builds \u00b6 pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#project-mu-silicon-arm-tiano-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_silicon_arm_tiano.git Branch: release/202005 Commit: 093c3bbce64d2872acedfa859c4cc5b705c363af Commit Date: 2020-06-25 14:27:27 -0700 This repository contains Project Mu code based on TianoCore edk2 code for ARM silicon features and ARM based platforms.","title":"Project Mu Silicon Arm Tiano Repository"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#more-info","text":"This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#builds","text":"pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json","title":"Builds"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_silicon_arm_tiano/ArmPkg/Library/ArmSoftFloatLib/berkeley-softfloat-3/","text":"Package Overview for Berkeley SoftFloat Release 3e \u00b6 John R. Hauser 2018 January 20 Berkeley SoftFloat is a software implementation of binary floating-point that conforms to the IEEE Standard for Floating-Point Arithmetic. SoftFloat is distributed in the form of C source code. Building the SoftFloat sources generates a library file (typically softfloat.a or libsoftfloat.a ) containing the floating-point subroutines. The SoftFloat package is documented in the following files in the doc subdirectory: SoftFloat.html Documentation for using the SoftFloat functions. SoftFloat-source.html Documentation for building SoftFloat. SoftFloat-history.html History of the major changes to SoftFloat. Other files in the package comprise the source code for SoftFloat.","title":"berkeley-softfloat-3"},{"location":"dyn/mu_silicon_arm_tiano/ArmPkg/Library/ArmSoftFloatLib/berkeley-softfloat-3/#package-overview-for-berkeley-softfloat-release-3e","text":"John R. Hauser 2018 January 20 Berkeley SoftFloat is a software implementation of binary floating-point that conforms to the IEEE Standard for Floating-Point Arithmetic. SoftFloat is distributed in the form of C source code. Building the SoftFloat sources generates a library file (typically softfloat.a or libsoftfloat.a ) containing the floating-point subroutines. The SoftFloat package is documented in the following files in the doc subdirectory: SoftFloat.html Documentation for using the SoftFloat functions. SoftFloat-source.html Documentation for building SoftFloat. SoftFloat-history.html History of the major changes to SoftFloat. Other files in the package comprise the source code for SoftFloat.","title":"Package Overview for Berkeley SoftFloat Release 3e"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/","text":"ArmVirtPkg - Platform CI \u00b6 This Readme.md describes the Azure DevOps based Platform CI for ArmVirtPkg and how to use the same Pytools based build infrastructure locally. Supported Configuration Details \u00b6 This solution for building and running ArmVirtPkg has only been validated with Ubuntu 18.04 and the GCC5 toolchain. Two different firmware builds are supported and are described below. Configuration name Architecture DSC File Additional Flags AARCH64 AARCH64 ArmVirtQemu.dsc None ARM ARM ArmVirtQemu.dsc None EDK2 Developer environment \u00b6 Python 3.8.x - Download & Install GIT - Download & Install QEMU - Download, Install, and add to your path Edk2 Source Additional packages found necessary for Ubuntu 18.04 apt-get install gcc g++ make uuid-dev Note: edksetup, Submodule initialization and manual installation of NASM, iASL, or the required cross-compiler toolchains are not required, this is handled by the Pytools build system. Building with Pytools for ArmVirtPkg \u00b6 [Optional] Create a Python Virtual Environment - generally once per workspace python -m venv <name of virtual environment> [Optional] Activate Virtual Environment - each time new shell opened Windows <name of virtual environment>/Scripts/activate.bat Linux source <name of virtual environment>/bin/activate Install Pytools - generally once per virtual env or whenever pip-requirements.txt changes pip install --upgrade -r pip-requirements.txt Initialize & Update Submodules - only when submodules updated stuart_setup -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> Initialize & Update Dependencies - only as needed when ext_deps change stuart_update -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> Compile the basetools if necessary - only when basetools C source files change python BaseTools/Edk2ToolsBuild.py -t <ToolChainTag> Compile Firmware stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> use stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py -h option to see additional options like --clean Running Emulator You can add --FlashRom to the end of your build command and the emulator will run after the build is complete. or use the --FlashOnly feature to just run the emulator. stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> --FlashOnly Notes \u00b6 Including the expected build architecture and toolchain to the stuart_update command is critical. This is because there are extra scopes and tools that will be resolved during the update step that need to match your build step. Configuring ACTIVE_PLATFORM and TARGET_ARCH in Conf/target.txt is not required. This environment is set by PlatformBuild.py based upon the [-a <TARGET_ARCH>] parameter. QEMU must be on your path. On Windows this is a manual process and not part of the QEMU installer. NOTE: Logging the execution output will be in the normal stuart log as well as to your console. Custom Build Options \u00b6 MAKE_STARTUP_NSH=TRUE will output a startup.nsh file to the location mapped as fs0. This is used in CI in combination with the --FlashOnly feature to run QEMU to the UEFI shell and then execute the contents of startup.nsh . QEMU_HEADLESS=TRUE Since CI servers run headless QEMU must be told to run with no display otherwise an error occurs. Locally you don't need to set this. Passing Build Defines \u00b6 To pass build defines through stuart_build , prepend BLD_*_ to the define name and pass it on the command-line. stuart_build currently requires values to be assigned, so add an =1 suffix for bare defines. For example, to enable the TPM2 support, instead of the traditional \"-D TPM2_ENABLE=TRUE\", the stuart_build command-line would be: stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py BLD_*_TPM2_ENABLE=TRUE References \u00b6 Installing and using Pytools More on python virtual environments","title":"Platform CI"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#armvirtpkg-platform-ci","text":"This Readme.md describes the Azure DevOps based Platform CI for ArmVirtPkg and how to use the same Pytools based build infrastructure locally.","title":"ArmVirtPkg - Platform CI"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#supported-configuration-details","text":"This solution for building and running ArmVirtPkg has only been validated with Ubuntu 18.04 and the GCC5 toolchain. Two different firmware builds are supported and are described below. Configuration name Architecture DSC File Additional Flags AARCH64 AARCH64 ArmVirtQemu.dsc None ARM ARM ArmVirtQemu.dsc None","title":"Supported Configuration Details"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#edk2-developer-environment","text":"Python 3.8.x - Download & Install GIT - Download & Install QEMU - Download, Install, and add to your path Edk2 Source Additional packages found necessary for Ubuntu 18.04 apt-get install gcc g++ make uuid-dev Note: edksetup, Submodule initialization and manual installation of NASM, iASL, or the required cross-compiler toolchains are not required, this is handled by the Pytools build system.","title":"EDK2 Developer environment"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#building-with-pytools-for-armvirtpkg","text":"[Optional] Create a Python Virtual Environment - generally once per workspace python -m venv <name of virtual environment> [Optional] Activate Virtual Environment - each time new shell opened Windows <name of virtual environment>/Scripts/activate.bat Linux source <name of virtual environment>/bin/activate Install Pytools - generally once per virtual env or whenever pip-requirements.txt changes pip install --upgrade -r pip-requirements.txt Initialize & Update Submodules - only when submodules updated stuart_setup -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> Initialize & Update Dependencies - only as needed when ext_deps change stuart_update -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> Compile the basetools if necessary - only when basetools C source files change python BaseTools/Edk2ToolsBuild.py -t <ToolChainTag> Compile Firmware stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> use stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py -h option to see additional options like --clean Running Emulator You can add --FlashRom to the end of your build command and the emulator will run after the build is complete. or use the --FlashOnly feature to just run the emulator. stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py TOOL_CHAIN_TAG = <TOOL_CHAIN_TAG> -a <TARGET_ARCH> --FlashOnly","title":"Building with Pytools for ArmVirtPkg"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#notes","text":"Including the expected build architecture and toolchain to the stuart_update command is critical. This is because there are extra scopes and tools that will be resolved during the update step that need to match your build step. Configuring ACTIVE_PLATFORM and TARGET_ARCH in Conf/target.txt is not required. This environment is set by PlatformBuild.py based upon the [-a <TARGET_ARCH>] parameter. QEMU must be on your path. On Windows this is a manual process and not part of the QEMU installer. NOTE: Logging the execution output will be in the normal stuart log as well as to your console.","title":"Notes"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#custom-build-options","text":"MAKE_STARTUP_NSH=TRUE will output a startup.nsh file to the location mapped as fs0. This is used in CI in combination with the --FlashOnly feature to run QEMU to the UEFI shell and then execute the contents of startup.nsh . QEMU_HEADLESS=TRUE Since CI servers run headless QEMU must be told to run with no display otherwise an error occurs. Locally you don't need to set this.","title":"Custom Build Options"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#passing-build-defines","text":"To pass build defines through stuart_build , prepend BLD_*_ to the define name and pass it on the command-line. stuart_build currently requires values to be assigned, so add an =1 suffix for bare defines. For example, to enable the TPM2 support, instead of the traditional \"-D TPM2_ENABLE=TRUE\", the stuart_build command-line would be: stuart_build -c ArmVirtPkg/PlatformCI/PlatformBuild.py BLD_*_TPM2_ENABLE=TRUE","title":"Passing Build Defines"},{"location":"dyn/mu_silicon_arm_tiano/ArmVirtPkg/PlatformCI/ReadMe/#references","text":"Installing and using Pytools More on python virtual environments","title":"References"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/","text":"Azure DevOps Pipelines \u00b6 These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution. Core CI \u00b6 Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details Platform CI \u00b6 Focused on building a single target platform and confirming functionality on that platform. Conventions \u00b6 Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml Links \u00b6 Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library Lessons Learned \u00b6 Templates and parameters \u00b6 They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late. File matching patterns \u00b6 On Linux this can hang if there are too many files in the search list. Templates and file splitting \u00b6 Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs. Conditional steps \u00b6 If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Read Me"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#azure-devops-pipelines","text":"These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution.","title":"Azure DevOps Pipelines"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#core-ci","text":"Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details","title":"Core CI"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#platform-ci","text":"Focused on building a single target platform and confirming functionality on that platform.","title":"Platform CI"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#conventions","text":"Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml","title":"Conventions"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#links","text":"Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library","title":"Links"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#lessons-learned","text":"","title":"Lessons Learned"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#templates-and-parameters","text":"They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late.","title":"Templates and parameters"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#file-matching-patterns","text":"On Linux this can hang if there are too many files in the search list.","title":"File matching patterns"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#templates-and-file-splitting","text":"Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs.","title":"Templates and file splitting"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/ReadMe/#conditional-steps","text":"If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Conditional steps"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/","text":"CI Templates \u00b6 This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation. Common CI templates \u00b6 basetools-build-steps.yml \u00b6 This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds. Core CI templates \u00b6 pr-gate-build-job.yml \u00b6 This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI. pr-gate-steps.yml \u00b6 This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\" spell-check-prereq-steps.yml \u00b6 This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. Platform CI templates \u00b6 platform-build-run-steps.yml \u00b6 This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools Special Notes \u00b6 For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"templates"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#ci-templates","text":"This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation.","title":"CI Templates"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#common-ci-templates","text":"","title":"Common CI templates"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#basetools-build-stepsyml","text":"This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds.","title":"basetools-build-steps.yml"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#core-ci-templates","text":"","title":"Core CI templates"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#pr-gate-build-jobyml","text":"This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI.","title":"pr-gate-build-job.yml"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#pr-gate-stepsyml","text":"This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\"","title":"pr-gate-steps.yml"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#spell-check-prereq-stepsyml","text":"This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0.","title":"spell-check-prereq-steps.yml"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#platform-ci-templates","text":"","title":"Platform CI templates"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#platform-build-run-stepsyml","text":"This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools","title":"platform-build-run-steps.yml"},{"location":"dyn/mu_silicon_arm_tiano/azurepipelines/templates/ReadMe/#special-notes","text":"For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"Special Notes"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues ArmPkg ArmPlatformPkg ArmVirtPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment Host Module Inclusion Test - HostUnitTestDscCompleteCheck \u00b6 This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin \u00b6 A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way... Including Host-Based Tests in the Package YAML \u00b6 For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided. Configuring the HostBased DSC \u00b6 The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"pytool"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues ArmPkg ArmPlatformPkg ArmVirtPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#host-module-inclusion-test-hostunittestdsccompletecheck","text":"This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment","title":"Host Module Inclusion Test - HostUnitTestDscCompleteCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#host-unit-test-compilation-and-run-test-hostunittestcompilerplugin","text":"A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way...","title":"Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#including-host-based-tests-in-the-package-yaml","text":"For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided.","title":"Including Host-Based Tests in the Package YAML"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#configuring-the-hostbased-dsc","text":"The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION.","title":"Configuring the HostBased DSC"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows","title":"PyTool Scopes"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"Future investments"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/","text":"Project Mu Silicon Intel Tiano Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_silicon_intel_tiano.git Branch: release/202005 Commit: 2adff19e21e1a29003c138f5a111780be8905a2b Commit Date: 2020-06-25 22:32:35 -0700 About \u00b6 This repository contains Project Mu code based on TianoCore edk2 code for Intel silicon features and Intel based platforms. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Copyright & License \u00b6 Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#project-mu-silicon-intel-tiano-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_silicon_intel_tiano.git Branch: release/202005 Commit: 2adff19e21e1a29003c138f5a111780be8905a2b Commit Date: 2020-06-25 22:32:35 -0700","title":"Project Mu Silicon Intel Tiano Repository"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#about","text":"This repository contains Project Mu code based on TianoCore edk2 code for Intel silicon features and Intel based platforms. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"About"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#copyright-license","text":"Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Readme/","text":"IntelFsp2Pkg \u00b6 This package provides the component to create an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2Pkg A whitepaper to describe the IntelFsp2Pkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Creating_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"Readme"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Readme/#intelfsp2pkg","text":"This package provides the component to create an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2Pkg A whitepaper to describe the IntelFsp2Pkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Creating_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"IntelFsp2Pkg"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/","text":"Name \u00b6 GenCfgOpt.py The python script that generates UPD text ( .txt ) files for the compiler, header files for the UPD regions, and generates a Boot Settings File ( BSF ), all from an EDK II Platform Description ( DSC ) file. Synopsis \u00b6 GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [TxtOutFile] [-D Macros] GenCfgOpt HEADER PlatformDscFile BuildFvDir [InputHFile] [-D Macros] GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [-D Macros] Description \u00b6 GenCfgOpt.py is a script that generates configuration options from an EDK II Platform Description (DSC) file. It has three functions. It produces a .txt file that is used by the compiler that summarizes the UPD section in the DSC file. It generates header files for the UPD regions. It generates a Boot Settings File (BSF) that can be used by the Binary Configuration Tool (BCT) to provide a graphical user interface for manipulating settings in the UPD regions. The GenCfgOpt.py script generates important files that are vital parts of your build process. The UPDTXT and HEADER use cases must be done before the 'build' command; the GENBSF use case may be done at any time. The following sections explain the three use cases. 1. GenCfgOpt.py UPDTXT \u00b6 The UPDTXT option creates a text file with all the UPD entries, offsets, size in bytes, and values. GenCfgOpt reads this information from the [PcdsDynamicVpd.Upd] section of the project's DSC file. The DSC file allows you to specify offsets and sizes for each entry, opening up the possibility of introducing gaps between entries. GenCfgOpt fills in these gaps with UPD entries that have the generic names UnusedUpdSpaceN where N begins with 0 and increments. The command signature for UPDTXT is: GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [TxtOutFile] [-D Macros] PlatformDscFile must be the location of the DSC file for the platform you're building. BuildFvDir is the location where the binary will be stored. The optional TxtOutFile is a name and location for the output of GenCfgOpt . The default name and location is the <UPD_TOOL_GUID>.txt in the directory specified by BuildFvDir . The macro UPD_TOOL_GUID must be defined in the DSC file or in the optional Macros arguments. Each optional macro argument must follow the form ?D <MACRO_NAME>=<VALUE> . GenCfgOpt checks to see if the UPD txt file has already been created and will only re-create it if the DSC was modified after it was created. 2. GenCfgOpt.py HEADER \u00b6 The HEADER option creates header files in the build folder. Both header files define the _UPD_DATA_REGION data structures in FspUpd.h, FsptUpd.h, FspmUpd.h and FspsUpd.h. In these header files any undefined elements of structures will be added as ReservedUpdSpaceN beginning with N=0. The command signature for HEADER is GenCfgOpt HEADER PlatformDscFile BuildFvDir [InputHFile] [-D Macros] PlatformDscFile and BuildFvDir are described in the previous section. The optional InputHFile is a header file that may contain data definitions that are used by variables in the UPD regions. This header file must contain the special keywords !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END in comments. Everything between these two keywords will be included in the generated header file. The mechanism to specify whether a variable appears as ReservedUpdSpaceN in the FspUpd.h header file is in special commands that appear in the comments of the DSC file. The special commands begin with !HDR , for header. The following table summarizes the two command options. HEADER \u00b6 Use the HEADER command to hide specific variables in the public header file. In your project DSC file, use !HDR HEADER:{OFF} at the beginning of the section you wish to hide and !HDR HEADER:{ON} at the end. STRUCT \u00b6 The STRUCT command allows you to specify a specific data type for a variable. You can specify a pointer to a data struct, for example. You define the data structure in the InputHFile between !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END . Example: \u00b6 !HDR STRUCT:{MY_DATA_STRUCT*} You then define MY_DATA_STRUCT in InputHFile . EMBED \u00b6 The EMBED command allows you to put one or more UPD data into a specify data structure. You can utilize it as a group of UPD for example. You must specify a start and an end for the specify data structure. Example: \u00b6 !HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:START} gTokenSpaceGuid.Upd1 | 0x0020 | 0x01 | 0x00 gTokenSpaceGuid.Upd2 | 0x0021 | 0x01 | 0x00 !HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:END} gTokenSpaceGuid.UpdN | 0x0022 | 0x01 | 0x00 Result: \u00b6 typedef struct { /** Offset 0x0020 **/ UINT8 Upd1; /** Offset 0x0021 **/ UINT8 Upd2; /** Offset 0x0022 **/ UINT8 UpdN; } MY_DATA_STRUCT; typedef struct _UPD_DATA_REGION { ... /** Offset 0x0020 **/ MY_DATA_STRUCT MyDataStruct; ... } UPD_DATA_REGION; 3. GenCfgOpt .py GENBSF \u00b6 The GENBSF option generates a BSF from the UPD entries in a package's DSC file. It does this by parsing special commands found in the comments of the DSC file. They roughly match the keywords that define the different sections of the BSF. The command signature for GENBSF is GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [-D Macros] In this case, the BsfOutFile parameter is required; it should be the relative path to where the BSF should be stored. Every BSF command in the DSC file begins with !BSF or @Bsf . The following table summarizes the options that come after !BSF or @Bsf : BSF Commands Description \u00b6 PAGES \u00b6 PAGES maps abbreviations to friendly-text descriptions of the pages in a BSF. Example: \u00b6 !BSF PAGES:{PG1:?Page 1?, PG2:?Page 2?} or @Bsf PAGES:{PG1:?Page 1?, PG2:?Page 2?} PAGE \u00b6 This marks the beginning of a page. Use the abbreviation specified in PAGES command. Example: \u00b6 !BSF PAGE:{PG1} or @Bsf PAGE:{PG1} All the entries that come after this command are assumed to be on that page, until the next PAGE command FIND \u00b6 FIND maps to the BSF Find command. It will be placed in the StructDef region of the BSF and should come at the beginning of the UPD sections of the DSC, immediately before the signatures that mark the beginning of these sections. The content should be the plain-text equivalent of the signature. The signature is usually 8 characters. Example: \u00b6 !BSF FIND:{PROJSIG1} or @Bsf FIND:{PROJSIG1} BLOCK \u00b6 The BLOCK command maps to the BeginInfoBlock section of the BSF. There are two elements: a version number and a plain-text description. Example: \u00b6 !BSF BLOCK:{NAME:\"My platform name\", VER:\"0.1\"} or @Bsf BLOCK:{NAME:\"My platform name\", VER:\"0.1\"} NAME \u00b6 NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT . Example: \u00b6 !BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the !BSF NAME or @Bsf NAME command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF. TYPE \u00b6 The TYPE command is used either by itself or with the NAME command. It is usually used by itself when defining an EditNum field for the BSF. You specify the type of data in the second parameter and the range of valid values in the third. Example: \u00b6 !BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)} TYPE appears on the same line as the NAME command when using a combo-box. Example: \u00b6 !BSF NAME:{Variable 1} TYPE:{Combo} or @Bsf NAME:{Variable 1} TYPE:{Combo} There is a special None type that puts the variable in the StructDef region of the BSF, but doesn't put it in any Page section. This makes the variable visible to BCT, but not to the end user. HELP \u00b6 The HELP command defines what will appear in the help text for each control in BCT. Example: \u00b6 !BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.} OPTION \u00b6 The OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options. Example: \u00b6 !BSF OPTION:{0:IDE, 1:AHCI, 2:RAID} !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} or @Bsf OPTION:{0:IDE, 1:AHCI, 2:RAID} @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} FIELD \u00b6 The FIELD command can be used to define a section of a consolidated PCD such that the PCD will be displayed in several fields via BCT interface instead of one long entry. Example: \u00b6 !BSF FIELD:{PcdDRAMSpeed:1} or @Bsf FIELD:{PcdDRAMSpeed:1} ORDER \u00b6 The ORDER command can be used to adjust the display order for the BSF items. By default the order value for a BSF item is assigned to be the UPD item (Offset * 256) . It can be overridden by declaring ORDER command using format ORDER: {HexMajor.HexMinor} . In this case the order value will be (HexMajor*256+HexMinor) . The item order value will be used as the sort key during the BSF item display. Example: \u00b6 !BSF ORDER:{0x0040.01} or @Bsf ORDER:{0x0040.01} For OPTION and HELP commands, it allows to split the contents into multiple lines by adding multiple OPTION and HELP command lines. The lines except for the very first line need to start with + in the content to tell the tool to append this string to the previous one. For example, the statement !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: !BSF OPTION:{0x00:0 MB, 0x01:32 MB,} !BSF OPTION:{+ 0x02:64 MB} or @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: @Bsf OPTION:{0x00:0 MB, 0x01:32 MB,} @Bsf OPTION:{+ 0x02:64 MB} The NAME , OPTION , TYPE , and HELP commands can all appear on the same line following the !BSF or @Bsf keyword or they may appear on separate lines to improve readability. There are four alternative ways to replace current BSF commands. 1. # @Prompt \u00b6 An alternative way replacing NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT. Example: \u00b6 # @Prompt Variable 0 The above example can replace the two methods as below. !BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the # @Prompt command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF. 2. ## \u00b6 An alternative way replacing HELP command defines what will appear in the help text for each control in BCT. Example: \u00b6 ## Enable/disable LAN controller. The above example can replace the two methods as below. !BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.} 3. # @ValidList \u00b6 An alternative way replacing OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options. Example: \u00b6 ``` # @ValidList 0x80000003 | 0, 1, 2 | IDE, AHCI, RAID Error Code | Options | Descriptions The above example can replace the two methods as below. ```!BSF OPTION:{0:IDE, 1:AHCI, 2:RAID}``` or ```@Bsf OPTION:{0:IDE, 1:AHCI, 2:RAID}``` ### 4. ```# @ValidRange``` An alternative way replace **EditNum** field for the BSF. #####Example: ```# @ValidRange 0x80000001 | 0x0 ? 0xFF Error Code | Range The above example can replace the two methods as below. !BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)}","title":"Gen Cfg Opt User Manual"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#name","text":"GenCfgOpt.py The python script that generates UPD text ( .txt ) files for the compiler, header files for the UPD regions, and generates a Boot Settings File ( BSF ), all from an EDK II Platform Description ( DSC ) file.","title":"Name"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#synopsis","text":"GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [TxtOutFile] [-D Macros] GenCfgOpt HEADER PlatformDscFile BuildFvDir [InputHFile] [-D Macros] GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [-D Macros]","title":"Synopsis"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#description","text":"GenCfgOpt.py is a script that generates configuration options from an EDK II Platform Description (DSC) file. It has three functions. It produces a .txt file that is used by the compiler that summarizes the UPD section in the DSC file. It generates header files for the UPD regions. It generates a Boot Settings File (BSF) that can be used by the Binary Configuration Tool (BCT) to provide a graphical user interface for manipulating settings in the UPD regions. The GenCfgOpt.py script generates important files that are vital parts of your build process. The UPDTXT and HEADER use cases must be done before the 'build' command; the GENBSF use case may be done at any time. The following sections explain the three use cases.","title":"Description"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#1-gencfgoptpy-updtxt","text":"The UPDTXT option creates a text file with all the UPD entries, offsets, size in bytes, and values. GenCfgOpt reads this information from the [PcdsDynamicVpd.Upd] section of the project's DSC file. The DSC file allows you to specify offsets and sizes for each entry, opening up the possibility of introducing gaps between entries. GenCfgOpt fills in these gaps with UPD entries that have the generic names UnusedUpdSpaceN where N begins with 0 and increments. The command signature for UPDTXT is: GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [TxtOutFile] [-D Macros] PlatformDscFile must be the location of the DSC file for the platform you're building. BuildFvDir is the location where the binary will be stored. The optional TxtOutFile is a name and location for the output of GenCfgOpt . The default name and location is the <UPD_TOOL_GUID>.txt in the directory specified by BuildFvDir . The macro UPD_TOOL_GUID must be defined in the DSC file or in the optional Macros arguments. Each optional macro argument must follow the form ?D <MACRO_NAME>=<VALUE> . GenCfgOpt checks to see if the UPD txt file has already been created and will only re-create it if the DSC was modified after it was created.","title":"1. GenCfgOpt.py UPDTXT"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#2-gencfgoptpy-header","text":"The HEADER option creates header files in the build folder. Both header files define the _UPD_DATA_REGION data structures in FspUpd.h, FsptUpd.h, FspmUpd.h and FspsUpd.h. In these header files any undefined elements of structures will be added as ReservedUpdSpaceN beginning with N=0. The command signature for HEADER is GenCfgOpt HEADER PlatformDscFile BuildFvDir [InputHFile] [-D Macros] PlatformDscFile and BuildFvDir are described in the previous section. The optional InputHFile is a header file that may contain data definitions that are used by variables in the UPD regions. This header file must contain the special keywords !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END in comments. Everything between these two keywords will be included in the generated header file. The mechanism to specify whether a variable appears as ReservedUpdSpaceN in the FspUpd.h header file is in special commands that appear in the comments of the DSC file. The special commands begin with !HDR , for header. The following table summarizes the two command options.","title":"2. GenCfgOpt.py HEADER"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#header","text":"Use the HEADER command to hide specific variables in the public header file. In your project DSC file, use !HDR HEADER:{OFF} at the beginning of the section you wish to hide and !HDR HEADER:{ON} at the end.","title":"HEADER"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#struct","text":"The STRUCT command allows you to specify a specific data type for a variable. You can specify a pointer to a data struct, for example. You define the data structure in the InputHFile between !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END .","title":"STRUCT"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example","text":"!HDR STRUCT:{MY_DATA_STRUCT*} You then define MY_DATA_STRUCT in InputHFile .","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#embed","text":"The EMBED command allows you to put one or more UPD data into a specify data structure. You can utilize it as a group of UPD for example. You must specify a start and an end for the specify data structure.","title":"EMBED"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_1","text":"!HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:START} gTokenSpaceGuid.Upd1 | 0x0020 | 0x01 | 0x00 gTokenSpaceGuid.Upd2 | 0x0021 | 0x01 | 0x00 !HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:END} gTokenSpaceGuid.UpdN | 0x0022 | 0x01 | 0x00","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#result","text":"typedef struct { /** Offset 0x0020 **/ UINT8 Upd1; /** Offset 0x0021 **/ UINT8 Upd2; /** Offset 0x0022 **/ UINT8 UpdN; } MY_DATA_STRUCT; typedef struct _UPD_DATA_REGION { ... /** Offset 0x0020 **/ MY_DATA_STRUCT MyDataStruct; ... } UPD_DATA_REGION;","title":"Result:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#3-gencfgopt-py-genbsf","text":"The GENBSF option generates a BSF from the UPD entries in a package's DSC file. It does this by parsing special commands found in the comments of the DSC file. They roughly match the keywords that define the different sections of the BSF. The command signature for GENBSF is GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [-D Macros] In this case, the BsfOutFile parameter is required; it should be the relative path to where the BSF should be stored. Every BSF command in the DSC file begins with !BSF or @Bsf . The following table summarizes the options that come after !BSF or @Bsf :","title":"3. GenCfgOpt .py GENBSF"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#bsf-commands-description","text":"","title":"BSF Commands Description"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#pages","text":"PAGES maps abbreviations to friendly-text descriptions of the pages in a BSF.","title":"PAGES"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_2","text":"!BSF PAGES:{PG1:?Page 1?, PG2:?Page 2?} or @Bsf PAGES:{PG1:?Page 1?, PG2:?Page 2?}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#page","text":"This marks the beginning of a page. Use the abbreviation specified in PAGES command.","title":"PAGE"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_3","text":"!BSF PAGE:{PG1} or @Bsf PAGE:{PG1} All the entries that come after this command are assumed to be on that page, until the next PAGE command","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#find","text":"FIND maps to the BSF Find command. It will be placed in the StructDef region of the BSF and should come at the beginning of the UPD sections of the DSC, immediately before the signatures that mark the beginning of these sections. The content should be the plain-text equivalent of the signature. The signature is usually 8 characters.","title":"FIND"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_4","text":"!BSF FIND:{PROJSIG1} or @Bsf FIND:{PROJSIG1}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#block","text":"The BLOCK command maps to the BeginInfoBlock section of the BSF. There are two elements: a version number and a plain-text description.","title":"BLOCK"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_5","text":"!BSF BLOCK:{NAME:\"My platform name\", VER:\"0.1\"} or @Bsf BLOCK:{NAME:\"My platform name\", VER:\"0.1\"}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#name_1","text":"NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT .","title":"NAME"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_6","text":"!BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the !BSF NAME or @Bsf NAME command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#type","text":"The TYPE command is used either by itself or with the NAME command. It is usually used by itself when defining an EditNum field for the BSF. You specify the type of data in the second parameter and the range of valid values in the third.","title":"TYPE"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_7","text":"!BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)} TYPE appears on the same line as the NAME command when using a combo-box.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_8","text":"!BSF NAME:{Variable 1} TYPE:{Combo} or @Bsf NAME:{Variable 1} TYPE:{Combo} There is a special None type that puts the variable in the StructDef region of the BSF, but doesn't put it in any Page section. This makes the variable visible to BCT, but not to the end user.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#help","text":"The HELP command defines what will appear in the help text for each control in BCT.","title":"HELP"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_9","text":"!BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#option","text":"The OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options.","title":"OPTION"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_10","text":"!BSF OPTION:{0:IDE, 1:AHCI, 2:RAID} !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} or @Bsf OPTION:{0:IDE, 1:AHCI, 2:RAID} @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#field","text":"The FIELD command can be used to define a section of a consolidated PCD such that the PCD will be displayed in several fields via BCT interface instead of one long entry.","title":"FIELD"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_11","text":"!BSF FIELD:{PcdDRAMSpeed:1} or @Bsf FIELD:{PcdDRAMSpeed:1}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#order","text":"The ORDER command can be used to adjust the display order for the BSF items. By default the order value for a BSF item is assigned to be the UPD item (Offset * 256) . It can be overridden by declaring ORDER command using format ORDER: {HexMajor.HexMinor} . In this case the order value will be (HexMajor*256+HexMinor) . The item order value will be used as the sort key during the BSF item display.","title":"ORDER"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_12","text":"!BSF ORDER:{0x0040.01} or @Bsf ORDER:{0x0040.01} For OPTION and HELP commands, it allows to split the contents into multiple lines by adding multiple OPTION and HELP command lines. The lines except for the very first line need to start with + in the content to tell the tool to append this string to the previous one. For example, the statement !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: !BSF OPTION:{0x00:0 MB, 0x01:32 MB,} !BSF OPTION:{+ 0x02:64 MB} or @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: @Bsf OPTION:{0x00:0 MB, 0x01:32 MB,} @Bsf OPTION:{+ 0x02:64 MB} The NAME , OPTION , TYPE , and HELP commands can all appear on the same line following the !BSF or @Bsf keyword or they may appear on separate lines to improve readability. There are four alternative ways to replace current BSF commands.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#1-prompt","text":"An alternative way replacing NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT.","title":"1. # @Prompt"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_13","text":"# @Prompt Variable 0 The above example can replace the two methods as below. !BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the # @Prompt command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#2","text":"An alternative way replacing HELP command defines what will appear in the help text for each control in BCT.","title":"2. ##"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_14","text":"## Enable/disable LAN controller. The above example can replace the two methods as below. !BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#3-validlist","text":"An alternative way replacing OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options.","title":"3. # @ValidList"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_15","text":"``` # @ValidList 0x80000003 | 0, 1, 2 | IDE, AHCI, RAID Error Code | Options | Descriptions The above example can replace the two methods as below. ```!BSF OPTION:{0:IDE, 1:AHCI, 2:RAID}``` or ```@Bsf OPTION:{0:IDE, 1:AHCI, 2:RAID}``` ### 4. ```# @ValidRange``` An alternative way replace **EditNum** field for the BSF. #####Example: ```# @ValidRange 0x80000001 | 0x0 ? 0xFF Error Code | Range The above example can replace the two methods as below. !BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/","text":"Name \u00b6 PatchFv.py - The python script that patches the firmware volumes ( FV ) with in the flash device ( FD ) file post FSP build. Synopsis \u00b6 PatchFv FvBuildDir [FvFileBaseNames:]FdFileBaseNameToPatch [\"Offset, Value\"]+ | [\"Offset, Value, @Comment\"]+ | [\"Offset, Value, $Command\"]+ | [\"Offset, Value, $Command, @Comment\"]+ Description \u00b6 The PatchFv.py tool allows the developer to fix up FD images to follow the Intel FSP Architecture specification. It also makes the FD image relocatable. The tool is written in Python and uses Python 2.7 or later to run. Consider using the tool in a build script. FvBuildDir (Argument 1) \u00b6 This is the first argument that PatchFv.py requires. It is the build directory for all firmware volumes created during the FSP build. The path must be either an absolute path or a relevant path, relevant to the top level of the FSP tree. Example usage: \u00b6 Build\\YouPlatformFspPkg\\%BD_TARGET%_%VS_VERSION%%VS_X86%\\FV The example used contains Windows batch script %VARIABLES%. FvFileBaseNames (Argument 2: Optional Part 1) \u00b6 The firmware volume file base names ( FvFileBaseNames ) are the independent Fv?s that are to be patched within the FD. (0 or more in the form FVFILEBASENAME: ) The colon : is used for delimiting the single argument and must be appended to the end of each ( FvFileBaseNames ). Example usage: \u00b6 STAGE1:STAGE2:MANIFEST:YOURPLATFORM In the example STAGE1 is STAGE1.Fv in YOURPLATFORM.fd . FdFileNameToPatch (Argument 2: Mandatory Part 2) \u00b6 Firmware device file name to patch ( FdFileNameToPatch ) is the base name of the FD file that is to be patched. (1 only, in the form YOURPLATFORM ) Example usage: \u00b6 STAGE1:STAGE2:MANIFEST:YOURPLATFORM In the example YOURPLATFORM is from YOURPLATFORM.fd \"Offset, Value[, Command][, Comment]\" (Argument 3) \u00b6 The Offset can be a positive or negative number and represents where the Value to be patched is located within the FD. The Value is what will be written at the given Offset in the FD. Constants may be used for both offsets and values. Also, this argument handles expressions for both offsets and values using these operators: = - * & | ~ ( ) [ ] { } < > The entire argument includes the quote marks like in the example argument below: 0xFFFFFFC0, SomeCore:__EntryPoint - [0x000000F0],@SomeCore Entry Constants: \u00b6 Hexadecimal (use 0x as prefix) | Decimal Examples: \u00b6 Positive Hex Negative Hex Positive Decimal Negative Decimal 0x000000BC 0xFFFFFFA2 188 -94 ModuleName:FunctionName | ModuleName:GlobalVariableName ModuleGuid:Offset Operators: \u00b6 + Addition - Subtraction * Multiplication & Logical and | Logical or ~ Complement ( ) Evaluation control [ ] Get a DWord value at the specified offset expression from [expr] { } Convert an offset {expr} into an absolute address (FSP_BASE + expr) < > Convert absolute address <expr> into an image offset (expr & FSP_SIZE) Special Commands: \u00b6 Special commands must use the $ symbol as a prefix to the command itself. There is only one command available at this time. $COPY ? Copy a binary block from source to destination. Example: \u00b6 0x94, [PlatformInit:__gPcd_BinPatch_FvRecOffset] + 0x94, [0x98], $COPY, @Sync up 2nd FSP Header Comments: \u00b6 Comments are allowed in the Offset, Value [, Comment] argument. Comments must use the @ symbol as a prefix. The comment will output to the build window upon successful completion of patching along with the offset and value data.","title":"Patch Fv User Manual"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#name","text":"PatchFv.py - The python script that patches the firmware volumes ( FV ) with in the flash device ( FD ) file post FSP build.","title":"Name"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#synopsis","text":"PatchFv FvBuildDir [FvFileBaseNames:]FdFileBaseNameToPatch [\"Offset, Value\"]+ | [\"Offset, Value, @Comment\"]+ | [\"Offset, Value, $Command\"]+ | [\"Offset, Value, $Command, @Comment\"]+","title":"Synopsis"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#description","text":"The PatchFv.py tool allows the developer to fix up FD images to follow the Intel FSP Architecture specification. It also makes the FD image relocatable. The tool is written in Python and uses Python 2.7 or later to run. Consider using the tool in a build script.","title":"Description"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#fvbuilddir-argument-1","text":"This is the first argument that PatchFv.py requires. It is the build directory for all firmware volumes created during the FSP build. The path must be either an absolute path or a relevant path, relevant to the top level of the FSP tree.","title":"FvBuildDir (Argument 1)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example-usage","text":"Build\\YouPlatformFspPkg\\%BD_TARGET%_%VS_VERSION%%VS_X86%\\FV The example used contains Windows batch script %VARIABLES%.","title":"Example usage:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#fvfilebasenames-argument-2-optional-part-1","text":"The firmware volume file base names ( FvFileBaseNames ) are the independent Fv?s that are to be patched within the FD. (0 or more in the form FVFILEBASENAME: ) The colon : is used for delimiting the single argument and must be appended to the end of each ( FvFileBaseNames ).","title":"FvFileBaseNames (Argument 2: Optional Part 1)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example-usage_1","text":"STAGE1:STAGE2:MANIFEST:YOURPLATFORM In the example STAGE1 is STAGE1.Fv in YOURPLATFORM.fd .","title":"Example usage:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#fdfilenametopatch-argument-2-mandatory-part-2","text":"Firmware device file name to patch ( FdFileNameToPatch ) is the base name of the FD file that is to be patched. (1 only, in the form YOURPLATFORM )","title":"FdFileNameToPatch (Argument 2: Mandatory Part 2)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example-usage_2","text":"STAGE1:STAGE2:MANIFEST:YOURPLATFORM In the example YOURPLATFORM is from YOURPLATFORM.fd","title":"Example usage:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#offset-value-command-comment-argument-3","text":"The Offset can be a positive or negative number and represents where the Value to be patched is located within the FD. The Value is what will be written at the given Offset in the FD. Constants may be used for both offsets and values. Also, this argument handles expressions for both offsets and values using these operators: = - * & | ~ ( ) [ ] { } < > The entire argument includes the quote marks like in the example argument below: 0xFFFFFFC0, SomeCore:__EntryPoint - [0x000000F0],@SomeCore Entry","title":"\"Offset, Value[, Command][, Comment]\" (Argument 3)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#constants","text":"Hexadecimal (use 0x as prefix) | Decimal","title":"Constants:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#examples","text":"Positive Hex Negative Hex Positive Decimal Negative Decimal 0x000000BC 0xFFFFFFA2 188 -94 ModuleName:FunctionName | ModuleName:GlobalVariableName ModuleGuid:Offset","title":"Examples:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#operators","text":"+ Addition - Subtraction * Multiplication & Logical and | Logical or ~ Complement ( ) Evaluation control [ ] Get a DWord value at the specified offset expression from [expr] { } Convert an offset {expr} into an absolute address (FSP_BASE + expr) < > Convert absolute address <expr> into an image offset (expr & FSP_SIZE)","title":"Operators:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#special-commands","text":"Special commands must use the $ symbol as a prefix to the command itself. There is only one command available at this time. $COPY ? Copy a binary block from source to destination.","title":"Special Commands:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example","text":"0x94, [PlatformInit:__gPcd_BinPatch_FvRecOffset] + 0x94, [0x98], $COPY, @Sync up 2nd FSP Header","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#comments","text":"Comments are allowed in the Offset, Value [, Comment] argument. Comments must use the @ symbol as a prefix. The comment will output to the build window upon successful completion of patching along with the offset and value data.","title":"Comments:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/","text":"SplitFspBin.py is a python script to support some operations on Intel FSP 1.x/2.x image. \u00b6 It supports: Split Intel FSP 2.x image into individual FSP-T/M/S/O component Rebase Intel FSP 1.x/2.x components to different base addresses Generate Intel FSP 1.x/2.x C header file Display Intel FSP 1.x/2.x information header for each FSP component Split Intel FSP 2.x image \u00b6 FSP 1.x image is not supported by split command. To split individual FSP component in Intel FSP 2.x image, the following command can be used: python SplitFspBin.py split [-h] -f FSPBINARY [-o OUTPUTDIR] [-n NAMETEMPLATE] For example: python SplitFspBin.py split -f FSP.bin It will create FSP_T.bin, FSP_M.bin and FSP_S.bin in current directory. Rebase Intel FSP 1.x/2.x components \u00b6 To rebase one or multiple FSP components in Intel FSP 1.x/2.x image, the following command can be used: python SplitFspBin.py rebase [-h] -f FSPBINARY -c {t,m,s,o} [{t,m,s,o} ...] -b FSPBASE [FSPBASE ...] [-o OUTPUTDIR] [-n OUTPUTFILE] For example: python SplitFspBin.py rebase -f FSP.bin -c t -b 0xFFF00000 -n FSP_new.bin It will rebase FSP-T component inside FSP.bin to new base 0xFFF00000 and save the rebased Intel FSP 2.x image into file FSP_new.bin. For FSP 1.x image there is only one component in binary so above command also works for FSP 1.x image. python SplitFspBin.py rebase -f FSP.bin -c t m -b 0xFFF00000 0xFEF80000 -n FSP_new.bin It will rebase FSP-T and FSP-M components inside FSP.bin to new base 0xFFF00000 and 0xFEF80000 respectively, and save the rebased Intel FSP 2.x image into file FSP_new.bin file. Generate Intel FSP 1.x/2.x C header file \u00b6 To generate Intel FSP 1.x/2.x C header file, the following command can be used: Python SplitFspBin.py genhdr [-h] -f FSPBINARY [-o OUTPUTDIR] [-n HFILENAME] For example: python SplitFspBin.py genhdr -f FSP.bin -n FSP.h It will create the C header file FSP.h containing the image ID, revision, offset and size for each individual FSP component. Display Intel FSP 1.x/2.x information header \u00b6 To display Intel FSP 1.x/2.x information headers, the following command can be used: Python SplitFspBin.py info [-h] -f FSPBINARY For example: python SplitFspBin.py info -f FSP.bin It will print out the FSP information header for each FSP component.","title":"Split Fsp Bin User Manual"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#splitfspbinpy-is-a-python-script-to-support-some-operations-on-intel-fsp-1x2x-image","text":"It supports: Split Intel FSP 2.x image into individual FSP-T/M/S/O component Rebase Intel FSP 1.x/2.x components to different base addresses Generate Intel FSP 1.x/2.x C header file Display Intel FSP 1.x/2.x information header for each FSP component","title":"SplitFspBin.py is a python script to support some operations on Intel FSP 1.x/2.x image."},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#split-intel-fsp-2x-image","text":"FSP 1.x image is not supported by split command. To split individual FSP component in Intel FSP 2.x image, the following command can be used: python SplitFspBin.py split [-h] -f FSPBINARY [-o OUTPUTDIR] [-n NAMETEMPLATE] For example: python SplitFspBin.py split -f FSP.bin It will create FSP_T.bin, FSP_M.bin and FSP_S.bin in current directory.","title":"Split Intel FSP 2.x image"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#rebase-intel-fsp-1x2x-components","text":"To rebase one or multiple FSP components in Intel FSP 1.x/2.x image, the following command can be used: python SplitFspBin.py rebase [-h] -f FSPBINARY -c {t,m,s,o} [{t,m,s,o} ...] -b FSPBASE [FSPBASE ...] [-o OUTPUTDIR] [-n OUTPUTFILE] For example: python SplitFspBin.py rebase -f FSP.bin -c t -b 0xFFF00000 -n FSP_new.bin It will rebase FSP-T component inside FSP.bin to new base 0xFFF00000 and save the rebased Intel FSP 2.x image into file FSP_new.bin. For FSP 1.x image there is only one component in binary so above command also works for FSP 1.x image. python SplitFspBin.py rebase -f FSP.bin -c t m -b 0xFFF00000 0xFEF80000 -n FSP_new.bin It will rebase FSP-T and FSP-M components inside FSP.bin to new base 0xFFF00000 and 0xFEF80000 respectively, and save the rebased Intel FSP 2.x image into file FSP_new.bin file.","title":"Rebase Intel FSP 1.x/2.x components"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#generate-intel-fsp-1x2x-c-header-file","text":"To generate Intel FSP 1.x/2.x C header file, the following command can be used: Python SplitFspBin.py genhdr [-h] -f FSPBINARY [-o OUTPUTDIR] [-n HFILENAME] For example: python SplitFspBin.py genhdr -f FSP.bin -n FSP.h It will create the C header file FSP.h containing the image ID, revision, offset and size for each individual FSP component.","title":"Generate Intel FSP 1.x/2.x C header file"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#display-intel-fsp-1x2x-information-header","text":"To display Intel FSP 1.x/2.x information headers, the following command can be used: Python SplitFspBin.py info [-h] -f FSPBINARY For example: python SplitFspBin.py info -f FSP.bin It will print out the FSP information header for each FSP component.","title":"Display Intel FSP 1.x/2.x information header"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2WrapperPkg/Readme/","text":"IntelFsp2WrapperPkg \u00b6 This package provides the component to use an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2WrapperPkg A whitepaper to describe the IntelFsp2WrapperPkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Using_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"Modules"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2WrapperPkg/Readme/#intelfsp2wrapperpkg","text":"This package provides the component to use an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2WrapperPkg A whitepaper to describe the IntelFsp2WrapperPkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Using_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"IntelFsp2WrapperPkg"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsulePdb/Readme/","text":"How to generate Microcode FMP from Microcode PDB file \u00b6 1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdatePdb to <Your Platform Package>/MicrocodeUpdatePdb . 2) Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.fdf with path to a Microcode PDB file. The PDB file can placed in <Your Platform Package>/MicrocodeUpdatePdb or any other path. FILE DATA = <your Microcode PDB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> 3) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.dsc 4) The Microcode FMP Capsule is generated at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"Microcode Capsule Pdb"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsulePdb/Readme/#how-to-generate-microcode-fmp-from-microcode-pdb-file","text":"1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdatePdb to <Your Platform Package>/MicrocodeUpdatePdb . 2) Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.fdf with path to a Microcode PDB file. The PDB file can placed in <Your Platform Package>/MicrocodeUpdatePdb or any other path. FILE DATA = <your Microcode PDB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> 3) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.dsc 4) The Microcode FMP Capsule is generated at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"How to generate Microcode FMP from Microcode PDB file"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsuleTxt/Readme/","text":"How to generate Microcode FMP from Microcode TXT file \u00b6 1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdateTxt to <Your Platform Package>/MicrocodeUpdateTxt 2) Copy microcode TXT file to <Your Platform Package>/MicrocodeUpdateTxt/Microcode 3) Uncomment and update statement in [Sources] section of <Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode.inf with name of Microcode TXT file copied in previous step. [Sources] <Your Microcode TXT file> Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.fdf with path to a Microcode MCB file. The MCB file is placed in $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/IA32/<Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode/OUTPUT/ . FILE DATA = <your Microcode MCB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> Uncomment and update statement in Components section of <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with path to a Microcode INF file. [Components] <Your Microcode INF file> 4) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.dsc 5) The generated Microcode FMP Capsule is found at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"Microcode Capsule Txt"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsuleTxt/Readme/#how-to-generate-microcode-fmp-from-microcode-txt-file","text":"1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdateTxt to <Your Platform Package>/MicrocodeUpdateTxt 2) Copy microcode TXT file to <Your Platform Package>/MicrocodeUpdateTxt/Microcode 3) Uncomment and update statement in [Sources] section of <Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode.inf with name of Microcode TXT file copied in previous step. [Sources] <Your Microcode TXT file> Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.fdf with path to a Microcode MCB file. The MCB file is placed in $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/IA32/<Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode/OUTPUT/ . FILE DATA = <your Microcode MCB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> Uncomment and update statement in Components section of <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with path to a Microcode INF file. [Components] <Your Microcode INF file> 4) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.dsc 5) The generated Microcode FMP Capsule is found at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"How to generate Microcode FMP from Microcode TXT file"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/","text":"Azure DevOps Pipelines \u00b6 These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution. Core CI \u00b6 Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details Platform CI \u00b6 Focused on building a single target platform and confirming functionality on that platform. Conventions \u00b6 Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml Links \u00b6 Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library Lessons Learned \u00b6 Templates and parameters \u00b6 They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late. File matching patterns \u00b6 On Linux this can hang if there are too many files in the search list. Templates and file splitting \u00b6 Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs. Conditional steps \u00b6 If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Read Me"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#azure-devops-pipelines","text":"These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution.","title":"Azure DevOps Pipelines"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#core-ci","text":"Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details","title":"Core CI"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#platform-ci","text":"Focused on building a single target platform and confirming functionality on that platform.","title":"Platform CI"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#conventions","text":"Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml","title":"Conventions"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#links","text":"Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library","title":"Links"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#lessons-learned","text":"","title":"Lessons Learned"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#templates-and-parameters","text":"They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late.","title":"Templates and parameters"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#file-matching-patterns","text":"On Linux this can hang if there are too many files in the search list.","title":"File matching patterns"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#templates-and-file-splitting","text":"Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs.","title":"Templates and file splitting"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/ReadMe/#conditional-steps","text":"If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Conditional steps"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/","text":"CI Templates \u00b6 This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation. Common CI templates \u00b6 basetools-build-steps.yml \u00b6 This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds. Core CI templates \u00b6 pr-gate-build-job.yml \u00b6 This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI. pr-gate-steps.yml \u00b6 This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\" spell-check-prereq-steps.yml \u00b6 This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. Platform CI templates \u00b6 platform-build-run-steps.yml \u00b6 This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools Special Notes \u00b6 For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"templates"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#ci-templates","text":"This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation.","title":"CI Templates"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#common-ci-templates","text":"","title":"Common CI templates"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#basetools-build-stepsyml","text":"This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds.","title":"basetools-build-steps.yml"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#core-ci-templates","text":"","title":"Core CI templates"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#pr-gate-build-jobyml","text":"This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI.","title":"pr-gate-build-job.yml"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#pr-gate-stepsyml","text":"This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\"","title":"pr-gate-steps.yml"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#spell-check-prereq-stepsyml","text":"This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0.","title":"spell-check-prereq-steps.yml"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#platform-ci-templates","text":"","title":"Platform CI templates"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#platform-build-run-stepsyml","text":"This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools","title":"platform-build-run-steps.yml"},{"location":"dyn/mu_silicon_intel_tiano/azurepipelines/templates/ReadMe/#special-notes","text":"For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"Special Notes"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues IntelFsp2Pkg IntelFsp2WrapperPkg IntelSiliconPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment Host Module Inclusion Test - HostUnitTestDscCompleteCheck \u00b6 This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin \u00b6 A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way... Including Host-Based Tests in the Package YAML \u00b6 For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided. Configuring the HostBased DSC \u00b6 The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"pytool"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues IntelFsp2Pkg IntelFsp2WrapperPkg IntelSiliconPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#host-module-inclusion-test-hostunittestdsccompletecheck","text":"This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment","title":"Host Module Inclusion Test - HostUnitTestDscCompleteCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#host-unit-test-compilation-and-run-test-hostunittestcompilerplugin","text":"A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way...","title":"Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#including-host-based-tests-in-the-package-yaml","text":"For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided.","title":"Including Host-Based Tests in the Package YAML"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#configuring-the-hostbased-dsc","text":"The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION.","title":"Configuring the HostBased DSC"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows","title":"PyTool Scopes"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"Future investments"},{"location":"dyn/mu_tiano_plus/RepoDetails/","text":"Project Mu Tiano Plus \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_tiano_plus.git Branch: release/202005 Commit: d09835f36700a7f76f839a669fb707700f665b37 Commit Date: 2020-07-30 17:22:01 +0000 About \u00b6 This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. The modules in this repo were taken with minimal modification from TianoCore. For full documentation, please see the Project Mu Docs site . This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_tiano_plus/RepoDetails/#project-mu-tiano-plus","text":"Git Details Repository Url: https://github.com/Microsoft/mu_tiano_plus.git Branch: release/202005 Commit: d09835f36700a7f76f839a669fb707700f665b37 Commit Date: 2020-07-30 17:22:01 +0000","title":"Project Mu Tiano Plus"},{"location":"dyn/mu_tiano_plus/RepoDetails/#about","text":"This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. The modules in this repo were taken with minimal modification from TianoCore. For full documentation, please see the Project Mu Docs site . This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"About"},{"location":"dyn/mu_tiano_plus/RepoDetails/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_tiano_plus/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/","text":"Crypto Driver \u00b6 This is a potentially prepacked version of the BaseCryptLib and TlsLib, delivered via protocol. There are two routes: using the pre-compiled version and compiling it into your platform. Benefits \u00b6 But first, why would you care about this? It has a few benefits, namely: Smaller Binary sizes Easier to service/upgrade Transparency on what version of crypto you're using Reduced build times (if using pre-compiled version) There are different flavors of Crypto available, with different functions supported. Don't need to use HMAC in your PEI phase? Select a service level or flavor that doesn't include HMAC in your platform. How include on your platform \u00b6 Now there are a few options for you. We'll start with the pre-compiled route. The Pre-compiled (easy) way \u00b6 The easy way involves setting a few variables and a few includes. The hard way is just to do it yourself. First the easy way: Define the service level that you want for each phase of UEFI in the defines section of your DSC. [Defines] DEFINE PEI_CRYPTO_SERVICES = TINY_SHA DEFINE DXE_CRYPTO_SERVICES = STANDARD DEFINE SMM_CRYPTO_SERVICES = STANDARD DEFINE PEI_CRYPTO_ARCH = IA32 DEFINE DXE_CRYPTO_ARCH = X64 DEFINE SMM_CRYPTO_ARCH = X64 The above example is for a standard intel platform, and the service levels or flavors available. Add the DSC include !include CryptoPkg/Driver/Bin/CryptoDriver.inc.dsc This sets the definitions for BaseCryptLib as well as includes the correct flavor level of the component you wish to use. Add the FDF includes to your platform FDF Currently, it isn't possible in an FDF to redefine a FV section and have them be combined. There are two includes: BOOTBLOCK and DXE. The first includes the PEI phase and is meant to be stuck in your BOOTBLOCK FV. The second contains the DXE and SMM modules and is meant to be stuck in your FVDXE. [FV.FVBOOTBLOCK] ... !include CryptoPkg/Driver/Bin/CryptoDriver.BOOTBLOCK.inc.fdf ... [FV.FVDXE] ... !include CryptoPkg/Driver/Bin/CryptoDriver.BOOTBLOCK.inc.fdf Recommendations \u00b6 It is highly recommended to put this logic behind conditionals like so: [FV.FVBOOTBLOCK] !if $(ENABLE_SHARED_CRYPTO) == TRUE !include CryptoPkg/Driver/Bin/CryptoDriver.BOOTBLOCK.inc.fdf !endif This allows developers on the platform to use their own BaseCryptLib or TlsLib if they want. Just add a check if it's not defined in your DSC like so. !ifndef ENABLE_SHARED_CRYPTO # by default true ENABLE_SHARED_CRYPTO = TRUE !endif The DIY way \u00b6 If you want to take advantage of the BaseCryptOnProtocol but don't want to use a pre-compiled method, you can compile it within your platform itself. Shown here is for an intel platform, adjust the architectures as needed. [LibraryClasses.IA32] BaseCryptLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/PeiCryptLib.inf TlsLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/PeiCryptLib.inf [LibraryClasses.X64] BaseCryptLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/DxeCryptLib.inf TlsLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/DxeCryptLib.inf [LibraryClasses.X64.DXE_SMM_DRIVER] BaseCryptLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/SmmCryptLib.inf TlsLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/SmmCryptLib.inf [Components.IA32] CryptoPkg/Driver/CryptoPei.inf { <LibraryClasses> BaseCryptLib|CryptoPkg/Library/BaseCryptLib/PeiCryptLib.inf TlsLib|CryptoPkg/Library/TlsLibNull/TlsLibNull.inf <PcdsFixedAtBuild> .. All the flavor PCDs here .. } [Components.X64] CryptoPkg/Driver/CryptoDxe.inf { <LibraryClasses> BaseCryptLib|CryptoPkg/Library/BaseCryptLib/BaseCryptLib.inf TlsLib|CryptoPkg/Library/TlsLib/TlsLib.inf <PcdsFixedAtBuild> .. All the flavor PCDs here .. } CryptoPkg/Driver/CryptoSmm.inf { <LibraryClasses> BaseCryptLib|CryptoPkg/Library/BaseCryptLib/SmmCryptLib.inf TlsLib|CryptoPkg/Library/TlsLibNull/TlsLibNull.inf <PcdsFixedAtBuild> .. All the flavor PCDs here .. } The PCDs are long and default to all false. The flavors are stored as .inc.dsc files at CryptoPkg\\Driver\\Packaging . An example would be CryptoPkg\\Driver\\Packaging\\Crypto.pcd.TINY_SHA.inc.dsc which is a flavor that just has Sha1, Sha256, and Sha386. You'll need to include these components in your FDF as well. [FV.FVBOOTBLOCK] INF CryptoPkg/Driver/CryptoPei.inf [FV.FVDXE] INF CryptoPkg/Driver/CryptoSmm.inf INF CryptoPkg/Driver/CryptoDxe.inf","title":"Driver"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/#crypto-driver","text":"This is a potentially prepacked version of the BaseCryptLib and TlsLib, delivered via protocol. There are two routes: using the pre-compiled version and compiling it into your platform.","title":"Crypto Driver"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/#benefits","text":"But first, why would you care about this? It has a few benefits, namely: Smaller Binary sizes Easier to service/upgrade Transparency on what version of crypto you're using Reduced build times (if using pre-compiled version) There are different flavors of Crypto available, with different functions supported. Don't need to use HMAC in your PEI phase? Select a service level or flavor that doesn't include HMAC in your platform.","title":"Benefits"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/#how-include-on-your-platform","text":"Now there are a few options for you. We'll start with the pre-compiled route.","title":"How include on your platform"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/#the-pre-compiled-easy-way","text":"The easy way involves setting a few variables and a few includes. The hard way is just to do it yourself. First the easy way: Define the service level that you want for each phase of UEFI in the defines section of your DSC. [Defines] DEFINE PEI_CRYPTO_SERVICES = TINY_SHA DEFINE DXE_CRYPTO_SERVICES = STANDARD DEFINE SMM_CRYPTO_SERVICES = STANDARD DEFINE PEI_CRYPTO_ARCH = IA32 DEFINE DXE_CRYPTO_ARCH = X64 DEFINE SMM_CRYPTO_ARCH = X64 The above example is for a standard intel platform, and the service levels or flavors available. Add the DSC include !include CryptoPkg/Driver/Bin/CryptoDriver.inc.dsc This sets the definitions for BaseCryptLib as well as includes the correct flavor level of the component you wish to use. Add the FDF includes to your platform FDF Currently, it isn't possible in an FDF to redefine a FV section and have them be combined. There are two includes: BOOTBLOCK and DXE. The first includes the PEI phase and is meant to be stuck in your BOOTBLOCK FV. The second contains the DXE and SMM modules and is meant to be stuck in your FVDXE. [FV.FVBOOTBLOCK] ... !include CryptoPkg/Driver/Bin/CryptoDriver.BOOTBLOCK.inc.fdf ... [FV.FVDXE] ... !include CryptoPkg/Driver/Bin/CryptoDriver.BOOTBLOCK.inc.fdf","title":"The Pre-compiled (easy) way"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/#recommendations","text":"It is highly recommended to put this logic behind conditionals like so: [FV.FVBOOTBLOCK] !if $(ENABLE_SHARED_CRYPTO) == TRUE !include CryptoPkg/Driver/Bin/CryptoDriver.BOOTBLOCK.inc.fdf !endif This allows developers on the platform to use their own BaseCryptLib or TlsLib if they want. Just add a check if it's not defined in your DSC like so. !ifndef ENABLE_SHARED_CRYPTO # by default true ENABLE_SHARED_CRYPTO = TRUE !endif","title":"Recommendations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Driver/readme/#the-diy-way","text":"If you want to take advantage of the BaseCryptOnProtocol but don't want to use a pre-compiled method, you can compile it within your platform itself. Shown here is for an intel platform, adjust the architectures as needed. [LibraryClasses.IA32] BaseCryptLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/PeiCryptLib.inf TlsLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/PeiCryptLib.inf [LibraryClasses.X64] BaseCryptLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/DxeCryptLib.inf TlsLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/DxeCryptLib.inf [LibraryClasses.X64.DXE_SMM_DRIVER] BaseCryptLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/SmmCryptLib.inf TlsLib|CryptoPkg/Library/BaseCryptLibOnProtocolPpi/SmmCryptLib.inf [Components.IA32] CryptoPkg/Driver/CryptoPei.inf { <LibraryClasses> BaseCryptLib|CryptoPkg/Library/BaseCryptLib/PeiCryptLib.inf TlsLib|CryptoPkg/Library/TlsLibNull/TlsLibNull.inf <PcdsFixedAtBuild> .. All the flavor PCDs here .. } [Components.X64] CryptoPkg/Driver/CryptoDxe.inf { <LibraryClasses> BaseCryptLib|CryptoPkg/Library/BaseCryptLib/BaseCryptLib.inf TlsLib|CryptoPkg/Library/TlsLib/TlsLib.inf <PcdsFixedAtBuild> .. All the flavor PCDs here .. } CryptoPkg/Driver/CryptoSmm.inf { <LibraryClasses> BaseCryptLib|CryptoPkg/Library/BaseCryptLib/SmmCryptLib.inf TlsLib|CryptoPkg/Library/TlsLibNull/TlsLibNull.inf <PcdsFixedAtBuild> .. All the flavor PCDs here .. } The PCDs are long and default to all false. The flavors are stored as .inc.dsc files at CryptoPkg\\Driver\\Packaging . An example would be CryptoPkg\\Driver\\Packaging\\Crypto.pcd.TINY_SHA.inc.dsc which is a flavor that just has Sha1, Sha256, and Sha386. You'll need to include these components in your FDF as well. [FV.FVBOOTBLOCK] INF CryptoPkg/Driver/CryptoPei.inf [FV.FVDXE] INF CryptoPkg/Driver/CryptoSmm.inf INF CryptoPkg/Driver/CryptoDxe.inf","title":"The DIY way"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/BaseCryptLibOnProtocolPpi/readme/","text":"BaseCryptLibOnProtocolPpi \u00b6 This is an implementation of Basecryptlib that uses a protocol. This makes binaries smaller, make servicing easier, and allows crypto to be pre-compiled. See CryptoPkg/Driver/readme.md for more information.","title":"Base Crypt Lib On Protocol Ppi"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/BaseCryptLibOnProtocolPpi/readme/#basecryptlibonprotocolppi","text":"This is an implementation of Basecryptlib that uses a protocol. This makes binaries smaller, make servicing easier, and allows crypto to be pre-compiled. See CryptoPkg/Driver/readme.md for more information.","title":"BaseCryptLibOnProtocolPpi"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/","text":"BoringSSL \u00b6 BoringSSL is a fork of OpenSSL that is designed to meet Google's needs. Although BoringSSL is an open source project, it is not intended for general use, as OpenSSL is. We don't recommend that third parties depend upon it. Doing so is likely to be frustrating because there are no guarantees of API or ABI stability. Programs ship their own copies of BoringSSL when they use it and we update everything as needed when deciding to make API changes. This allows us to mostly avoid compromises in the name of compatibility. It works for us, but it may not work for you. BoringSSL arose because Google used OpenSSL for many years in various ways and, over time, built up a large number of patches that were maintained while tracking upstream OpenSSL. As Google's product portfolio became more complex, more copies of OpenSSL sprung up and the effort involved in maintaining all these patches in multiple places was growing steadily. Currently BoringSSL is the SSL library in Chrome/Chromium, Android (but it's not part of the NDK) and a number of other apps/programs. There are other files in this directory which might be helpful: PORTING.md : how to port OpenSSL-using code to BoringSSL. BUILDING.md : how to build BoringSSL INCORPORATING.md : how to incorporate BoringSSL into a project. API-CONVENTIONS.md : general API conventions for BoringSSL consumers and developers. STYLE.md : rules and guidelines for coding style. include/openssl: public headers with API documentation in comments. Also available online . FUZZING.md : information about fuzzing BoringSSL. CONTRIBUTING.md : how to contribute to BoringSSL.","title":"README"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/#boringssl","text":"BoringSSL is a fork of OpenSSL that is designed to meet Google's needs. Although BoringSSL is an open source project, it is not intended for general use, as OpenSSL is. We don't recommend that third parties depend upon it. Doing so is likely to be frustrating because there are no guarantees of API or ABI stability. Programs ship their own copies of BoringSSL when they use it and we update everything as needed when deciding to make API changes. This allows us to mostly avoid compromises in the name of compatibility. It works for us, but it may not work for you. BoringSSL arose because Google used OpenSSL for many years in various ways and, over time, built up a large number of patches that were maintained while tracking upstream OpenSSL. As Google's product portfolio became more complex, more copies of OpenSSL sprung up and the effort involved in maintaining all these patches in multiple places was growing steadily. Currently BoringSSL is the SSL library in Chrome/Chromium, Android (but it's not part of the NDK) and a number of other apps/programs. There are other files in this directory which might be helpful: PORTING.md : how to port OpenSSL-using code to BoringSSL. BUILDING.md : how to build BoringSSL INCORPORATING.md : how to incorporate BoringSSL into a project. API-CONVENTIONS.md : general API conventions for BoringSSL consumers and developers. STYLE.md : rules and guidelines for coding style. include/openssl: public headers with API documentation in comments. Also available online . FUZZING.md : information about fuzzing BoringSSL. CONTRIBUTING.md : how to contribute to BoringSSL.","title":"BoringSSL"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/","text":"BoringSSL API Conventions \u00b6 This document describes conventions for BoringSSL APIs. The style guide also includes guidelines, but this document is targeted at both API consumers and developers. Documentation \u00b6 All supported public APIs are documented in the public header files, found in include/openssl . The API documentation is also available online . Some headers lack documention comments. These are functions and structures from OpenSSL's legacy ASN.1, X.509, and PEM implementation. If possible, avoid using them. These are left largely unmodified from upstream and are retained only for compatibility with existing OpenSSL consumers. Forward declarations \u00b6 Do not write typedef struct foo_st FOO or try otherwise to define BoringSSL's types. Including openssl/base.h (or openssl/ossl_typ.h for consumers who wish to be OpenSSL-compatible) will forward-declare each type without importing the rest of the library or invasive macros. Error-handling \u00b6 Most functions in BoringSSL may fail, either due to allocation failures or input errors. Functions which return an int typically return one on success and zero on failure. Functions which return a pointer typically return NULL on failure. However, due to legacy constraints, some functions are more complex. Consult the API documentation before using a function. On error, most functions also push errors on the error queue, an errno -like mechanism. See the documentation for err.h for more details. As with errno , callers must test the function's return value, not the error queue to determine whether an operation failed. Some codepaths may not interact with the error queue, and the error queue may have state from a previous failed operation. When ignoring a failed operation, it is recommended to call ERR_clear_error to avoid the state interacting with future operations. Failing to do so should not affect the actual behavior of any functions, but may result in errors from both operations being mixed in error logging. We hope to improve this situation in the future. Where possible, avoid conditioning on specific reason codes and limit usage to logging. The reason codes are very specific and may change over time. Memory allocation \u00b6 BoringSSL allocates memory via OPENSSL_malloc , found in mem.h . Use OPENSSL_free , found in the same header file, to release it. BoringSSL functions will fail gracefully on allocation error, but it is recommended to use a malloc implementation that abort s on failure. Object initialization and cleanup \u00b6 BoringSSL defines a number of structs for use in its APIs. It is a C library, so the caller is responsible for ensuring these structs are properly initialized and released. Consult the documentation for a module for the proper use of its types. Some general conventions are listed below. Heap-allocated types \u00b6 Some types, such as RSA , are heap-allocated. All instances will be allocated and returned from BoringSSL's APIs. It is an error to instantiate a heap- allocated type on the stack or embedded within another object. Heap-allocated types may have functioned named like RSA_new which allocates a fresh blank RSA . Other functions may also return newly-allocated instances. For example, RSA_parse_public_key is documented to return a newly-allocated RSA object. Heap-allocated objects must be released by the corresponding free function, named like RSA_free . Like C's free and C++'s delete , all free functions internally check for NULL . Consumers are not required to check for NULL before calling. A heap-allocated type may be reference-counted. In this case, a function named like RSA_up_ref will be available to take an additional reference count. The free function must be called to decrement the reference count. It will only release resources when the final reference is released. For OpenSSL compatibility, these functions return int , but callers may assume they always successfully return one because reference counts use saturating arithmetic. C++ consumers are recommended to use bssl::UniquePtr to manage heap-allocated objects. bssl::UniquePtr<T> , like other types, is forward-declared in openssl/base.h . Code that needs access to the free functions, such as code which destroys a bssl::UniquePtr , must include the corresponding module's header. (This matches std::unique_ptr 's relationship with forward declarations.) Stack-allocated types \u00b6 Other types in BoringSSL are stack-allocated, such as EVP_MD_CTX . These types may be allocated on the stack or embedded within another object. However, they must still be initialized before use. Every stack-allocated object in BoringSSL has a zero state , analogous to initializing a pointer to NULL . In this state, the object may not be completely initialized, but it is safe to call cleanup functions. Entering the zero state cannot fail. (It is usually memset(0) .) The function to enter the zero state is named like EVP_MD_CTX_init or CBB_zero and will always return void . To release resources associated with the type, call the cleanup function, named like EVP_MD_CTX_cleanup . The cleanup function must be called on all codepaths, regardless of success or failure. For example: uint8_t md [ EVP_MAX_MD_SIZE ] ; unsigned md_len ; EVP_MD_CTX ctx ; EVP_MD_CTX_init ( & ctx ); /* Enter the zero state. */ int ok = EVP_DigestInit_ex ( & ctx , EVP_sha256 (), NULL ) && EVP_DigestUpdate ( & ctx , \"hello \" , 6 ) && EVP_DigestUpdate ( & ctx , \"world\" , 5 ) && EVP_DigestFinal_ex ( & ctx , md , & md_len ); EVP_MD_CTX_cleanup ( & ctx ); /* Release |ctx|. */ Note that EVP_MD_CTX_cleanup is called whether or not the EVP_Digest* operations succeeded. More complex C functions may use the goto err pattern: int ret = 0 ; EVP_MD_CTX ctx ; EVP_MD_CTX_init ( & ctx ); if ( ! some_other_operation ()) { goto err ; } uint8_t md [ EVP_MAX_MD_SIZE ] ; unsigned md_len ; if ( ! EVP_DigestInit_ex ( & ctx , EVP_sha256 (), NULL ) || ! EVP_DigestUpdate ( & ctx , \"hello \" , 6 ) || ! EVP_DigestUpdate ( & ctx , \"world\" , 5 ) || ! EVP_DigestFinal_ex ( & ctx , md , & md_len ) { goto err ; } ret = 1 ; err : EVP_MD_CTX_cleanup ( & ctx ); return ret ; Note that, because ctx is set to the zero state before any failures, EVP_MD_CTX_cleanup is safe to call even if the first operation fails before EVP_DigestInit_ex . However, it would be illegal to move the EVP_MD_CTX_init below the some_other_operation call. As a rule of thumb, enter the zero state of stack-allocated structs in the same place they are declared. C++ consumers are recommended to use the wrappers named like bssl::ScopedEVP_MD_CTX , defined in the corresponding module's header. These wrappers are automatically initialized to the zero state and are automatically cleaned up. Data-only types \u00b6 A few types, such as SHA_CTX , are data-only types and do not require cleanup. These are usually for low-level cryptographic operations. These types may be used freely without special cleanup conventions. Thread safety \u00b6 BoringSSL is internally aware of the platform threading library and calls into it as needed. Consult the API documentation for the threading guarantees of particular objects. In general, stateless reference-counted objects like RSA or EVP_PKEY which represent keys may typically be used from multiple threads simultaneously, provided no thread mutates the key.","title":"API-CONVENTIONS"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#boringssl-api-conventions","text":"This document describes conventions for BoringSSL APIs. The style guide also includes guidelines, but this document is targeted at both API consumers and developers.","title":"BoringSSL API Conventions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#documentation","text":"All supported public APIs are documented in the public header files, found in include/openssl . The API documentation is also available online . Some headers lack documention comments. These are functions and structures from OpenSSL's legacy ASN.1, X.509, and PEM implementation. If possible, avoid using them. These are left largely unmodified from upstream and are retained only for compatibility with existing OpenSSL consumers.","title":"Documentation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#forward-declarations","text":"Do not write typedef struct foo_st FOO or try otherwise to define BoringSSL's types. Including openssl/base.h (or openssl/ossl_typ.h for consumers who wish to be OpenSSL-compatible) will forward-declare each type without importing the rest of the library or invasive macros.","title":"Forward declarations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#error-handling","text":"Most functions in BoringSSL may fail, either due to allocation failures or input errors. Functions which return an int typically return one on success and zero on failure. Functions which return a pointer typically return NULL on failure. However, due to legacy constraints, some functions are more complex. Consult the API documentation before using a function. On error, most functions also push errors on the error queue, an errno -like mechanism. See the documentation for err.h for more details. As with errno , callers must test the function's return value, not the error queue to determine whether an operation failed. Some codepaths may not interact with the error queue, and the error queue may have state from a previous failed operation. When ignoring a failed operation, it is recommended to call ERR_clear_error to avoid the state interacting with future operations. Failing to do so should not affect the actual behavior of any functions, but may result in errors from both operations being mixed in error logging. We hope to improve this situation in the future. Where possible, avoid conditioning on specific reason codes and limit usage to logging. The reason codes are very specific and may change over time.","title":"Error-handling"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#memory-allocation","text":"BoringSSL allocates memory via OPENSSL_malloc , found in mem.h . Use OPENSSL_free , found in the same header file, to release it. BoringSSL functions will fail gracefully on allocation error, but it is recommended to use a malloc implementation that abort s on failure.","title":"Memory allocation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#object-initialization-and-cleanup","text":"BoringSSL defines a number of structs for use in its APIs. It is a C library, so the caller is responsible for ensuring these structs are properly initialized and released. Consult the documentation for a module for the proper use of its types. Some general conventions are listed below.","title":"Object initialization and cleanup"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#heap-allocated-types","text":"Some types, such as RSA , are heap-allocated. All instances will be allocated and returned from BoringSSL's APIs. It is an error to instantiate a heap- allocated type on the stack or embedded within another object. Heap-allocated types may have functioned named like RSA_new which allocates a fresh blank RSA . Other functions may also return newly-allocated instances. For example, RSA_parse_public_key is documented to return a newly-allocated RSA object. Heap-allocated objects must be released by the corresponding free function, named like RSA_free . Like C's free and C++'s delete , all free functions internally check for NULL . Consumers are not required to check for NULL before calling. A heap-allocated type may be reference-counted. In this case, a function named like RSA_up_ref will be available to take an additional reference count. The free function must be called to decrement the reference count. It will only release resources when the final reference is released. For OpenSSL compatibility, these functions return int , but callers may assume they always successfully return one because reference counts use saturating arithmetic. C++ consumers are recommended to use bssl::UniquePtr to manage heap-allocated objects. bssl::UniquePtr<T> , like other types, is forward-declared in openssl/base.h . Code that needs access to the free functions, such as code which destroys a bssl::UniquePtr , must include the corresponding module's header. (This matches std::unique_ptr 's relationship with forward declarations.)","title":"Heap-allocated types"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#stack-allocated-types","text":"Other types in BoringSSL are stack-allocated, such as EVP_MD_CTX . These types may be allocated on the stack or embedded within another object. However, they must still be initialized before use. Every stack-allocated object in BoringSSL has a zero state , analogous to initializing a pointer to NULL . In this state, the object may not be completely initialized, but it is safe to call cleanup functions. Entering the zero state cannot fail. (It is usually memset(0) .) The function to enter the zero state is named like EVP_MD_CTX_init or CBB_zero and will always return void . To release resources associated with the type, call the cleanup function, named like EVP_MD_CTX_cleanup . The cleanup function must be called on all codepaths, regardless of success or failure. For example: uint8_t md [ EVP_MAX_MD_SIZE ] ; unsigned md_len ; EVP_MD_CTX ctx ; EVP_MD_CTX_init ( & ctx ); /* Enter the zero state. */ int ok = EVP_DigestInit_ex ( & ctx , EVP_sha256 (), NULL ) && EVP_DigestUpdate ( & ctx , \"hello \" , 6 ) && EVP_DigestUpdate ( & ctx , \"world\" , 5 ) && EVP_DigestFinal_ex ( & ctx , md , & md_len ); EVP_MD_CTX_cleanup ( & ctx ); /* Release |ctx|. */ Note that EVP_MD_CTX_cleanup is called whether or not the EVP_Digest* operations succeeded. More complex C functions may use the goto err pattern: int ret = 0 ; EVP_MD_CTX ctx ; EVP_MD_CTX_init ( & ctx ); if ( ! some_other_operation ()) { goto err ; } uint8_t md [ EVP_MAX_MD_SIZE ] ; unsigned md_len ; if ( ! EVP_DigestInit_ex ( & ctx , EVP_sha256 (), NULL ) || ! EVP_DigestUpdate ( & ctx , \"hello \" , 6 ) || ! EVP_DigestUpdate ( & ctx , \"world\" , 5 ) || ! EVP_DigestFinal_ex ( & ctx , md , & md_len ) { goto err ; } ret = 1 ; err : EVP_MD_CTX_cleanup ( & ctx ); return ret ; Note that, because ctx is set to the zero state before any failures, EVP_MD_CTX_cleanup is safe to call even if the first operation fails before EVP_DigestInit_ex . However, it would be illegal to move the EVP_MD_CTX_init below the some_other_operation call. As a rule of thumb, enter the zero state of stack-allocated structs in the same place they are declared. C++ consumers are recommended to use the wrappers named like bssl::ScopedEVP_MD_CTX , defined in the corresponding module's header. These wrappers are automatically initialized to the zero state and are automatically cleaned up.","title":"Stack-allocated types"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#data-only-types","text":"A few types, such as SHA_CTX , are data-only types and do not require cleanup. These are usually for low-level cryptographic operations. These types may be used freely without special cleanup conventions.","title":"Data-only types"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/API-CONVENTIONS/#thread-safety","text":"BoringSSL is internally aware of the platform threading library and calls into it as needed. Consult the API documentation for the threading guarantees of particular objects. In general, stateless reference-counted objects like RSA or EVP_PKEY which represent keys may typically be used from multiple threads simultaneously, provided no thread mutates the key.","title":"Thread safety"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/","text":"Building BoringSSL \u00b6 Build Prerequisites \u00b6 CMake 2.8.11 or later is required. Perl 5.6.1 or later is required. On Windows, Active State Perl has been reported to work, as has MSYS Perl. Strawberry Perl also works but it adds GCC to PATH , which can confuse some build tools when identifying the compiler (removing C:\\Strawberry\\c\\bin from PATH should resolve any problems). If Perl is not found by CMake, it may be configured explicitly by setting PERL_EXECUTABLE . On Windows you currently must use Ninja to build; on other platforms, it is not required, but recommended, because it makes builds faster. If you need to build Ninja from source, then a recent version of Python is required (Python 2.7.5 works). On Windows only, Yasm is required. If not found by CMake, it may be configured explicitly by setting CMAKE_ASM_NASM_COMPILER . A C compiler is required. On Windows, MSVC 14 (Visual Studio 2015) or later with Platform SDK 8.1 or later are supported. Recent versions of GCC (4.8+) and Clang should work on non-Windows platforms, and maybe on Windows too. To build the tests, you also need a C++ compiler with C++11 support. Go is required. If not found by CMake, the go executable may be configured explicitly by setting GO_EXECUTABLE . To build the x86 and x86_64 assembly, your assembler must support AVX2 instructions and MOVBE. If using GNU binutils, you must have 2.22 or later. Building \u00b6 Using Ninja (note the 'N' is capitalized in the cmake invocation): mkdir build cd build cmake -GNinja .. ninja Using Make (does not work on Windows): mkdir build cd build cmake .. make You usually don't need to run cmake again after changing CMakeLists.txt files because the build scripts will detect changes to them and rebuild themselves automatically. Note that the default build flags in the top-level CMakeLists.txt are for debugging\u2014optimisation isn't enabled. Pass -DCMAKE_BUILD_TYPE=Release to cmake to configure a release build. If you want to cross-compile then there is an example toolchain file for 32-bit Intel in util/ . Wipe out the build directory, recreate it and run cmake like this: cmake -DCMAKE_TOOLCHAIN_FILE=../util/32-bit-toolchain.cmake -GNinja .. If you want to build as a shared library, pass -DBUILD_SHARED_LIBS=1 . On Windows, where functions need to be tagged with dllimport when coming from a shared library, define BORINGSSL_SHARED_LIBRARY in any code which #include s the BoringSSL headers. In order to serve environments where code-size is important as well as those where performance is the overriding concern, OPENSSL_SMALL can be defined to remove some code that is especially large. See CMake's documentation for other variables which may be used to configure the build. Building for Android \u00b6 It's possible to build BoringSSL with the Android NDK using CMake. This has been tested with version 10d of the NDK. Unpack the Android NDK somewhere and export ANDROID_NDK to point to the directory. Then make a build directory as above and run CMake like this: cmake -DANDROID_ABI=armeabi-v7a \\ -DCMAKE_TOOLCHAIN_FILE=../third_party/android-cmake/android.toolchain.cmake \\ -DANDROID_NATIVE_API_LEVEL=16 \\ -GNinja .. Once you've run that, Ninja should produce Android-compatible binaries. You can replace armeabi-v7a in the above with arm64-v8a and use API level 21 or higher to build aarch64 binaries. For other options, see android-cmake's documentation . Known Limitations on Windows \u00b6 Versions of CMake since 3.0.2 have a bug in its Ninja generator that causes yasm to output warnings yasm : warning : can open only one input file , only the last file will be processed These warnings can be safely ignored. The cmake bug is http://www.cmake.org/Bug/view.php?id=15253 . CMake can generate Visual Studio projects, but the generated project files don't have steps for assembling the assembly language source files, so they currently cannot be used to build BoringSSL. Embedded ARM \u00b6 ARM, unlike Intel, does not have an instruction that allows applications to discover the capabilities of the processor. Instead, the capability information has to be provided by the operating system somehow. BoringSSL will try to use getauxval to discover the capabilities and, failing that, will probe for NEON support by executing a NEON instruction and handling any illegal-instruction signal. But some environments don't support that sort of thing and, for them, it's possible to configure the CPU capabilities at compile time. If you define OPENSSL_STATIC_ARMCAP then you can define any of the following to enabling the corresponding ARM feature. OPENSSL_STATIC_ARMCAP_NEON or __ARM_NEON__ (note that the latter is set by compilers when NEON support is enabled). OPENSSL_STATIC_ARMCAP_AES OPENSSL_STATIC_ARMCAP_SHA1 OPENSSL_STATIC_ARMCAP_SHA256 OPENSSL_STATIC_ARMCAP_PMULL Note that if a feature is enabled in this way, but not actually supported at run-time, BoringSSL will likely crash. Assembling ARMv8 with Clang \u00b6 In order to support the ARMv8 crypto instructions, Clang requires that the architecture be armv8-a+crypto . However, setting that as a general build flag would allow the compiler to assume that crypto instructions are always supported, even without testing for them. It's possible to set the architecture in an assembly file using the .arch directive, but only very recent versions of Clang support this. If BORINGSSL_CLANG_SUPPORTS_DOT_ARCH is defined then .arch directives will be used with Clang, otherwise you may need to craft acceptable assembler flags. Running tests \u00b6 There are two sets of tests: the C/C++ tests and the blackbox tests. For former are built by Ninja and can be run from the top-level directory with go run util/all_tests.go . The latter have to be run separately by running go test from within ssl/test/runner . Both sets of tests may also be run with ninja -C build run_tests , but CMake 3.2 or later is required to avoid Ninja's output buffering.","title":"BUILDING"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#building-boringssl","text":"","title":"Building BoringSSL"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#build-prerequisites","text":"CMake 2.8.11 or later is required. Perl 5.6.1 or later is required. On Windows, Active State Perl has been reported to work, as has MSYS Perl. Strawberry Perl also works but it adds GCC to PATH , which can confuse some build tools when identifying the compiler (removing C:\\Strawberry\\c\\bin from PATH should resolve any problems). If Perl is not found by CMake, it may be configured explicitly by setting PERL_EXECUTABLE . On Windows you currently must use Ninja to build; on other platforms, it is not required, but recommended, because it makes builds faster. If you need to build Ninja from source, then a recent version of Python is required (Python 2.7.5 works). On Windows only, Yasm is required. If not found by CMake, it may be configured explicitly by setting CMAKE_ASM_NASM_COMPILER . A C compiler is required. On Windows, MSVC 14 (Visual Studio 2015) or later with Platform SDK 8.1 or later are supported. Recent versions of GCC (4.8+) and Clang should work on non-Windows platforms, and maybe on Windows too. To build the tests, you also need a C++ compiler with C++11 support. Go is required. If not found by CMake, the go executable may be configured explicitly by setting GO_EXECUTABLE . To build the x86 and x86_64 assembly, your assembler must support AVX2 instructions and MOVBE. If using GNU binutils, you must have 2.22 or later.","title":"Build Prerequisites"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#building","text":"Using Ninja (note the 'N' is capitalized in the cmake invocation): mkdir build cd build cmake -GNinja .. ninja Using Make (does not work on Windows): mkdir build cd build cmake .. make You usually don't need to run cmake again after changing CMakeLists.txt files because the build scripts will detect changes to them and rebuild themselves automatically. Note that the default build flags in the top-level CMakeLists.txt are for debugging\u2014optimisation isn't enabled. Pass -DCMAKE_BUILD_TYPE=Release to cmake to configure a release build. If you want to cross-compile then there is an example toolchain file for 32-bit Intel in util/ . Wipe out the build directory, recreate it and run cmake like this: cmake -DCMAKE_TOOLCHAIN_FILE=../util/32-bit-toolchain.cmake -GNinja .. If you want to build as a shared library, pass -DBUILD_SHARED_LIBS=1 . On Windows, where functions need to be tagged with dllimport when coming from a shared library, define BORINGSSL_SHARED_LIBRARY in any code which #include s the BoringSSL headers. In order to serve environments where code-size is important as well as those where performance is the overriding concern, OPENSSL_SMALL can be defined to remove some code that is especially large. See CMake's documentation for other variables which may be used to configure the build.","title":"Building"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#building-for-android","text":"It's possible to build BoringSSL with the Android NDK using CMake. This has been tested with version 10d of the NDK. Unpack the Android NDK somewhere and export ANDROID_NDK to point to the directory. Then make a build directory as above and run CMake like this: cmake -DANDROID_ABI=armeabi-v7a \\ -DCMAKE_TOOLCHAIN_FILE=../third_party/android-cmake/android.toolchain.cmake \\ -DANDROID_NATIVE_API_LEVEL=16 \\ -GNinja .. Once you've run that, Ninja should produce Android-compatible binaries. You can replace armeabi-v7a in the above with arm64-v8a and use API level 21 or higher to build aarch64 binaries. For other options, see android-cmake's documentation .","title":"Building for Android"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#known-limitations-on-windows","text":"Versions of CMake since 3.0.2 have a bug in its Ninja generator that causes yasm to output warnings yasm : warning : can open only one input file , only the last file will be processed These warnings can be safely ignored. The cmake bug is http://www.cmake.org/Bug/view.php?id=15253 . CMake can generate Visual Studio projects, but the generated project files don't have steps for assembling the assembly language source files, so they currently cannot be used to build BoringSSL.","title":"Known Limitations on Windows"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#embedded-arm","text":"ARM, unlike Intel, does not have an instruction that allows applications to discover the capabilities of the processor. Instead, the capability information has to be provided by the operating system somehow. BoringSSL will try to use getauxval to discover the capabilities and, failing that, will probe for NEON support by executing a NEON instruction and handling any illegal-instruction signal. But some environments don't support that sort of thing and, for them, it's possible to configure the CPU capabilities at compile time. If you define OPENSSL_STATIC_ARMCAP then you can define any of the following to enabling the corresponding ARM feature. OPENSSL_STATIC_ARMCAP_NEON or __ARM_NEON__ (note that the latter is set by compilers when NEON support is enabled). OPENSSL_STATIC_ARMCAP_AES OPENSSL_STATIC_ARMCAP_SHA1 OPENSSL_STATIC_ARMCAP_SHA256 OPENSSL_STATIC_ARMCAP_PMULL Note that if a feature is enabled in this way, but not actually supported at run-time, BoringSSL will likely crash.","title":"Embedded ARM"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#assembling-armv8-with-clang","text":"In order to support the ARMv8 crypto instructions, Clang requires that the architecture be armv8-a+crypto . However, setting that as a general build flag would allow the compiler to assume that crypto instructions are always supported, even without testing for them. It's possible to set the architecture in an assembly file using the .arch directive, but only very recent versions of Clang support this. If BORINGSSL_CLANG_SUPPORTS_DOT_ARCH is defined then .arch directives will be used with Clang, otherwise you may need to craft acceptable assembler flags.","title":"Assembling ARMv8 with Clang"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/BUILDING/#running-tests","text":"There are two sets of tests: the C/C++ tests and the blackbox tests. For former are built by Ninja and can be run from the top-level directory with go run util/all_tests.go . The latter have to be run separately by running go test from within ssl/test/runner . Both sets of tests may also be run with ninja -C build run_tests , but CMake 3.2 or later is required to avoid Ninja's output buffering.","title":"Running tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/CONTRIBUTING/","text":"Want to contribute? Great! First, read this page (including the small print at the end). Before you contribute \u00b6 Before we can use your code, you must sign the Google Individual Contributor License Agreement (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first via email with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on. Code reviews \u00b6 All submissions, including submissions by project members, require review. We use Gerrit for this purpose. Setup \u00b6 If you have not done so on this machine, you will need to set up a password for Gerrit. Sign in with a Google account, visit this link , and click the \"Generate Password\" link in the top right. You will also need to prepare your checkout to add Change-Ids on commit. Run: curl -Lo .git/hooks/commit-msg https://boringssl-review.googlesource.com/tools/hooks/commit-msg chmod u+x .git/hooks/commit-msg Uploading changes \u00b6 To upload a change, push it to the special refs/for/master target: git push origin HEAD:refs/for/master The output will then give you a link to the change. Add agl@google.com and davidben@google.com as reviewers. Pushing a commit with the same Change-Id as an existing change will upload a new version of it. (Use the git rebase or git commit --amend commands.) For more detailed instructions, see the Gerrit User Guide . The small print \u00b6 Contributions made by corporations are covered by a different agreement than the one above, the Software Grant and Corporate Contributor License Agreement .","title":"CONTRIBUTING"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/CONTRIBUTING/#before-you-contribute","text":"Before we can use your code, you must sign the Google Individual Contributor License Agreement (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first via email with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on.","title":"Before you contribute"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/CONTRIBUTING/#code-reviews","text":"All submissions, including submissions by project members, require review. We use Gerrit for this purpose.","title":"Code reviews"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/CONTRIBUTING/#setup","text":"If you have not done so on this machine, you will need to set up a password for Gerrit. Sign in with a Google account, visit this link , and click the \"Generate Password\" link in the top right. You will also need to prepare your checkout to add Change-Ids on commit. Run: curl -Lo .git/hooks/commit-msg https://boringssl-review.googlesource.com/tools/hooks/commit-msg chmod u+x .git/hooks/commit-msg","title":"Setup"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/CONTRIBUTING/#uploading-changes","text":"To upload a change, push it to the special refs/for/master target: git push origin HEAD:refs/for/master The output will then give you a link to the change. Add agl@google.com and davidben@google.com as reviewers. Pushing a commit with the same Change-Id as an existing change will upload a new version of it. (Use the git rebase or git commit --amend commands.) For more detailed instructions, see the Gerrit User Guide .","title":"Uploading changes"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/CONTRIBUTING/#the-small-print","text":"Contributions made by corporations are covered by a different agreement than the one above, the Software Grant and Corporate Contributor License Agreement .","title":"The small print"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/FUZZING/","text":"Fuzz testing \u00b6 Modern fuzz testers are very effective and we wish to use them to ensure that no silly bugs creep into BoringSSL. We primarily use Clang's libFuzzer for fuzz testing and there are a number of fuzz testing functions in fuzz/ . They are not built by default because they require libFuzzer at build time. In order to build the fuzz tests you will need at least Clang 3.7. Pass -DFUZZ=1 on the CMake command line to enable building BoringSSL with coverage and AddressSanitizer, and to build the fuzz test binaries. You'll probably need to set the CC and CXX environment variables too, like this: CC=clang CXX=clang++ cmake -GNinja -DFUZZ=1 .. In order for the fuzz tests to link, the linker needs to find libFuzzer. This is not commonly provided and you may need to download the Clang source code and do the following: svn co http://llvm.org/svn/llvm-project/llvm/trunk/lib/Fuzzer clang++ -c -g -O2 -std=c++11 Fuzzer/*.cpp -IFuzzer ar ruv libFuzzer.a Fuzzer*.o Then copy libFuzzer.a to the top-level of your BoringSSL source directory. From the build/ directory, you can then run the fuzzers. For example: ./fuzz/cert -max_len=10000 -jobs=32 -workers=32 ../fuzz/cert_corpus/ The arguments to jobs and workers should be the number of cores that you wish to dedicate to fuzzing. By default, libFuzzer uses the largest test in the corpus (or 64 if empty) as the maximum test case length. The max_len argument overrides this. The recommended values of max_len for each test are: Test max_len value cert 10000 client 20000 pkcs8 2048 privkey 2048 server 4096 session 8192 spki 1024 read_pem 512 ssl_ctx_api 256 These were determined by rounding up the length of the largest case in the corpus. There are directories in fuzz/ for each of the fuzzing tests which contain seed files for fuzzing. Some of the seed files were generated manually but many of them are \u201cinteresting\u201d results generated by the fuzzing itself. (Where \u201cinteresting\u201d means that it triggered a previously unknown path in the code.) Minimising the corpuses \u00b6 When a large number of new seeds are available, it's a good idea to minimise the corpus so that different seeds that trigger the same code paths can be deduplicated. In order to minimise all the corpuses, build for fuzzing and run ./fuzz/minimise_corpuses.sh . Note that minimisation is, oddly, often not idempotent for unknown reasons. Fuzzer mode \u00b6 When -DFUZZ=1 is passed into CMake, BoringSSL builds with BORINGSSL_UNSAFE_FUZZER_MODE and BORINGSSL_UNSAFE_DETERMINISTIC_MODE defined. This modifies the library to be more friendly to fuzzers. If BORINGSSL_UNSAFE_DETERMINISTIC_MODE is set, BoringSSL will: Replace RAND_bytes with a deterministic PRNG. Call RAND_reset_for_fuzzing() at the start of fuzzers which use RAND_bytes to reset the PRNG state. Use a hard-coded time instead of the actual time. Additionally, if BORINGSSL_UNSAFE_FUZZER_MODE is set, BoringSSL will: Modify the TLS stack to perform all signature checks (CertificateVerify and ServerKeyExchange) and the Finished check, but always act as if the check succeeded. Treat every cipher as the NULL cipher. Tickets are unencrypted and the MAC check is performed but ignored. This is to prevent the fuzzer from getting stuck at a cryptographic invariant in the protocol. TLS transcripts \u00b6 The client and server corpora are seeded from the test suite. The test suite has a -fuzzer flag which mirrors the fuzzer mode changes above and a -deterministic flag which removes all non-determinism on the Go side. Not all tests pass, so ssl/test/runner/fuzzer_mode.json contains the necessary suppressions. The run_tests target will pass appropriate command-line flags. There are separate corpora, client_corpus_no_fuzzer_mode and server_corpus_no_fuzzer_mode . These are transcripts for fuzzers with only BORINGSSL_UNSAFE_DETERMINISTIC_MODE defined. To build in this mode, pass -DNO_FUZZER_MODE=1 into CMake. This configuration is run in the same way but without -fuzzer and -shim-path flags. If both sets of tests pass, refresh the fuzzer corpora with refresh_ssl_corpora.sh : cd fuzz ./refresh_fuzzer_corpora.sh /path/to/fuzzer/mode/build /path/to/non/fuzzer/mode/build","title":"FUZZING"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/FUZZING/#fuzz-testing","text":"Modern fuzz testers are very effective and we wish to use them to ensure that no silly bugs creep into BoringSSL. We primarily use Clang's libFuzzer for fuzz testing and there are a number of fuzz testing functions in fuzz/ . They are not built by default because they require libFuzzer at build time. In order to build the fuzz tests you will need at least Clang 3.7. Pass -DFUZZ=1 on the CMake command line to enable building BoringSSL with coverage and AddressSanitizer, and to build the fuzz test binaries. You'll probably need to set the CC and CXX environment variables too, like this: CC=clang CXX=clang++ cmake -GNinja -DFUZZ=1 .. In order for the fuzz tests to link, the linker needs to find libFuzzer. This is not commonly provided and you may need to download the Clang source code and do the following: svn co http://llvm.org/svn/llvm-project/llvm/trunk/lib/Fuzzer clang++ -c -g -O2 -std=c++11 Fuzzer/*.cpp -IFuzzer ar ruv libFuzzer.a Fuzzer*.o Then copy libFuzzer.a to the top-level of your BoringSSL source directory. From the build/ directory, you can then run the fuzzers. For example: ./fuzz/cert -max_len=10000 -jobs=32 -workers=32 ../fuzz/cert_corpus/ The arguments to jobs and workers should be the number of cores that you wish to dedicate to fuzzing. By default, libFuzzer uses the largest test in the corpus (or 64 if empty) as the maximum test case length. The max_len argument overrides this. The recommended values of max_len for each test are: Test max_len value cert 10000 client 20000 pkcs8 2048 privkey 2048 server 4096 session 8192 spki 1024 read_pem 512 ssl_ctx_api 256 These were determined by rounding up the length of the largest case in the corpus. There are directories in fuzz/ for each of the fuzzing tests which contain seed files for fuzzing. Some of the seed files were generated manually but many of them are \u201cinteresting\u201d results generated by the fuzzing itself. (Where \u201cinteresting\u201d means that it triggered a previously unknown path in the code.)","title":"Fuzz testing"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/FUZZING/#minimising-the-corpuses","text":"When a large number of new seeds are available, it's a good idea to minimise the corpus so that different seeds that trigger the same code paths can be deduplicated. In order to minimise all the corpuses, build for fuzzing and run ./fuzz/minimise_corpuses.sh . Note that minimisation is, oddly, often not idempotent for unknown reasons.","title":"Minimising the corpuses"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/FUZZING/#fuzzer-mode","text":"When -DFUZZ=1 is passed into CMake, BoringSSL builds with BORINGSSL_UNSAFE_FUZZER_MODE and BORINGSSL_UNSAFE_DETERMINISTIC_MODE defined. This modifies the library to be more friendly to fuzzers. If BORINGSSL_UNSAFE_DETERMINISTIC_MODE is set, BoringSSL will: Replace RAND_bytes with a deterministic PRNG. Call RAND_reset_for_fuzzing() at the start of fuzzers which use RAND_bytes to reset the PRNG state. Use a hard-coded time instead of the actual time. Additionally, if BORINGSSL_UNSAFE_FUZZER_MODE is set, BoringSSL will: Modify the TLS stack to perform all signature checks (CertificateVerify and ServerKeyExchange) and the Finished check, but always act as if the check succeeded. Treat every cipher as the NULL cipher. Tickets are unencrypted and the MAC check is performed but ignored. This is to prevent the fuzzer from getting stuck at a cryptographic invariant in the protocol.","title":"Fuzzer mode"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/FUZZING/#tls-transcripts","text":"The client and server corpora are seeded from the test suite. The test suite has a -fuzzer flag which mirrors the fuzzer mode changes above and a -deterministic flag which removes all non-determinism on the Go side. Not all tests pass, so ssl/test/runner/fuzzer_mode.json contains the necessary suppressions. The run_tests target will pass appropriate command-line flags. There are separate corpora, client_corpus_no_fuzzer_mode and server_corpus_no_fuzzer_mode . These are transcripts for fuzzers with only BORINGSSL_UNSAFE_DETERMINISTIC_MODE defined. To build in this mode, pass -DNO_FUZZER_MODE=1 into CMake. This configuration is run in the same way but without -fuzzer and -shim-path flags. If both sets of tests pass, refresh the fuzzer corpora with refresh_ssl_corpora.sh : cd fuzz ./refresh_fuzzer_corpora.sh /path/to/fuzzer/mode/build /path/to/non/fuzzer/mode/build","title":"TLS transcripts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/","text":"Incorporating BoringSSL into a project \u00b6 Note : if your target project is not a Google project then first read the main README about the purpose of BoringSSL. Bazel \u00b6 If you are using Bazel then you can incorporate BoringSSL as an external repository by using a commit from the master-with-bazel branch. That branch is maintained by a bot from master and includes the needed generated files and a top-level BUILD file. For example: git_repository( name = \"boringssl\", commit = \"_some commit_\", remote = \"https://boringssl.googlesource.com/boringssl\", ) You would still need to keep the referenced commit up to date if a specific commit is referred to. Directory layout \u00b6 Typically projects create a third_party/boringssl directory to put BoringSSL-specific files into. The source code of BoringSSL itself goes into third_party/boringssl/src , either by copying or as a submodule . It's generally a mistake to put BoringSSL's source code into third_party/boringssl directly because pre-built files and custom build files need to go somewhere and merging these with the BoringSSL source code makes updating things more complex. Build support \u00b6 BoringSSL is designed to work with many different build systems. Currently, different projects use GYP , GN , Bazel and Make to build BoringSSL, without too much pain. The development build system is CMake and the CMake build knows how to automatically generate the intermediate files that BoringSSL needs. However, outside of the CMake environment, these intermediates are generated once and checked into the incorporating project's source repository. This avoids incorporating projects needing to support Perl and Go in their build systems. The script util/generate_build_files.py expects to be run from the third_party/boringssl directory and to find the BoringSSL source code in src/ . You should pass it a single argument: the name of the build system that you're using. If you don't use any of the supported build systems then you should augment generate_build_files.py with support for it. The script will pregenerate the intermediate files (see BUILDING.md for details about which tools will need to be installed) and output helper files for that build system. It doesn't generate a complete build script, just file and test lists, which change often. For example, see the file and test lists generated for GN in Chromium. Generally one checks in these generated files alongside the hand-written build files. Periodically an engineer updates the BoringSSL revision, regenerates these files and checks in the updated result. As an example, see how this is done in Chromium . Defines \u00b6 BoringSSL does not present a lot of configurability in order to reduce the number of configurations that need to be tested. But there are a couple of #defines that you may wish to set: OPENSSL_NO_ASM prevents the use of assembly code (although it's up to you to ensure that the build system doesn't link it in if you wish to reduce binary size). This will have a significant performance impact but can be useful if you wish to use tools like AddressSanitizer that interact poorly with assembly code. OPENSSL_SMALL removes some code that is especially large at some performance cost. Symbols \u00b6 You cannot link multiple versions of BoringSSL or OpenSSL into a single binary without dealing with symbol conflicts. If you are statically linking multiple versions together, there's not a lot that can be done because C doesn't have a module system. If you are using multiple versions in a single binary, in different shared objects, ensure you build BoringSSL with -fvisibility=hidden and do not export any of BoringSSL's symbols. This will prevent any collisions with other verisons that may be included in other shared objects. Note that this requires that all callers of BoringSSL APIs live in the same shared object as BoringSSL. If you require that BoringSSL APIs be used across shared object boundaries, continue to build with -fvisibility=hidden but define BORINGSSL_SHARED_LIBRARY in both BoringSSL and consumers. BoringSSL's own source files (but not consumers' source files) must also build with BORINGSSL_IMPLEMENTATION defined. This will export BoringSSL's public symbols in the resulting shared object while hiding private symbols. However note that, as with a static link, this precludes dynamically linking with another version of BoringSSL or OpenSSL.","title":"INCORPORATING"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/#incorporating-boringssl-into-a-project","text":"Note : if your target project is not a Google project then first read the main README about the purpose of BoringSSL.","title":"Incorporating BoringSSL into a project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/#bazel","text":"If you are using Bazel then you can incorporate BoringSSL as an external repository by using a commit from the master-with-bazel branch. That branch is maintained by a bot from master and includes the needed generated files and a top-level BUILD file. For example: git_repository( name = \"boringssl\", commit = \"_some commit_\", remote = \"https://boringssl.googlesource.com/boringssl\", ) You would still need to keep the referenced commit up to date if a specific commit is referred to.","title":"Bazel"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/#directory-layout","text":"Typically projects create a third_party/boringssl directory to put BoringSSL-specific files into. The source code of BoringSSL itself goes into third_party/boringssl/src , either by copying or as a submodule . It's generally a mistake to put BoringSSL's source code into third_party/boringssl directly because pre-built files and custom build files need to go somewhere and merging these with the BoringSSL source code makes updating things more complex.","title":"Directory layout"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/#build-support","text":"BoringSSL is designed to work with many different build systems. Currently, different projects use GYP , GN , Bazel and Make to build BoringSSL, without too much pain. The development build system is CMake and the CMake build knows how to automatically generate the intermediate files that BoringSSL needs. However, outside of the CMake environment, these intermediates are generated once and checked into the incorporating project's source repository. This avoids incorporating projects needing to support Perl and Go in their build systems. The script util/generate_build_files.py expects to be run from the third_party/boringssl directory and to find the BoringSSL source code in src/ . You should pass it a single argument: the name of the build system that you're using. If you don't use any of the supported build systems then you should augment generate_build_files.py with support for it. The script will pregenerate the intermediate files (see BUILDING.md for details about which tools will need to be installed) and output helper files for that build system. It doesn't generate a complete build script, just file and test lists, which change often. For example, see the file and test lists generated for GN in Chromium. Generally one checks in these generated files alongside the hand-written build files. Periodically an engineer updates the BoringSSL revision, regenerates these files and checks in the updated result. As an example, see how this is done in Chromium .","title":"Build support"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/#defines","text":"BoringSSL does not present a lot of configurability in order to reduce the number of configurations that need to be tested. But there are a couple of #defines that you may wish to set: OPENSSL_NO_ASM prevents the use of assembly code (although it's up to you to ensure that the build system doesn't link it in if you wish to reduce binary size). This will have a significant performance impact but can be useful if you wish to use tools like AddressSanitizer that interact poorly with assembly code. OPENSSL_SMALL removes some code that is especially large at some performance cost.","title":"Defines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/INCORPORATING/#symbols","text":"You cannot link multiple versions of BoringSSL or OpenSSL into a single binary without dealing with symbol conflicts. If you are statically linking multiple versions together, there's not a lot that can be done because C doesn't have a module system. If you are using multiple versions in a single binary, in different shared objects, ensure you build BoringSSL with -fvisibility=hidden and do not export any of BoringSSL's symbols. This will prevent any collisions with other verisons that may be included in other shared objects. Note that this requires that all callers of BoringSSL APIs live in the same shared object as BoringSSL. If you require that BoringSSL APIs be used across shared object boundaries, continue to build with -fvisibility=hidden but define BORINGSSL_SHARED_LIBRARY in both BoringSSL and consumers. BoringSSL's own source files (but not consumers' source files) must also build with BORINGSSL_IMPLEMENTATION defined. This will export BoringSSL's public symbols in the resulting shared object while hiding private symbols. However note that, as with a static link, this precludes dynamically linking with another version of BoringSSL or OpenSSL.","title":"Symbols"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/","text":"Porting from OpenSSL to BoringSSL \u00b6 BoringSSL is an OpenSSL derivative and is mostly source-compatible, for the subset of OpenSSL retained. Libraries ideally need little to no changes for BoringSSL support, provided they do not use removed APIs. In general, see if the library compiles and, on failure, consult the documentation in the header files and see if problematic features can be removed. In some cases, BoringSSL-specific code may be necessary. In that case, the OPENSSL_IS_BORINGSSL preprocessor macro may be used in #ifdef s. This macro should also be used in lieu of the presence of any particular function to detect OpenSSL vs BoringSSL in configure scripts, etc., where those are necessary. Before using the preprocessor, however, contact the BoringSSL maintainers about the missing APIs. If not an intentionally removed feature, BoringSSL will typically add compatibility functions for convenience. For convenience, BoringSSL defines upstream's OPENSSL_NO_* feature macros corresponding to removed features. These may also be used to disable code which uses a removed feature. Note: BoringSSL does not have a stable API or ABI. It must be updated with its consumers. It is not suitable for, say, a system library in a traditional Linux distribution. For instance, Chromium statically links the specific revision of BoringSSL it was built against. Likewise, Android's system-internal copy of BoringSSL is not exposed by the NDK and must not be used by third-party applications. Major API changes \u00b6 Integer types \u00b6 Some APIs have been converted to use size_t for consistency and to avoid integer overflows at the API boundary. (Existing logic uses a mismash of int , long , and unsigned .) For the most part, implicit casts mean that existing code continues to compile. In some cases, this may require BoringSSL-specific code, particularly to avoid compiler warnings. Most notably, the STACK_OF(T) types have all been converted to use size_t instead of int for indices and lengths. Reference counts \u00b6 Some external consumers increment reference counts directly by calling CRYPTO_add with the corresponding CRYPTO_LOCK_* value. These APIs no longer exist in BoringSSL. Instead, code which increments reference counts should call the corresponding FOO_up_ref function, such as EVP_PKEY_up_ref . Note that not all of these APIs are present in OpenSSL and may require #ifdef s. Error codes \u00b6 OpenSSL's errors are extremely specific, leaking internals of the library, including even a function code for the function which emitted the error! As some logic in BoringSSL has been rewritten, code which conditions on the error may break (grep for ERR_GET_REASON and ERR_GET_FUNC ). This danger also exists when upgrading OpenSSL versions. Where possible, avoid conditioning on the exact error reason. Otherwise, a BoringSSL #ifdef may be necessary. Exactly how best to resolve this issue is still being determined. It's possible some new APIs will be added in the future. Function codes have been completely removed. Remove code which conditions on these as it will break with the slightest change in the library, OpenSSL or BoringSSL. *_ctrl functions \u00b6 Some OpenSSL APIs are implemented with ioctl -style functions such as SSL_ctrl and EVP_PKEY_CTX_ctrl , combined with convenience macros, such as # define SSL_CTX_set_mode(ctx,op) \\ SSL_CTX_ctrl((ctx),SSL_CTRL_MODE,(op),NULL) In BoringSSL, these macros have been replaced with proper functions. The underlying _ctrl functions have been removed. For convenience, SSL_CTRL_* values are retained as macros to doesnt_exist so existing code which uses them (or the wrapper macros) in #ifdef expressions will continue to function. However, the macros themselves will not work. Switch any *_ctrl callers to the macro/function versions. This works in both OpenSSL and BoringSSL. Note that BoringSSL's function versions will be type-checked and may require more care with types. See the end of this document for a table of functions to use. HMAC EVP_PKEY s \u00b6 EVP_PKEY_HMAC is removed. Use the HMAC_* functions in hmac.h instead. This is compatible with OpenSSL. DSA EVP_PKEY s \u00b6 EVP_PKEY_DSA is deprecated. It is currently still possible to parse DER into a DSA EVP_PKEY , but signing or verifying with those objects will not work. DES \u00b6 The DES_cblock type has been switched from an array to a struct to avoid the pitfalls around array types in C. Where features which require DES cannot be disabled, BoringSSL-specific codepaths may be necessary. TLS renegotiation \u00b6 OpenSSL enables TLS renegotiation by default and accepts renegotiation requests from the peer transparently. Renegotiation is an extremely problematic protocol feature, so BoringSSL rejects peer renegotiations by default. To enable renegotiation, call SSL_set_renegotiate_mode and set it to ssl_renegotiate_once or ssl_renegotiate_freely . Renegotiation is only supported as a client in SSL3/TLS and the HelloRequest must be received at a quiet point in the application protocol. This is sufficient to support the common use of requesting a new client certificate between an HTTP request and response in (unpipelined) HTTP/1.1. Things which do not work: There is no support for renegotiation as a server. There is no support for renegotiation in DTLS. There is no support for initiating renegotiation; SSL_renegotiate always fails and SSL_set_state does nothing. Interleaving application data with the new handshake is forbidden. If a HelloRequest is received while SSL_write has unsent application data, the renegotiation is rejected. Lowercase hexadecimal \u00b6 BoringSSL's BN_bn2hex function uses lowercase hexadecimal digits instead of uppercase. Some code may require changes to avoid being sensitive to this difference. Legacy ASN.1 functions \u00b6 OpenSSL's ASN.1 stack uses d2i functions for parsing. They have the form: RSA *d2i_RSAPrivateKey(RSA **out, const uint8_t **inp, long len); In addition to returning the result, OpenSSL places it in *out if out is not NULL . On input, if *out is not NULL , OpenSSL will usually (but not always) reuse that object rather than allocating a new one. In BoringSSL, these functions are compatibility wrappers over a newer ASN.1 stack. Even if *out is not NULL , these wrappers will always allocate a new object and free the previous one. Ensure that callers do not rely on this object reuse behavior. It is recommended to avoid the out parameter completely and always pass in NULL . Note that less error-prone APIs are available for BoringSSL-specific code (see below). Optional BoringSSL-specific simplifications \u00b6 BoringSSL makes some changes to OpenSSL which simplify the API but remain compatible with OpenSSL consumers. In general, consult the BoringSSL documentation for any functions in new BoringSSL-only code. Return values \u00b6 Most OpenSSL APIs return 1 on success and either 0 or -1 on failure. BoringSSL has narrowed most of these to 1 on success and 0 on failure. BoringSSL-specific code may take advantage of the less error-prone APIs and use ! to check for errors. Initialization \u00b6 OpenSSL has a number of different initialization functions for setting up error strings and loading algorithms, etc. All of these functions still exist in BoringSSL for convenience, but they do nothing and are not necessary. The one exception is CRYPTO_library_init . In BORINGSSL_NO_STATIC_INITIALIZER builds, it must be called to query CPU capabitilies before the rest of the library. In the default configuration, this is done with a static initializer and is also unnecessary. Threading \u00b6 OpenSSL provides a number of APIs to configure threading callbacks and set up locks. Without initializing these, the library is not thread-safe. Configuring these does nothing in BoringSSL. Instead, BoringSSL calls pthreads and the corresponding Windows APIs internally and is always thread-safe where the API guarantees it. ASN.1 \u00b6 BoringSSL is in the process of deprecating OpenSSL's d2i and i2d in favor of new functions using the much less error-prone CBS and CBB types. BoringSSL-only code should use those functions where available. Replacements for CTRL values \u00b6 When porting code which uses SSL_CTX_ctrl or SSL_ctrl , use the replacement functions below. If a function has both SSL_CTX and SSL variants, only the SSL_CTX version is listed. Note some values correspond to multiple functions depending on the larg parameter. CTRL value Replacement function(s) DTLS_CTRL_GET_TIMEOUT DTLSv1_get_timeout DTLS_CTRL_HANDLE_TIMEOUT DTLSv1_handle_timeout SSL_CTRL_CHAIN SSL_CTX_set0_chain or SSL_CTX_set1_chain SSL_CTRL_CHAIN_CERT SSL_add0_chain_cert or SSL_add1_chain_cert SSL_CTRL_CLEAR_EXTRA_CHAIN_CERTS SSL_CTX_clear_extra_chain_certs SSL_CTRL_CLEAR_MODE SSL_CTX_clear_mode SSL_CTRL_CLEAR_OPTIONS SSL_CTX_clear_options SSL_CTRL_EXTRA_CHAIN_CERT SSL_CTX_add_extra_chain_cert SSL_CTRL_GET_CHAIN_CERTS SSL_CTX_get0_chain_certs SSL_CTRL_GET_CLIENT_CERT_TYPES SSL_get0_certificate_types SSL_CTRL_GET_EXTRA_CHAIN_CERTS SSL_CTX_get_extra_chain_certs or SSL_CTX_get_extra_chain_certs_only SSL_CTRL_GET_MAX_CERT_LIST SSL_CTX_get_max_cert_list SSL_CTRL_GET_NUM_RENEGOTIATIONS SSL_num_renegotiations SSL_CTRL_GET_READ_AHEAD SSL_CTX_get_read_ahead SSL_CTRL_GET_RI_SUPPORT SSL_get_secure_renegotiation_support SSL_CTRL_GET_SESSION_REUSED SSL_session_reused SSL_CTRL_GET_SESS_CACHE_MODE SSL_CTX_get_session_cache_mode SSL_CTRL_GET_SESS_CACHE_SIZE SSL_CTX_sess_get_cache_size SSL_CTRL_GET_TLSEXT_TICKET_KEYS SSL_CTX_get_tlsext_ticket_keys SSL_CTRL_GET_TOTAL_RENEGOTIATIONS SSL_total_renegotiations SSL_CTRL_MODE SSL_CTX_get_mode or SSL_CTX_set_mode SSL_CTRL_NEED_TMP_RSA SSL_CTX_need_tmp_RSA is equivalent, but do not use this function . (It is a no-op in BoringSSL.) SSL_CTRL_OPTIONS SSL_CTX_get_options or SSL_CTX_set_options SSL_CTRL_SESS_NUMBER SSL_CTX_sess_number SSL_CTRL_SET_CURVES SSL_CTX_set1_curves SSL_CTRL_SET_MAX_CERT_LIST SSL_CTX_set_max_cert_list SSL_CTRL_SET_MAX_SEND_FRAGMENT SSL_CTX_set_max_send_fragment SSL_CTRL_SET_MSG_CALLBACK SSL_set_msg_callback SSL_CTRL_SET_MSG_CALLBACK_ARG SSL_set_msg_callback_arg SSL_CTRL_SET_MTU SSL_set_mtu SSL_CTRL_SET_READ_AHEAD SSL_CTX_set_read_ahead SSL_CTRL_SET_SESS_CACHE_MODE SSL_CTX_set_session_cache_mode SSL_CTRL_SET_SESS_CACHE_SIZE SSL_CTX_sess_set_cache_size SSL_CTRL_SET_TLSEXT_HOSTNAME SSL_set_tlsext_host_name SSL_CTRL_SET_TLSEXT_SERVERNAME_ARG SSL_CTX_set_tlsext_servername_arg SSL_CTRL_SET_TLSEXT_SERVERNAME_CB SSL_CTX_set_tlsext_servername_callback SSL_CTRL_SET_TLSEXT_TICKET_KEYS SSL_CTX_set_tlsext_ticket_keys SSL_CTRL_SET_TLSEXT_TICKET_KEY_CB SSL_CTX_set_tlsext_ticket_key_cb SSL_CTRL_SET_TMP_DH SSL_CTX_set_tmp_dh SSL_CTRL_SET_TMP_DH_CB SSL_CTX_set_tmp_dh_callback SSL_CTRL_SET_TMP_ECDH SSL_CTX_set_tmp_ecdh SSL_CTRL_SET_TMP_ECDH_CB SSL_CTX_set_tmp_ecdh_callback SSL_CTRL_SET_TMP_RSA SSL_CTX_set_tmp_rsa is equivalent, but do not use this function . (It is a no-op in BoringSSL.) SSL_CTRL_SET_TMP_RSA_CB SSL_CTX_set_tmp_rsa_callback is equivalent, but do not use this function . (It is a no-op in BoringSSL.)","title":"PORTING"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#porting-from-openssl-to-boringssl","text":"BoringSSL is an OpenSSL derivative and is mostly source-compatible, for the subset of OpenSSL retained. Libraries ideally need little to no changes for BoringSSL support, provided they do not use removed APIs. In general, see if the library compiles and, on failure, consult the documentation in the header files and see if problematic features can be removed. In some cases, BoringSSL-specific code may be necessary. In that case, the OPENSSL_IS_BORINGSSL preprocessor macro may be used in #ifdef s. This macro should also be used in lieu of the presence of any particular function to detect OpenSSL vs BoringSSL in configure scripts, etc., where those are necessary. Before using the preprocessor, however, contact the BoringSSL maintainers about the missing APIs. If not an intentionally removed feature, BoringSSL will typically add compatibility functions for convenience. For convenience, BoringSSL defines upstream's OPENSSL_NO_* feature macros corresponding to removed features. These may also be used to disable code which uses a removed feature. Note: BoringSSL does not have a stable API or ABI. It must be updated with its consumers. It is not suitable for, say, a system library in a traditional Linux distribution. For instance, Chromium statically links the specific revision of BoringSSL it was built against. Likewise, Android's system-internal copy of BoringSSL is not exposed by the NDK and must not be used by third-party applications.","title":"Porting from OpenSSL to BoringSSL"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#major-api-changes","text":"","title":"Major API changes"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#integer-types","text":"Some APIs have been converted to use size_t for consistency and to avoid integer overflows at the API boundary. (Existing logic uses a mismash of int , long , and unsigned .) For the most part, implicit casts mean that existing code continues to compile. In some cases, this may require BoringSSL-specific code, particularly to avoid compiler warnings. Most notably, the STACK_OF(T) types have all been converted to use size_t instead of int for indices and lengths.","title":"Integer types"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#reference-counts","text":"Some external consumers increment reference counts directly by calling CRYPTO_add with the corresponding CRYPTO_LOCK_* value. These APIs no longer exist in BoringSSL. Instead, code which increments reference counts should call the corresponding FOO_up_ref function, such as EVP_PKEY_up_ref . Note that not all of these APIs are present in OpenSSL and may require #ifdef s.","title":"Reference counts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#error-codes","text":"OpenSSL's errors are extremely specific, leaking internals of the library, including even a function code for the function which emitted the error! As some logic in BoringSSL has been rewritten, code which conditions on the error may break (grep for ERR_GET_REASON and ERR_GET_FUNC ). This danger also exists when upgrading OpenSSL versions. Where possible, avoid conditioning on the exact error reason. Otherwise, a BoringSSL #ifdef may be necessary. Exactly how best to resolve this issue is still being determined. It's possible some new APIs will be added in the future. Function codes have been completely removed. Remove code which conditions on these as it will break with the slightest change in the library, OpenSSL or BoringSSL.","title":"Error codes"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#_ctrl-functions","text":"Some OpenSSL APIs are implemented with ioctl -style functions such as SSL_ctrl and EVP_PKEY_CTX_ctrl , combined with convenience macros, such as # define SSL_CTX_set_mode(ctx,op) \\ SSL_CTX_ctrl((ctx),SSL_CTRL_MODE,(op),NULL) In BoringSSL, these macros have been replaced with proper functions. The underlying _ctrl functions have been removed. For convenience, SSL_CTRL_* values are retained as macros to doesnt_exist so existing code which uses them (or the wrapper macros) in #ifdef expressions will continue to function. However, the macros themselves will not work. Switch any *_ctrl callers to the macro/function versions. This works in both OpenSSL and BoringSSL. Note that BoringSSL's function versions will be type-checked and may require more care with types. See the end of this document for a table of functions to use.","title":"*_ctrl functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#hmac-evp_pkeys","text":"EVP_PKEY_HMAC is removed. Use the HMAC_* functions in hmac.h instead. This is compatible with OpenSSL.","title":"HMAC EVP_PKEYs"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#dsa-evp_pkeys","text":"EVP_PKEY_DSA is deprecated. It is currently still possible to parse DER into a DSA EVP_PKEY , but signing or verifying with those objects will not work.","title":"DSA EVP_PKEYs"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#des","text":"The DES_cblock type has been switched from an array to a struct to avoid the pitfalls around array types in C. Where features which require DES cannot be disabled, BoringSSL-specific codepaths may be necessary.","title":"DES"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#tls-renegotiation","text":"OpenSSL enables TLS renegotiation by default and accepts renegotiation requests from the peer transparently. Renegotiation is an extremely problematic protocol feature, so BoringSSL rejects peer renegotiations by default. To enable renegotiation, call SSL_set_renegotiate_mode and set it to ssl_renegotiate_once or ssl_renegotiate_freely . Renegotiation is only supported as a client in SSL3/TLS and the HelloRequest must be received at a quiet point in the application protocol. This is sufficient to support the common use of requesting a new client certificate between an HTTP request and response in (unpipelined) HTTP/1.1. Things which do not work: There is no support for renegotiation as a server. There is no support for renegotiation in DTLS. There is no support for initiating renegotiation; SSL_renegotiate always fails and SSL_set_state does nothing. Interleaving application data with the new handshake is forbidden. If a HelloRequest is received while SSL_write has unsent application data, the renegotiation is rejected.","title":"TLS renegotiation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#lowercase-hexadecimal","text":"BoringSSL's BN_bn2hex function uses lowercase hexadecimal digits instead of uppercase. Some code may require changes to avoid being sensitive to this difference.","title":"Lowercase hexadecimal"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#legacy-asn1-functions","text":"OpenSSL's ASN.1 stack uses d2i functions for parsing. They have the form: RSA *d2i_RSAPrivateKey(RSA **out, const uint8_t **inp, long len); In addition to returning the result, OpenSSL places it in *out if out is not NULL . On input, if *out is not NULL , OpenSSL will usually (but not always) reuse that object rather than allocating a new one. In BoringSSL, these functions are compatibility wrappers over a newer ASN.1 stack. Even if *out is not NULL , these wrappers will always allocate a new object and free the previous one. Ensure that callers do not rely on this object reuse behavior. It is recommended to avoid the out parameter completely and always pass in NULL . Note that less error-prone APIs are available for BoringSSL-specific code (see below).","title":"Legacy ASN.1 functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#optional-boringssl-specific-simplifications","text":"BoringSSL makes some changes to OpenSSL which simplify the API but remain compatible with OpenSSL consumers. In general, consult the BoringSSL documentation for any functions in new BoringSSL-only code.","title":"Optional BoringSSL-specific simplifications"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#return-values","text":"Most OpenSSL APIs return 1 on success and either 0 or -1 on failure. BoringSSL has narrowed most of these to 1 on success and 0 on failure. BoringSSL-specific code may take advantage of the less error-prone APIs and use ! to check for errors.","title":"Return values"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#initialization","text":"OpenSSL has a number of different initialization functions for setting up error strings and loading algorithms, etc. All of these functions still exist in BoringSSL for convenience, but they do nothing and are not necessary. The one exception is CRYPTO_library_init . In BORINGSSL_NO_STATIC_INITIALIZER builds, it must be called to query CPU capabitilies before the rest of the library. In the default configuration, this is done with a static initializer and is also unnecessary.","title":"Initialization"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#threading","text":"OpenSSL provides a number of APIs to configure threading callbacks and set up locks. Without initializing these, the library is not thread-safe. Configuring these does nothing in BoringSSL. Instead, BoringSSL calls pthreads and the corresponding Windows APIs internally and is always thread-safe where the API guarantees it.","title":"Threading"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#asn1","text":"BoringSSL is in the process of deprecating OpenSSL's d2i and i2d in favor of new functions using the much less error-prone CBS and CBB types. BoringSSL-only code should use those functions where available.","title":"ASN.1"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/PORTING/#replacements-for-ctrl-values","text":"When porting code which uses SSL_CTX_ctrl or SSL_ctrl , use the replacement functions below. If a function has both SSL_CTX and SSL variants, only the SSL_CTX version is listed. Note some values correspond to multiple functions depending on the larg parameter. CTRL value Replacement function(s) DTLS_CTRL_GET_TIMEOUT DTLSv1_get_timeout DTLS_CTRL_HANDLE_TIMEOUT DTLSv1_handle_timeout SSL_CTRL_CHAIN SSL_CTX_set0_chain or SSL_CTX_set1_chain SSL_CTRL_CHAIN_CERT SSL_add0_chain_cert or SSL_add1_chain_cert SSL_CTRL_CLEAR_EXTRA_CHAIN_CERTS SSL_CTX_clear_extra_chain_certs SSL_CTRL_CLEAR_MODE SSL_CTX_clear_mode SSL_CTRL_CLEAR_OPTIONS SSL_CTX_clear_options SSL_CTRL_EXTRA_CHAIN_CERT SSL_CTX_add_extra_chain_cert SSL_CTRL_GET_CHAIN_CERTS SSL_CTX_get0_chain_certs SSL_CTRL_GET_CLIENT_CERT_TYPES SSL_get0_certificate_types SSL_CTRL_GET_EXTRA_CHAIN_CERTS SSL_CTX_get_extra_chain_certs or SSL_CTX_get_extra_chain_certs_only SSL_CTRL_GET_MAX_CERT_LIST SSL_CTX_get_max_cert_list SSL_CTRL_GET_NUM_RENEGOTIATIONS SSL_num_renegotiations SSL_CTRL_GET_READ_AHEAD SSL_CTX_get_read_ahead SSL_CTRL_GET_RI_SUPPORT SSL_get_secure_renegotiation_support SSL_CTRL_GET_SESSION_REUSED SSL_session_reused SSL_CTRL_GET_SESS_CACHE_MODE SSL_CTX_get_session_cache_mode SSL_CTRL_GET_SESS_CACHE_SIZE SSL_CTX_sess_get_cache_size SSL_CTRL_GET_TLSEXT_TICKET_KEYS SSL_CTX_get_tlsext_ticket_keys SSL_CTRL_GET_TOTAL_RENEGOTIATIONS SSL_total_renegotiations SSL_CTRL_MODE SSL_CTX_get_mode or SSL_CTX_set_mode SSL_CTRL_NEED_TMP_RSA SSL_CTX_need_tmp_RSA is equivalent, but do not use this function . (It is a no-op in BoringSSL.) SSL_CTRL_OPTIONS SSL_CTX_get_options or SSL_CTX_set_options SSL_CTRL_SESS_NUMBER SSL_CTX_sess_number SSL_CTRL_SET_CURVES SSL_CTX_set1_curves SSL_CTRL_SET_MAX_CERT_LIST SSL_CTX_set_max_cert_list SSL_CTRL_SET_MAX_SEND_FRAGMENT SSL_CTX_set_max_send_fragment SSL_CTRL_SET_MSG_CALLBACK SSL_set_msg_callback SSL_CTRL_SET_MSG_CALLBACK_ARG SSL_set_msg_callback_arg SSL_CTRL_SET_MTU SSL_set_mtu SSL_CTRL_SET_READ_AHEAD SSL_CTX_set_read_ahead SSL_CTRL_SET_SESS_CACHE_MODE SSL_CTX_set_session_cache_mode SSL_CTRL_SET_SESS_CACHE_SIZE SSL_CTX_sess_set_cache_size SSL_CTRL_SET_TLSEXT_HOSTNAME SSL_set_tlsext_host_name SSL_CTRL_SET_TLSEXT_SERVERNAME_ARG SSL_CTX_set_tlsext_servername_arg SSL_CTRL_SET_TLSEXT_SERVERNAME_CB SSL_CTX_set_tlsext_servername_callback SSL_CTRL_SET_TLSEXT_TICKET_KEYS SSL_CTX_set_tlsext_ticket_keys SSL_CTRL_SET_TLSEXT_TICKET_KEY_CB SSL_CTX_set_tlsext_ticket_key_cb SSL_CTRL_SET_TMP_DH SSL_CTX_set_tmp_dh SSL_CTRL_SET_TMP_DH_CB SSL_CTX_set_tmp_dh_callback SSL_CTRL_SET_TMP_ECDH SSL_CTX_set_tmp_ecdh SSL_CTRL_SET_TMP_ECDH_CB SSL_CTX_set_tmp_ecdh_callback SSL_CTRL_SET_TMP_RSA SSL_CTX_set_tmp_rsa is equivalent, but do not use this function . (It is a no-op in BoringSSL.) SSL_CTRL_SET_TMP_RSA_CB SSL_CTX_set_tmp_rsa_callback is equivalent, but do not use this function . (It is a no-op in BoringSSL.)","title":"Replacements for CTRL values"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/","text":"BoringSSL Style Guide \u00b6 BoringSSL usually follows the Google C++ style guide , The rest of this document describes differences and clarifications on top of the base guide. Legacy code \u00b6 As a derivative of OpenSSL, BoringSSL contains a lot of legacy code that does not follow this style guide. Particularly where public API is concerned, balance consistency within a module with the benefits of a given rule. Module-wide deviations on naming should be respected while integer and return value conventions take precedence over consistency. Modules from OpenSSL's legacy ASN.1 and X.509 stack are retained for compatibility and left largely unmodified. To ease importing patches from upstream, they match OpenSSL's new indentation style. For Emacs, doc/openssl-c-indent.el from OpenSSL may be helpful in this. Language \u00b6 The majority of the project is in C, so C++-specific rules in the Google style guide do not apply. Support for C99 features depends on our target platforms. Typically, Chromium's target MSVC is the most restrictive. Variable declarations in the middle of a function or inside a for loop are allowed and preferred where possible. Note that the common goto err cleanup pattern requires lifting some variable declarations. Comments should be /* C-style */ for consistency. When declaration pointer types, * should be placed next to the variable name, not the type. So uint8_t *ptr; not uint8_t* ptr; Rather than malloc() and free() , use the wrappers OPENSSL_malloc() and OPENSSL_free() . Use the standard C assert() function freely. Use the following wrappers, found in crypto/internal.h instead of the corresponding C standard library functions. They behave the same but avoid confusing undefined behavior. OPENSSL_memchr OPENSSL_memcmp OPENSSL_memcpy OPENSSL_memmove OPENSSL_memset For new constants, prefer enums when the values are sequential and typed constants for flags. If adding values to an existing set of #define s, continue with #define . Formatting \u00b6 Single-statement blocks are not allowed. All conditions and loops must use braces: if (foo) { do_something(); } not if (foo) do_something(); Integers \u00b6 Prefer using explicitly-sized integers where appropriate rather than generic C ones. For instance, to represent a byte, use uint8_t , not unsigned char . Likewise, represent a two-byte field as uint16_t , not unsigned short . Sizes are represented as size_t . Within a struct that is retained across the lifetime of an SSL connection, if bounds of a size are known and it's easy, use a smaller integer type like uint8_t . This is a \"free\" connection footprint optimization for servers. Don't make code significantly more complex for it, and do still check the bounds when passing in and out of the struct. This narrowing should not propagate to local variables and function parameters. When doing arithmetic, account for overflow conditions. Except with platform APIs, do not use ssize_t . MSVC lacks it, and prefer out-of-band error signaling for size_t (see Return values). Naming \u00b6 Follow Google naming conventions in C++ files. In C files, use the following naming conventions for consistency with existing OpenSSL and C styles: Define structs with typedef named TYPE_NAME . The corresponding struct should be named struct type_name_st . Name public functions as MODULE_function_name , unless the module already uses a different naming scheme for legacy reasons. The module name should be a type name if the function is a method of a particular type. Some types are allocated within the library while others are initialized into a struct allocated by the caller, often on the stack. Name these functions TYPE_NAME_new / TYPE_NAME_free and TYPE_NAME_init / TYPE_NAME_cleanup , respectively. All TYPE_NAME_free functions must do nothing on NULL input. If a variable is the length of a pointer value, it has the suffix _len . An output parameter is named out or has an out_ prefix. For instance, For instance: uint8_t *out, size_t *out_len, const uint8_t *in, size_t in_len, Name public headers like include/openssl/evp.h with header guards like OPENSSL_HEADER_EVP_H . Name internal headers like crypto/ec/internal.h with header guards like OPENSSL_HEADER_EC_INTERNAL_H . Name enums like enum unix_hacker_t . For instance: enum should_free_handshake_buffer_t { free_handshake_buffer, dont_free_handshake_buffer, }; Return values \u00b6 As even malloc may fail in BoringSSL, the vast majority of functions will have a failure case. Functions should return int with one on success and zero on error. Do not overload the return value to both signal success/failure and output an integer. For example: OPENSSL_EXPORT int CBS_get_u16(CBS *cbs, uint16_t *out); If a function needs more than a true/false result code, define an enum rather than arbitrarily assigning meaning to int values. If a function outputs a pointer to an object on success and there are no other outputs, return the pointer directly and NULL on error. Parameters \u00b6 Where not constrained by legacy code, parameter order should be: context parameters output parameters input parameters For example, /* CBB_add_asn sets |*out_contents| to a |CBB| into which the contents of an * ASN.1 object can be written. The |tag| argument will be used as the tag for * the object. It returns one on success or zero on error. */ OPENSSL_EXPORT int CBB_add_asn1(CBB *cbb, CBB *out_contents, unsigned tag); Documentation \u00b6 All public symbols must have a documentation comment in their header file. The style is based on that of Go. The first sentence begins with the symbol name, optionally prefixed with \"A\" or \"An\". Apart from the initial mention of symbol, references to other symbols or parameter names should be surrounded by |pipes|. Documentation should be concise but completely describe the exposed behavior of the function. Pay special note to success/failure behaviors and caller obligations on object lifetimes. If this sacrifices conciseness, consider simplifying the function's behavior. /* EVP_DigestVerifyUpdate appends |len| bytes from |data| to the data which * will be verified by |EVP_DigestVerifyFinal|. It returns one on success and * zero otherwise. */ OPENSSL_EXPORT int EVP_DigestVerifyUpdate(EVP_MD_CTX *ctx, const void *data, size_t len); Explicitly mention any surprising edge cases or deviations from common return value patterns in legacy functions. /* RSA_private_encrypt encrypts |flen| bytes from |from| with the private key in * |rsa| and writes the encrypted data to |to|. The |to| buffer must have at * least |RSA_size| bytes of space. It returns the number of bytes written, or * -1 on error. The |padding| argument must be one of the |RSA_*_PADDING| * values. If in doubt, |RSA_PKCS1_PADDING| is the most common. * * WARNING: this function is dangerous because it breaks the usual return value * convention. Use |RSA_sign_raw| instead. */ OPENSSL_EXPORT int RSA_private_encrypt(int flen, const uint8_t *from, uint8_t *to, RSA *rsa, int padding); Document private functions in their internal.h header or, if static, where defined.","title":"STYLE"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#boringssl-style-guide","text":"BoringSSL usually follows the Google C++ style guide , The rest of this document describes differences and clarifications on top of the base guide.","title":"BoringSSL Style Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#legacy-code","text":"As a derivative of OpenSSL, BoringSSL contains a lot of legacy code that does not follow this style guide. Particularly where public API is concerned, balance consistency within a module with the benefits of a given rule. Module-wide deviations on naming should be respected while integer and return value conventions take precedence over consistency. Modules from OpenSSL's legacy ASN.1 and X.509 stack are retained for compatibility and left largely unmodified. To ease importing patches from upstream, they match OpenSSL's new indentation style. For Emacs, doc/openssl-c-indent.el from OpenSSL may be helpful in this.","title":"Legacy code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#language","text":"The majority of the project is in C, so C++-specific rules in the Google style guide do not apply. Support for C99 features depends on our target platforms. Typically, Chromium's target MSVC is the most restrictive. Variable declarations in the middle of a function or inside a for loop are allowed and preferred where possible. Note that the common goto err cleanup pattern requires lifting some variable declarations. Comments should be /* C-style */ for consistency. When declaration pointer types, * should be placed next to the variable name, not the type. So uint8_t *ptr; not uint8_t* ptr; Rather than malloc() and free() , use the wrappers OPENSSL_malloc() and OPENSSL_free() . Use the standard C assert() function freely. Use the following wrappers, found in crypto/internal.h instead of the corresponding C standard library functions. They behave the same but avoid confusing undefined behavior. OPENSSL_memchr OPENSSL_memcmp OPENSSL_memcpy OPENSSL_memmove OPENSSL_memset For new constants, prefer enums when the values are sequential and typed constants for flags. If adding values to an existing set of #define s, continue with #define .","title":"Language"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#formatting","text":"Single-statement blocks are not allowed. All conditions and loops must use braces: if (foo) { do_something(); } not if (foo) do_something();","title":"Formatting"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#integers","text":"Prefer using explicitly-sized integers where appropriate rather than generic C ones. For instance, to represent a byte, use uint8_t , not unsigned char . Likewise, represent a two-byte field as uint16_t , not unsigned short . Sizes are represented as size_t . Within a struct that is retained across the lifetime of an SSL connection, if bounds of a size are known and it's easy, use a smaller integer type like uint8_t . This is a \"free\" connection footprint optimization for servers. Don't make code significantly more complex for it, and do still check the bounds when passing in and out of the struct. This narrowing should not propagate to local variables and function parameters. When doing arithmetic, account for overflow conditions. Except with platform APIs, do not use ssize_t . MSVC lacks it, and prefer out-of-band error signaling for size_t (see Return values).","title":"Integers"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#naming","text":"Follow Google naming conventions in C++ files. In C files, use the following naming conventions for consistency with existing OpenSSL and C styles: Define structs with typedef named TYPE_NAME . The corresponding struct should be named struct type_name_st . Name public functions as MODULE_function_name , unless the module already uses a different naming scheme for legacy reasons. The module name should be a type name if the function is a method of a particular type. Some types are allocated within the library while others are initialized into a struct allocated by the caller, often on the stack. Name these functions TYPE_NAME_new / TYPE_NAME_free and TYPE_NAME_init / TYPE_NAME_cleanup , respectively. All TYPE_NAME_free functions must do nothing on NULL input. If a variable is the length of a pointer value, it has the suffix _len . An output parameter is named out or has an out_ prefix. For instance, For instance: uint8_t *out, size_t *out_len, const uint8_t *in, size_t in_len, Name public headers like include/openssl/evp.h with header guards like OPENSSL_HEADER_EVP_H . Name internal headers like crypto/ec/internal.h with header guards like OPENSSL_HEADER_EC_INTERNAL_H . Name enums like enum unix_hacker_t . For instance: enum should_free_handshake_buffer_t { free_handshake_buffer, dont_free_handshake_buffer, };","title":"Naming"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#return-values","text":"As even malloc may fail in BoringSSL, the vast majority of functions will have a failure case. Functions should return int with one on success and zero on error. Do not overload the return value to both signal success/failure and output an integer. For example: OPENSSL_EXPORT int CBS_get_u16(CBS *cbs, uint16_t *out); If a function needs more than a true/false result code, define an enum rather than arbitrarily assigning meaning to int values. If a function outputs a pointer to an object on success and there are no other outputs, return the pointer directly and NULL on error.","title":"Return values"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#parameters","text":"Where not constrained by legacy code, parameter order should be: context parameters output parameters input parameters For example, /* CBB_add_asn sets |*out_contents| to a |CBB| into which the contents of an * ASN.1 object can be written. The |tag| argument will be used as the tag for * the object. It returns one on success or zero on error. */ OPENSSL_EXPORT int CBB_add_asn1(CBB *cbb, CBB *out_contents, unsigned tag);","title":"Parameters"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/STYLE/#documentation","text":"All public symbols must have a documentation comment in their header file. The style is based on that of Go. The first sentence begins with the symbol name, optionally prefixed with \"A\" or \"An\". Apart from the initial mention of symbol, references to other symbols or parameter names should be surrounded by |pipes|. Documentation should be concise but completely describe the exposed behavior of the function. Pay special note to success/failure behaviors and caller obligations on object lifetimes. If this sacrifices conciseness, consider simplifying the function's behavior. /* EVP_DigestVerifyUpdate appends |len| bytes from |data| to the data which * will be verified by |EVP_DigestVerifyFinal|. It returns one on success and * zero otherwise. */ OPENSSL_EXPORT int EVP_DigestVerifyUpdate(EVP_MD_CTX *ctx, const void *data, size_t len); Explicitly mention any surprising edge cases or deviations from common return value patterns in legacy functions. /* RSA_private_encrypt encrypts |flen| bytes from |from| with the private key in * |rsa| and writes the encrypted data to |to|. The |to| buffer must have at * least |RSA_size| bytes of space. It returns the number of bytes written, or * -1 on error. The |padding| argument must be one of the |RSA_*_PADDING| * values. If in doubt, |RSA_PKCS1_PADDING| is the most common. * * WARNING: this function is dangerous because it breaks the usual return value * convention. Use |RSA_sign_raw| instead. */ OPENSSL_EXPORT int RSA_private_encrypt(int flen, const uint8_t *from, uint8_t *to, RSA *rsa, int padding); Document private functions in their internal.h header or, if static, where defined.","title":"Documentation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/infra/config/","text":"Infrastructure location. This contains Commit Queue config for automatic testing of changes before they are committed. Initial set up bug: http://crbug.com/618641 .","title":"config"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/","text":"BoringSSL SSL Tests \u00b6 This directory contains BoringSSL's protocol-level test suite. Testing a TLS implementation can be difficult. We need to produce invalid but sufficiently correct handshakes to get our implementation close to its edge cases. TLS's cryptographic steps mean we cannot use a transcript and effectively need a TLS implementation on the other end. But we do not wish to litter BoringSSL with options for bugs to test against. Instead, we use a fork of the Go crypto/tls package, heavily patched with configurable bugs. This code, along with a test suite and harness written in Go, lives in the runner directory. The harness runs BoringSSL via a C/C++ shim binary which lives in this directory. All communication with the shim binary occurs with command-line flags, sockets, and standard I/O. This strategy also ensures we always test against a second implementation. All features should be implemented twice, once in C for BoringSSL and once in Go for testing. If possible, the Go code should be suitable for potentially upstreaming. However, sometimes test code has different needs. For example, our test DTLS code enforces strict ordering on sequence numbers and has controlled packet drop simulation. To run the tests manually, run go test from the runner directory. It takes command-line flags found at the top of runner/runner.go . The -help option also works after using go test -c to make a runner.test binary first. If adding a new test, these files may be a good starting point: runner/runner.go : the test harness and all the individual tests. runner/common.go : contains the Config and ProtocolBugs struct which control the Go TLS implementation's behavior. test_config.h , test_config.cc : the command-line flags which control the shim's behavior. bssl_shim.cc : the shim binary itself. For porting the test suite to a different implementation see PORTING.md .","title":"README"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/#boringssl-ssl-tests","text":"This directory contains BoringSSL's protocol-level test suite. Testing a TLS implementation can be difficult. We need to produce invalid but sufficiently correct handshakes to get our implementation close to its edge cases. TLS's cryptographic steps mean we cannot use a transcript and effectively need a TLS implementation on the other end. But we do not wish to litter BoringSSL with options for bugs to test against. Instead, we use a fork of the Go crypto/tls package, heavily patched with configurable bugs. This code, along with a test suite and harness written in Go, lives in the runner directory. The harness runs BoringSSL via a C/C++ shim binary which lives in this directory. All communication with the shim binary occurs with command-line flags, sockets, and standard I/O. This strategy also ensures we always test against a second implementation. All features should be implemented twice, once in C for BoringSSL and once in Go for testing. If possible, the Go code should be suitable for potentially upstreaming. However, sometimes test code has different needs. For example, our test DTLS code enforces strict ordering on sequence numbers and has controlled packet drop simulation. To run the tests manually, run go test from the runner directory. It takes command-line flags found at the top of runner/runner.go . The -help option also works after using go test -c to make a runner.test binary first. If adding a new test, these files may be a good starting point: runner/runner.go : the test harness and all the individual tests. runner/common.go : contains the Config and ProtocolBugs struct which control the Go TLS implementation's behavior. test_config.h , test_config.cc : the command-line flags which control the shim's behavior. bssl_shim.cc : the shim binary itself. For porting the test suite to a different implementation see PORTING.md .","title":"BoringSSL SSL Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/","text":"Porting to Other Implementations \u00b6 Introduction \u00b6 This document provides an overview of the test runner and how to integrate it with other stacks. So far we have it working with BoringSSL and some incomplete integrations with NSS and OpenSSL. Note that supporting non-BoringSSL implementations is a work in progress and interfaces may change in the future. Consumers should pin to a particular revision rather than using BoringSSL\u2019s master branch directly. As we gain experience with other implementations, we hope to make further improvements to portability, so please contact davidben@google.com and ekr@rtfm.com if implementing a new shim. Integration Architecture \u00b6 The test runner integrates with the TLS stack under test through a \u201cshim\u201d: a command line program which encapsulates the stack. By default, the shim points to the BoringSSL shim in the same source tree, but any program can be supplied via the -shim-path flag. The runner opens up a server socket and provides the shim with a -port argument that points to that socket. The shim always connects to the runner as a TCP client even when acting as a TLS server. For DTLS, there is a small framing layer that gives packet boundaries over TCP. The shim can also pass a variety of command line arguments which are used to configure the stack under test. These can be found at test_config.cc . The shim reports success by exiting with a 0 error code and failure by reporting a non-zero error code and generally sending a textual error value to stderr. Many of the tests expect specific error string (such as NO_SHARED_CIPHER ) that indicates what went wrong. Compatibility Issues \u00b6 There are a number of situations in which the runner might succeed with some tests and not others: Defects in the stack under test Features which haven\u2019t yet been implemented Failure to implement one or more of the command line flags the runner uses with the shim Disagreement about the right behavior/interpretation of the spec We have implemented several features which allow implementations to ease these compatibility issues. Configuration File \u00b6 The runner can be supplied with a JSON configuration file which is intended to allow for a per-stack mapping. This file currently takes two directives: DisabledTests : A JSON map consisting of the pattern matching the tests to be disabled as the key and some sort of reason why it was disabled as the value. The key is used as a match against the test name. The value is ignored and is just used for documentation purposes so you can remember why you disabled a test. -include-disabled overrides this filter. ErrorMap : A JSON map from the internal errors the runner expects to the error strings that your implementation spits out. Generally you\u2019ll need to map every error, but if you also provide the -loose-errors flag, then every un-mapped error just gets mapped to the empty string and treated as if it matched every error the runner expects. The -shim-config flag is used to provide the config file. Unimplemented Features \u00b6 If the shim encounters some request from the runner that it knows it can\u2019t fulfill (e.g., a command line flag that it doesn\u2019t recognize), then it can exit with the special code 89 . Shims are recommended to use this exit code on unknown command-line arguments. The test runner interprets this as \u201cunimplemented\u201d and skips the test. If run normally, this will cause the test runner to report that the entire test suite failed. The -allow-unimplemented flag suppresses this behavior and causes the test runner to ignore these tests for the purpose of evaluating the success or failure of the test suite. Malloc Tests \u00b6 The test runner can also be used to stress malloc failure codepaths. If passed -malloc-test=0 , the runner will run each test repeatedly with an incrementing MALLOC_NUMBER_TO_FAIL environment variable. The shim should then replace the malloc implementation with one which fails at the specified number of calls. If there are not enough calls to reach the number, the shim should fail with exit code 88 . This signals to the runner that the test has completed. See crypto/test/malloc.cc for an example malloc implementation. Note these tests are slow and will hit Go's test timeout. Pass -timeout 72h to avoid crashing after 10 minutes. Example: Running Against NSS \u00b6 DYLD_LIBRARY_PATH=~/dev/nss-dev/nss-sandbox/dist/Darwin15.6.0_64_DBG.OBJ/lib go test -shim-path ~/dev/nss-dev/nss-sandbox/dist/Darwin15.6.0_64_DBG.OBJ/bin/nss_bogo_shim -loose-errors -allow-unimplemented -shim-config ~/dev/nss-dev/nss-sandbox/nss/external_tests/nss_bogo_shim/config.json","title":"PORTING"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#porting-to-other-implementations","text":"","title":"Porting to Other Implementations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#introduction","text":"This document provides an overview of the test runner and how to integrate it with other stacks. So far we have it working with BoringSSL and some incomplete integrations with NSS and OpenSSL. Note that supporting non-BoringSSL implementations is a work in progress and interfaces may change in the future. Consumers should pin to a particular revision rather than using BoringSSL\u2019s master branch directly. As we gain experience with other implementations, we hope to make further improvements to portability, so please contact davidben@google.com and ekr@rtfm.com if implementing a new shim.","title":"Introduction"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#integration-architecture","text":"The test runner integrates with the TLS stack under test through a \u201cshim\u201d: a command line program which encapsulates the stack. By default, the shim points to the BoringSSL shim in the same source tree, but any program can be supplied via the -shim-path flag. The runner opens up a server socket and provides the shim with a -port argument that points to that socket. The shim always connects to the runner as a TCP client even when acting as a TLS server. For DTLS, there is a small framing layer that gives packet boundaries over TCP. The shim can also pass a variety of command line arguments which are used to configure the stack under test. These can be found at test_config.cc . The shim reports success by exiting with a 0 error code and failure by reporting a non-zero error code and generally sending a textual error value to stderr. Many of the tests expect specific error string (such as NO_SHARED_CIPHER ) that indicates what went wrong.","title":"Integration Architecture"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#compatibility-issues","text":"There are a number of situations in which the runner might succeed with some tests and not others: Defects in the stack under test Features which haven\u2019t yet been implemented Failure to implement one or more of the command line flags the runner uses with the shim Disagreement about the right behavior/interpretation of the spec We have implemented several features which allow implementations to ease these compatibility issues.","title":"Compatibility Issues"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#configuration-file","text":"The runner can be supplied with a JSON configuration file which is intended to allow for a per-stack mapping. This file currently takes two directives: DisabledTests : A JSON map consisting of the pattern matching the tests to be disabled as the key and some sort of reason why it was disabled as the value. The key is used as a match against the test name. The value is ignored and is just used for documentation purposes so you can remember why you disabled a test. -include-disabled overrides this filter. ErrorMap : A JSON map from the internal errors the runner expects to the error strings that your implementation spits out. Generally you\u2019ll need to map every error, but if you also provide the -loose-errors flag, then every un-mapped error just gets mapped to the empty string and treated as if it matched every error the runner expects. The -shim-config flag is used to provide the config file.","title":"Configuration File"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#unimplemented-features","text":"If the shim encounters some request from the runner that it knows it can\u2019t fulfill (e.g., a command line flag that it doesn\u2019t recognize), then it can exit with the special code 89 . Shims are recommended to use this exit code on unknown command-line arguments. The test runner interprets this as \u201cunimplemented\u201d and skips the test. If run normally, this will cause the test runner to report that the entire test suite failed. The -allow-unimplemented flag suppresses this behavior and causes the test runner to ignore these tests for the purpose of evaluating the success or failure of the test suite.","title":"Unimplemented Features"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#malloc-tests","text":"The test runner can also be used to stress malloc failure codepaths. If passed -malloc-test=0 , the runner will run each test repeatedly with an incrementing MALLOC_NUMBER_TO_FAIL environment variable. The shim should then replace the malloc implementation with one which fails at the specified number of calls. If there are not enough calls to reach the number, the shim should fail with exit code 88 . This signals to the runner that the test has completed. See crypto/test/malloc.cc for an example malloc implementation. Note these tests are slow and will hit Go's test timeout. Pass -timeout 72h to avoid crashing after 10 minutes.","title":"Malloc Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/ssl/test/PORTING/#example-running-against-nss","text":"DYLD_LIBRARY_PATH=~/dev/nss-dev/nss-sandbox/dist/Darwin15.6.0_64_DBG.OBJ/lib go test -shim-path ~/dev/nss-dev/nss-sandbox/dist/Darwin15.6.0_64_DBG.OBJ/bin/nss_bogo_shim -loose-errors -allow-unimplemented -shim-config ~/dev/nss-dev/nss-sandbox/nss/external_tests/nss_bogo_shim/config.json","title":"Example: Running Against NSS"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/","text":"android-cmake \u00b6 CMake is great, and so is Android. This is a collection of CMake scripts that may be useful to the Android NDK community. It is based on experience from porting OpenCV library to Android: http://opencv.org/platforms/android.html Main goal is to share these scripts so that devs that use CMake as their build system may easily compile native code for Android. TL;DR \u00b6 cmake -DCMAKE_TOOLCHAIN_FILE=android.toolchain.cmake \\ -DANDROID_NDK=<ndk_path> \\ -DCMAKE_BUILD_TYPE=Release \\ -DANDROID_ABI=\"armeabi-v7a with NEON\" \\ <source_path> cmake --build . One-liner: cmake -DCMAKE_TOOLCHAIN_FILE=android.toolchain.cmake -DANDROID_NDK=<ndk_path> -DCMAKE_BUILD_TYPE=Release -DANDROID_ABI=\"armeabi-v7a with NEON\" <source_path> && cmake --build . android-cmake will search for your NDK install in the following order: Value of ANDROID_NDK CMake variable; Value of ANDROID_NDK environment variable; Search under paths from ANDROID_NDK_SEARCH_PATHS CMake variable; Search platform specific locations (home folder, Windows \"Program Files\", etc). So if you have installed the NDK as ~/android-ndk-r10d then android-cmake will locate it automatically. Getting started \u00b6 To build a cmake-based C/C++ project for Android you need: Android NDK (>= r5) http://developer.android.com/tools/sdk/ndk/index.html CMake (>= v2.6.3, >= v2.8.9 recommended) http://www.cmake.org/download The android-cmake is also capable to build with NDK from AOSP or Linaro Android source tree, but you may be required to manually specify path to libm binary to link with. Difference from traditional CMake \u00b6 Folowing the ndk-build the android-cmake supports only two build targets : -DCMAKE_BUILD_TYPE=Release -DCMAKE_BUILD_TYPE=Debug So don't even try other targets that can be found in CMake documentation and don't forget to explicitly specify Release or Debug because CMake builds without a build configuration by default. Difference from ndk-build \u00b6 Latest GCC available in NDK is used as the default compiler; Release builds with -O3 instead of -Os ; Release builds without debug info (without -g ) (because ndk-build always creates a stripped version but cmake delays this for install/strip target); -fsigned-char is added to compiler flags to make char signed by default as it is on x86/x86_64; GCC's stack protector is not used neither in Debug nor Release configurations; No builds for multiple platforms (e.g. building for both arm and x86 require to run cmake twice with different parameters); No file level Neon via .neon suffix; The following features of ndk-build are not supported by the android-cmake yet: armeabi-v7a-hard ABI libc++_static / libc++_shared STL runtime Basic options \u00b6 Similarly to the NDK build system android-cmake allows to select between several compiler toolchains and target platforms. Most of the options can be set either as cmake arguments: -D<NAME>=<VALUE> or as environment variables: ANDROID_NDK - path to the Android NDK. If not set then android-cmake will search for the most recent version of supported NDK in commonly used locations; ANDROID_ABI - specifies the target Application Binary Interface (ABI). This option nearly matches to the APP_ABI variable used by ndk-build tool from Android NDK. If not specified then set to armeabi-v7a . Possible target names are: armeabi - ARMv5TE based CPU with software floating point operations; armeabi-v7a - ARMv7 based devices with hardware FPU instructions (VFPv3_D16); armeabi-v7a with NEON - same as armeabi-v7a, but sets NEON as floating-point unit; armeabi-v7a with VFPV3 - same as armeabi-v7a, but sets VFPv3_D32 as floating-point unit; armeabi-v6 with VFP - tuned for ARMv6 processors having VFP; x86 - IA-32 instruction set mips - MIPS32 instruction set arm64-v8a - ARMv8 AArch64 instruction set - only for NDK r10 and newer x86_64 - Intel64 instruction set (r1) - only for NDK r10 and newer mips64 - MIPS64 instruction set (r6) - only for NDK r10 and newer ANDROID_NATIVE_API_LEVEL - level of android API to build for. Can be set either to full name (example: android-8 ) or a numeric value (example: 17 ). The default API level depends on the target ABI: android-8 for ARM; android-9 for x86 and MIPS; android-21 for 64-bit ABIs. Building for android-L is possible only when it is explicitly selected. * ANDROID_TOOLCHAIN_NAME - the name of compiler toolchain to be used. This option allows to select between different GCC and Clang versions. The list of possible values depends on the NDK version and will be printed by toolchain file if an invalid value is set. By default android-cmake selects the most recent version of GCC which can build for specified ANDROID_ABI . Example values are: * aarch64-linux-android-4.9 * aarch64-linux-android-clang3.5 * arm-linux-androideabi-4.8 * arm-linux-androideabi-4.9 * arm-linux-androideabi-clang3.5 * mips64el-linux-android-4.9 * mipsel-linux-android-4.8 * x86-4.9 * x86_64-4.9 * etc. * ANDROID_STL - the name of C++ runtime to use. The default is gnustl_static . * none - do not configure the runtime. * system - use the default minimal system C++ runtime library. * Implies -fno-rtti -fno-exceptions . * system_re - use the default minimal system C++ runtime library. * Implies -frtti -fexceptions . * gabi++_static - use the GAbi++ runtime as a static library. * Implies -frtti -fno-exceptions . * Available for NDK r7 and newer. * gabi++_shared - use the GAbi++ runtime as a shared library. * Implies -frtti -fno-exceptions . * Available for NDK r7 and newer. * stlport_static - use the STLport runtime as a static library. * Implies -fno-rtti -fno-exceptions for NDK before r7. * Implies -frtti -fno-exceptions for NDK r7 and newer. * stlport_shared - use the STLport runtime as a shared library. * Implies -fno-rtti -fno-exceptions for NDK before r7. * Implies -frtti -fno-exceptions for NDK r7 and newer. * gnustl_static - use the GNU STL as a static library. * Implies -frtti -fexceptions . * gnustl_shared - use the GNU STL as a shared library. * Implies -frtti -fno-exceptions . * Available for NDK r7b and newer. * Silently degrades to gnustl_static if not available. * NDK_CCACHE - path to ccache executable. If not set then initialized from NDK_CCACHE environment variable. Advanced android-cmake options \u00b6 Normally android-cmake users are not supposed to touch these variables but they might be useful to workaround some build issues: ANDROID_FORCE_ARM_BUILD = OFF - generate 32-bit ARM instructions instead of Thumb. Applicable only for arm ABIs and is forced to be ON for armeabi-v6 with VFP ; ANDROID_NO_UNDEFINED = ON - show all undefined symbols as linker errors; ANDROID_SO_UNDEFINED = OFF - allow undefined symbols in shared libraries; actually it is turned ON by default for NDK older than r7 ANDROID_STL_FORCE_FEATURES = ON - automatically configure rtti and exceptions support based on C++ runtime; ANDROID_NDK_LAYOUT = RELEASE - inner layout of Android NDK, should be detected automatically. Possible values are: RELEASE - public releases from Google; LINARO - NDK from Linaro project; ANDROID - NDK from AOSP. ANDROID_FUNCTION_LEVEL_LINKING = ON - enables saparate putting each function and data items into separate sections and enable garbage collection of unused input sections at link time ( -fdata-sections -ffunction-sections -Wl,--gc-sections ); ANDROID_GOLD_LINKER = ON - use gold linker with GCC 4.6 for NDK r8b and newer (only for ARM and x86); ANDROID_NOEXECSTACK = ON - enables or disables stack execution protection code ( -Wl,-z,noexecstack ); ANDROID_RELRO = ON - Enables RELRO - a memory corruption mitigation technique ( -Wl,-z,relro -Wl,-z,now ); ANDROID_LIBM_PATH - path to libm.so (set to something like $(TOP)/out/target/product/<product_name>/obj/lib/libm.so ) to workaround unresolved sincos . Fine-tuning CMakeLists.txt for android-cmake \u00b6 Recognizing Android build \u00b6 android-cmake defines ANDROID CMake variable which can be used to add Android-specific stuff: if (ANDROID) message(STATUS \"Hello from Android build!\") endif() The recommended way to identify ARM/MIPS/x86 architecture is examining CMAKE_SYSTEM_PROCESSOR which is set to the appropriate value: armv5te - for armeabi ABI armv6 - for armeabi-v6 with VFP ABI armv7-a - for armeabi-v7a , armeabi-v7a with VFPV3 and armeabi-v7a with NEON ABIs aarch64 - for arm64-v8a ABI i686 - for x86 ABI x86_64 - for x86_64 ABI mips - for mips ABI mips64 - for mips64 ABI Other variables that are set by android-cmake and can be used for the fine-grained build configuration are: NEON - set if target ABI supports Neon; ANDROID_NATIVE_API_LEVEL - native Android API level we are building for (note: Java part of Andoid application can be built for another API level) ANDROID_NDK_RELEASE - version of the Android NDK ANDROID_NDK_HOST_SYSTEM_NAME - \"windows\", \"linux-x86\" or \"darwin-x86\" depending on the host platform ANDROID_RTTI - set if rtti is enabled by the runtime ANDROID_EXCEPTIONS - set if exceptions are enabled by the runtime Finding packages \u00b6 When crosscompiling CMake find_* commands are normally expected to find libraries and packages belonging to the same build target. So android-cmake configures CMake to search in Android-specific paths only and ignore your host system locations. So find_package(ZLIB) will surely find libz.so within the Android NDK. However sometimes you need to locate a host package even when cross-compiling. For example you can be searching for your documentation generator. The android-cmake recommends you to use find_host_package and find_host_program macro defined in the android.toolchain.cmake : find_host_package(Doxygen) find_host_program(PDFLATEX pdflatex) However this will break regular builds so instead of wrapping package search into platform-specific logic you can copy the following snippet into your project (put it after your top-level project() command): # Search packages for host system instead of packages for target system # in case of cross compilation these macro should be defined by toolchain file if(NOT COMMAND find_host_package) macro(find_host_package) find_package( ${ ARGN } ) endmacro() endif() if(NOT COMMAND find_host_program) macro(find_host_program) find_program( ${ ARGN } ) endmacro() endif() Compiler flags recycling \u00b6 Make sure to do the following in your scripts: set(CMAKE_C_FLAGS \" ${ CMAKE_C_FLAGS } ${ my_cxx_flags } \") set(CMAKE_CXX_FLAGS \" ${ CMAKE_CXX_FLAGS } ${ my_cxx_flags } \") The flags will be prepopulated with critical flags, so don't loose them. Also be aware that android-cmake also sets configuration-specific compiler and linker flags. Troubleshooting \u00b6 Building on Windows \u00b6 First of all cygwin builds are NOT supported and will not be supported by android-cmake . To build natively on Windows you need a port of make but I recommend http://martine.github.io/ninja/ instead. To build with Ninja you need: Ensure you are using CMake newer than 2.8.9; Download the latest Ninja from https://github.com/martine/ninja/releases ; Put the ninja.exe into your PATH (or add path to ninja.exe to your PATH environment variable); Pass -GNinja to cmake alongside with other arguments (or choose Ninja generator in cmake-gui ). Enjoy the fast native multithreaded build :) But if you still want to stick to old make then: Get a Windows port of GNU Make: Android NDK r7 (and newer) already has make.exe on board; mingw-make should work as fine; Download some other port. For example, this one: http://gnuwin32.sourceforge.net/packages/make.htm . Add path to your make.exe to system PATH or always use full path; Pass -G\"MinGW Makefiles\" and -DCMAKE_MAKE_PROGRAM=\"<full/path/to/>make.exe\" It must be MinGW Makefiles and not Unix Makefiles even if your make.exe is not a MinGW's make. Run make.exe or cmake --build . for single-threaded build. Projects with assembler files \u00b6 The android-cmake should correctly handle projects with assembler sources ( *.s or *.S ). But if you still facing problems with assembler then try to upgrade your CMake to version newer than 2.8.5 Copying \u00b6 android-cmake is distributed under the terms of BSD 3-Clause License","title":"README"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#android-cmake","text":"CMake is great, and so is Android. This is a collection of CMake scripts that may be useful to the Android NDK community. It is based on experience from porting OpenCV library to Android: http://opencv.org/platforms/android.html Main goal is to share these scripts so that devs that use CMake as their build system may easily compile native code for Android.","title":"android-cmake"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#tldr","text":"cmake -DCMAKE_TOOLCHAIN_FILE=android.toolchain.cmake \\ -DANDROID_NDK=<ndk_path> \\ -DCMAKE_BUILD_TYPE=Release \\ -DANDROID_ABI=\"armeabi-v7a with NEON\" \\ <source_path> cmake --build . One-liner: cmake -DCMAKE_TOOLCHAIN_FILE=android.toolchain.cmake -DANDROID_NDK=<ndk_path> -DCMAKE_BUILD_TYPE=Release -DANDROID_ABI=\"armeabi-v7a with NEON\" <source_path> && cmake --build . android-cmake will search for your NDK install in the following order: Value of ANDROID_NDK CMake variable; Value of ANDROID_NDK environment variable; Search under paths from ANDROID_NDK_SEARCH_PATHS CMake variable; Search platform specific locations (home folder, Windows \"Program Files\", etc). So if you have installed the NDK as ~/android-ndk-r10d then android-cmake will locate it automatically.","title":"TL;DR"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#getting-started","text":"To build a cmake-based C/C++ project for Android you need: Android NDK (>= r5) http://developer.android.com/tools/sdk/ndk/index.html CMake (>= v2.6.3, >= v2.8.9 recommended) http://www.cmake.org/download The android-cmake is also capable to build with NDK from AOSP or Linaro Android source tree, but you may be required to manually specify path to libm binary to link with.","title":"Getting started"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#difference-from-traditional-cmake","text":"Folowing the ndk-build the android-cmake supports only two build targets : -DCMAKE_BUILD_TYPE=Release -DCMAKE_BUILD_TYPE=Debug So don't even try other targets that can be found in CMake documentation and don't forget to explicitly specify Release or Debug because CMake builds without a build configuration by default.","title":"Difference from traditional CMake"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#difference-from-ndk-build","text":"Latest GCC available in NDK is used as the default compiler; Release builds with -O3 instead of -Os ; Release builds without debug info (without -g ) (because ndk-build always creates a stripped version but cmake delays this for install/strip target); -fsigned-char is added to compiler flags to make char signed by default as it is on x86/x86_64; GCC's stack protector is not used neither in Debug nor Release configurations; No builds for multiple platforms (e.g. building for both arm and x86 require to run cmake twice with different parameters); No file level Neon via .neon suffix; The following features of ndk-build are not supported by the android-cmake yet: armeabi-v7a-hard ABI libc++_static / libc++_shared STL runtime","title":"Difference from ndk-build"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#basic-options","text":"Similarly to the NDK build system android-cmake allows to select between several compiler toolchains and target platforms. Most of the options can be set either as cmake arguments: -D<NAME>=<VALUE> or as environment variables: ANDROID_NDK - path to the Android NDK. If not set then android-cmake will search for the most recent version of supported NDK in commonly used locations; ANDROID_ABI - specifies the target Application Binary Interface (ABI). This option nearly matches to the APP_ABI variable used by ndk-build tool from Android NDK. If not specified then set to armeabi-v7a . Possible target names are: armeabi - ARMv5TE based CPU with software floating point operations; armeabi-v7a - ARMv7 based devices with hardware FPU instructions (VFPv3_D16); armeabi-v7a with NEON - same as armeabi-v7a, but sets NEON as floating-point unit; armeabi-v7a with VFPV3 - same as armeabi-v7a, but sets VFPv3_D32 as floating-point unit; armeabi-v6 with VFP - tuned for ARMv6 processors having VFP; x86 - IA-32 instruction set mips - MIPS32 instruction set arm64-v8a - ARMv8 AArch64 instruction set - only for NDK r10 and newer x86_64 - Intel64 instruction set (r1) - only for NDK r10 and newer mips64 - MIPS64 instruction set (r6) - only for NDK r10 and newer ANDROID_NATIVE_API_LEVEL - level of android API to build for. Can be set either to full name (example: android-8 ) or a numeric value (example: 17 ). The default API level depends on the target ABI: android-8 for ARM; android-9 for x86 and MIPS; android-21 for 64-bit ABIs. Building for android-L is possible only when it is explicitly selected. * ANDROID_TOOLCHAIN_NAME - the name of compiler toolchain to be used. This option allows to select between different GCC and Clang versions. The list of possible values depends on the NDK version and will be printed by toolchain file if an invalid value is set. By default android-cmake selects the most recent version of GCC which can build for specified ANDROID_ABI . Example values are: * aarch64-linux-android-4.9 * aarch64-linux-android-clang3.5 * arm-linux-androideabi-4.8 * arm-linux-androideabi-4.9 * arm-linux-androideabi-clang3.5 * mips64el-linux-android-4.9 * mipsel-linux-android-4.8 * x86-4.9 * x86_64-4.9 * etc. * ANDROID_STL - the name of C++ runtime to use. The default is gnustl_static . * none - do not configure the runtime. * system - use the default minimal system C++ runtime library. * Implies -fno-rtti -fno-exceptions . * system_re - use the default minimal system C++ runtime library. * Implies -frtti -fexceptions . * gabi++_static - use the GAbi++ runtime as a static library. * Implies -frtti -fno-exceptions . * Available for NDK r7 and newer. * gabi++_shared - use the GAbi++ runtime as a shared library. * Implies -frtti -fno-exceptions . * Available for NDK r7 and newer. * stlport_static - use the STLport runtime as a static library. * Implies -fno-rtti -fno-exceptions for NDK before r7. * Implies -frtti -fno-exceptions for NDK r7 and newer. * stlport_shared - use the STLport runtime as a shared library. * Implies -fno-rtti -fno-exceptions for NDK before r7. * Implies -frtti -fno-exceptions for NDK r7 and newer. * gnustl_static - use the GNU STL as a static library. * Implies -frtti -fexceptions . * gnustl_shared - use the GNU STL as a shared library. * Implies -frtti -fno-exceptions . * Available for NDK r7b and newer. * Silently degrades to gnustl_static if not available. * NDK_CCACHE - path to ccache executable. If not set then initialized from NDK_CCACHE environment variable.","title":"Basic options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#advanced-android-cmake-options","text":"Normally android-cmake users are not supposed to touch these variables but they might be useful to workaround some build issues: ANDROID_FORCE_ARM_BUILD = OFF - generate 32-bit ARM instructions instead of Thumb. Applicable only for arm ABIs and is forced to be ON for armeabi-v6 with VFP ; ANDROID_NO_UNDEFINED = ON - show all undefined symbols as linker errors; ANDROID_SO_UNDEFINED = OFF - allow undefined symbols in shared libraries; actually it is turned ON by default for NDK older than r7 ANDROID_STL_FORCE_FEATURES = ON - automatically configure rtti and exceptions support based on C++ runtime; ANDROID_NDK_LAYOUT = RELEASE - inner layout of Android NDK, should be detected automatically. Possible values are: RELEASE - public releases from Google; LINARO - NDK from Linaro project; ANDROID - NDK from AOSP. ANDROID_FUNCTION_LEVEL_LINKING = ON - enables saparate putting each function and data items into separate sections and enable garbage collection of unused input sections at link time ( -fdata-sections -ffunction-sections -Wl,--gc-sections ); ANDROID_GOLD_LINKER = ON - use gold linker with GCC 4.6 for NDK r8b and newer (only for ARM and x86); ANDROID_NOEXECSTACK = ON - enables or disables stack execution protection code ( -Wl,-z,noexecstack ); ANDROID_RELRO = ON - Enables RELRO - a memory corruption mitigation technique ( -Wl,-z,relro -Wl,-z,now ); ANDROID_LIBM_PATH - path to libm.so (set to something like $(TOP)/out/target/product/<product_name>/obj/lib/libm.so ) to workaround unresolved sincos .","title":"Advanced android-cmake options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#fine-tuning-cmakeliststxt-for-android-cmake","text":"","title":"Fine-tuning CMakeLists.txt for android-cmake"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#recognizing-android-build","text":"android-cmake defines ANDROID CMake variable which can be used to add Android-specific stuff: if (ANDROID) message(STATUS \"Hello from Android build!\") endif() The recommended way to identify ARM/MIPS/x86 architecture is examining CMAKE_SYSTEM_PROCESSOR which is set to the appropriate value: armv5te - for armeabi ABI armv6 - for armeabi-v6 with VFP ABI armv7-a - for armeabi-v7a , armeabi-v7a with VFPV3 and armeabi-v7a with NEON ABIs aarch64 - for arm64-v8a ABI i686 - for x86 ABI x86_64 - for x86_64 ABI mips - for mips ABI mips64 - for mips64 ABI Other variables that are set by android-cmake and can be used for the fine-grained build configuration are: NEON - set if target ABI supports Neon; ANDROID_NATIVE_API_LEVEL - native Android API level we are building for (note: Java part of Andoid application can be built for another API level) ANDROID_NDK_RELEASE - version of the Android NDK ANDROID_NDK_HOST_SYSTEM_NAME - \"windows\", \"linux-x86\" or \"darwin-x86\" depending on the host platform ANDROID_RTTI - set if rtti is enabled by the runtime ANDROID_EXCEPTIONS - set if exceptions are enabled by the runtime","title":"Recognizing Android build"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#finding-packages","text":"When crosscompiling CMake find_* commands are normally expected to find libraries and packages belonging to the same build target. So android-cmake configures CMake to search in Android-specific paths only and ignore your host system locations. So find_package(ZLIB) will surely find libz.so within the Android NDK. However sometimes you need to locate a host package even when cross-compiling. For example you can be searching for your documentation generator. The android-cmake recommends you to use find_host_package and find_host_program macro defined in the android.toolchain.cmake : find_host_package(Doxygen) find_host_program(PDFLATEX pdflatex) However this will break regular builds so instead of wrapping package search into platform-specific logic you can copy the following snippet into your project (put it after your top-level project() command): # Search packages for host system instead of packages for target system # in case of cross compilation these macro should be defined by toolchain file if(NOT COMMAND find_host_package) macro(find_host_package) find_package( ${ ARGN } ) endmacro() endif() if(NOT COMMAND find_host_program) macro(find_host_program) find_program( ${ ARGN } ) endmacro() endif()","title":"Finding packages"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#compiler-flags-recycling","text":"Make sure to do the following in your scripts: set(CMAKE_C_FLAGS \" ${ CMAKE_C_FLAGS } ${ my_cxx_flags } \") set(CMAKE_CXX_FLAGS \" ${ CMAKE_CXX_FLAGS } ${ my_cxx_flags } \") The flags will be prepopulated with critical flags, so don't loose them. Also be aware that android-cmake also sets configuration-specific compiler and linker flags.","title":"Compiler flags recycling"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#building-on-windows","text":"First of all cygwin builds are NOT supported and will not be supported by android-cmake . To build natively on Windows you need a port of make but I recommend http://martine.github.io/ninja/ instead. To build with Ninja you need: Ensure you are using CMake newer than 2.8.9; Download the latest Ninja from https://github.com/martine/ninja/releases ; Put the ninja.exe into your PATH (or add path to ninja.exe to your PATH environment variable); Pass -GNinja to cmake alongside with other arguments (or choose Ninja generator in cmake-gui ). Enjoy the fast native multithreaded build :) But if you still want to stick to old make then: Get a Windows port of GNU Make: Android NDK r7 (and newer) already has make.exe on board; mingw-make should work as fine; Download some other port. For example, this one: http://gnuwin32.sourceforge.net/packages/make.htm . Add path to your make.exe to system PATH or always use full path; Pass -G\"MinGW Makefiles\" and -DCMAKE_MAKE_PROGRAM=\"<full/path/to/>make.exe\" It must be MinGW Makefiles and not Unix Makefiles even if your make.exe is not a MinGW's make. Run make.exe or cmake --build . for single-threaded build.","title":"Building on Windows"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#projects-with-assembler-files","text":"The android-cmake should correctly handle projects with assembler sources ( *.s or *.S ). But if you still facing problems with assembler then try to upgrade your CMake to version newer than 2.8.5","title":"Projects with assembler files"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/#copying","text":"android-cmake is distributed under the terms of BSD 3-Clause License","title":"Copying"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/android-cmake/ndk_links/","text":"============== r1 ============== (dead links) http://dl.google.com/android/ndk/android-ndk-1.5_r1-windows.zip http://dl.google.com/android/ndk/android-ndk-1.5_r1-darwin-x86.zip http://dl.google.com/android/ndk/android-ndk-1.5_r1-linux-x86.zip ============== r2 ============== http://dl.google.com/android/ndk/android-ndk-1.6_r1-windows.zip http://dl.google.com/android/ndk/android-ndk-1.6_r1-darwin-x86.zip http://dl.google.com/android/ndk/android-ndk-1.6_r1-linux-x86.zip ============== r3 ============== http://dl.google.com/android/ndk/android-ndk-r3-windows.zip http://dl.google.com/android/ndk/android-ndk-r3-darwin-x86.zip http://dl.google.com/android/ndk/android-ndk-r3-linux-x86.zip ============== r4 ============== http://dl.google.com/android/ndk/android-ndk-r4-windows.zip http://dl.google.com/android/ndk/android-ndk-r4-darwin-x86.zip http://dl.google.com/android/ndk/android-ndk-r4-linux-x86.zip ============== r4b ============== http://dl.google.com/android/ndk/android-ndk-r4b-windows.zip http://dl.google.com/android/ndk/android-ndk-r4b-darwin-x86.zip http://dl.google.com/android/ndk/android-ndk-r4b-linux-x86.zip ============== r5 ============== http://dl.google.com/android/ndk/android-ndk-r5-windows.zip http://dl.google.com/android/ndk/android-ndk-r5-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r5-linux-x86.tar.bz2 ============== r5b ============== http://dl.google.com/android/ndk/android-ndk-r5b-windows.zip http://dl.google.com/android/ndk/android-ndk-r5b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r5b-linux-x86.tar.bz2 ============== r5c ============== http://dl.google.com/android/ndk/android-ndk-r5c-windows.zip http://dl.google.com/android/ndk/android-ndk-r5c-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r5c-linux-x86.tar.bz2 ============== r6 ============== http://dl.google.com/android/ndk/android-ndk-r6-windows.zip http://dl.google.com/android/ndk/android-ndk-r6-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r6-linux-x86.tar.bz2 ============== r6b ============== http://dl.google.com/android/ndk/android-ndk-r6b-windows.zip http://dl.google.com/android/ndk/android-ndk-r6b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r6b-linux-x86.tar.bz2 ============== r7 ============== http://dl.google.com/android/ndk/android-ndk-r7-windows.zip http://dl.google.com/android/ndk/android-ndk-r7-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r7-linux-x86.tar.bz2 ============== r7b ============== http://dl.google.com/android/ndk/android-ndk-r7b-windows.zip http://dl.google.com/android/ndk/android-ndk-r7b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r7b-linux-x86.tar.bz2 ============== r7c ============== http://dl.google.com/android/ndk/android-ndk-r7c-windows.zip http://dl.google.com/android/ndk/android-ndk-r7c-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r7c-linux-x86.tar.bz2 ============== r8 ============== http://dl.google.com/android/ndk/android-ndk-r8-windows.zip http://dl.google.com/android/ndk/android-ndk-r8-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8-linux-x86.tar.bz2 ============== r8b ============== http://dl.google.com/android/ndk/android-ndk-r8b-windows.zip http://dl.google.com/android/ndk/android-ndk-r8b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8b-linux-x86.tar.bz2 ============== r8c ============== http://dl.google.com/android/ndk/android-ndk-r8c-windows.zip http://dl.google.com/android/ndk/android-ndk-r8c-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8c-linux-x86.tar.bz2 ============== r8d ============== http://dl.google.com/android/ndk/android-ndk-r8d-windows.zip http://dl.google.com/android/ndk/android-ndk-r8d-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8d-linux-x86.tar.bz2 ============== r8e ============== http://dl.google.com/android/ndk/android-ndk-r8e-windows-x86.zip http://dl.google.com/android/ndk/android-ndk-r8e-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk-r8e-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8e-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8e-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r8e-linux-x86_64.tar.bz2 ============== r9 ============== http://dl.google.com/android/ndk/android-ndk-r9-windows-x86.zip http://dl.google.com/android/ndk/android-ndk-r9-windows-x86-legacy-toolchains.zip http://dl.google.com/android/ndk/android-ndk-r9-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk-r9-windows-x86_64-legacy-toolchains.zip http://dl.google.com/android/ndk/android-ndk-r9-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-darwin-x86-legacy-toolchains.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-darwin-x86_64-legacy-toolchains.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-linux-x86-legacy-toolchains.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9-linux-x86_64-legacy-toolchains.tar.bz2 ============== r9b ============== http://dl.google.com/android/ndk/android-ndk-r9b-windows-x86.zip http://dl.google.com/android/ndk/android-ndk-r9b-windows-x86-legacy-toolchains.zip http://dl.google.com/android/ndk/android-ndk-r9b-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk-r9b-windows-x86_64-legacy-toolchains.zip http://dl.google.com/android/ndk/android-ndk-r9b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-darwin-x86-legacy-toolchains.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-darwin-x86_64-legacy-toolchains.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-linux-x86-legacy-toolchains.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9b-linux-x86_64-legacy-toolchains.tar.bz2 ============== r9c ============== http://dl.google.com/android/ndk/android-ndk-r9c-windows-x86.zip http://dl.google.com/android/ndk/android-ndk-r9c-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk-r9c-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9c-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9c-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9c-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9c-cxx-stl-libs-with-debugging-info.zip ============== r9d ============== http://dl.google.com/android/ndk/android-ndk-r9d-windows-x86.zip http://dl.google.com/android/ndk/android-ndk-r9d-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk-r9d-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9d-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9d-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9d-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r9d-cxx-stl-libs-with-debug-info.zip ============== r10 ============== http://dl.google.com/android/ndk/android-ndk32-r10-windows-x86.zip http://dl.google.com/android/ndk/android-ndk32-r10-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk32-r10-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk32-r10-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk32-r10-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk32-r10-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10-windows-x86.zip http://dl.google.com/android/ndk/android-ndk64-r10-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk64-r10-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r10-cxx-stl-libs-with-debug-info.zip ============== r10b ============== http://dl.google.com/android/ndk/android-ndk32-r10b-windows-x86.zip http://dl.google.com/android/ndk/android-ndk32-r10b-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk32-r10b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk32-r10b-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk32-r10b-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk32-r10b-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10b-windows-x86.zip http://dl.google.com/android/ndk/android-ndk64-r10b-windows-x86_64.zip http://dl.google.com/android/ndk/android-ndk64-r10b-darwin-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10b-darwin-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10b-linux-x86.tar.bz2 http://dl.google.com/android/ndk/android-ndk64-r10b-linux-x86_64.tar.bz2 http://dl.google.com/android/ndk/android-ndk-r10b-cxx-stl-libs-with-debug-info.zip ============== r10c ============== http://dl.google.com/android/ndk/android-ndk-r10c-windows-x86.exe http://dl.google.com/android/ndk/android-ndk-r10c-windows-x86_64.exe http://dl.google.com/android/ndk/android-ndk-r10c-darwin-x86.bin http://dl.google.com/android/ndk/android-ndk-r10c-darwin-x86_64.bin http://dl.google.com/android/ndk/android-ndk-r10c-linux-x86.bin http://dl.google.com/android/ndk/android-ndk-r10c-linux-x86_64.bin ============== r10d ============== http://dl.google.com/android/ndk/android-ndk-r10d-windows-x86.exe http://dl.google.com/android/ndk/android-ndk-r10d-windows-x86_64.exe http://dl.google.com/android/ndk/android-ndk-r10d-darwin-x86.bin http://dl.google.com/android/ndk/android-ndk-r10d-darwin-x86_64.bin http://dl.google.com/android/ndk/android-ndk-r10d-linux-x86.bin http://dl.google.com/android/ndk/android-ndk-r10d-linux-x86_64.bin","title":"ndk links"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/","text":"Generic Build Instructions \u00b6 Setup \u00b6 To build Google Test and your tests that use it, you need to tell your build system where to find its headers and source files. The exact way to do it depends on which build system you use, and is usually straightforward. Build \u00b6 Suppose you put Google Test in directory ${GTEST_DIR} . To build it, create a library build target (or a project as called by Visual Studio and Xcode) to compile ${ GTEST_DIR } /src/gtest-all.cc with ${GTEST_DIR}/include in the system header search path and ${GTEST_DIR} in the normal header search path. Assuming a Linux-like system and gcc, something like the following will do: g++ -isystem ${ GTEST_DIR } /include -I ${ GTEST_DIR } \\ -pthread -c ${ GTEST_DIR } /src/gtest-all.cc ar -rv libgtest.a gtest-all.o (We need -pthread as Google Test uses threads.) Next, you should compile your test source file with ${GTEST_DIR}/include in the system header search path, and link it with gtest and any other necessary libraries: g++ -isystem ${ GTEST_DIR } /include -pthread path/to/your_test.cc libgtest.a \\ -o your_test As an example, the make/ directory contains a Makefile that you can use to build Google Test on systems where GNU make is available (e.g. Linux, Mac OS X, and Cygwin). It doesn't try to build Google Test's own tests. Instead, it just builds the Google Test library and a sample test. You can use it as a starting point for your own build script. If the default settings are correct for your environment, the following commands should succeed: cd ${ GTEST_DIR } /make make ./sample1_unittest If you see errors, try to tweak the contents of make/Makefile to make them go away. There are instructions in make/Makefile on how to do it. Using CMake \u00b6 Google Test comes with a CMake build script ( CMakeLists.txt ) that can be used on a wide range of platforms (\"C\" stands for cross-platform.). If you don't have CMake installed already, you can download it for free from http://www.cmake.org/ . CMake works by generating native makefiles or build projects that can be used in the compiler environment of your choice. You can either build Google Test as a standalone project or it can be incorporated into an existing CMake build for another project. Standalone CMake Project \u00b6 When building Google Test as a standalone project, the typical workflow starts with: mkdir mybuild # Create a directory to hold the build output. cd mybuild cmake ${ GTEST_DIR } # Generate native build scripts. If you want to build Google Test's samples, you should replace the last command with cmake -Dgtest_build_samples=ON ${ GTEST_DIR } If you are on a *nix system, you should now see a Makefile in the current directory. Just type 'make' to build gtest. If you use Windows and have Visual Studio installed, a gtest.sln file and several .vcproj files will be created. You can then build them using Visual Studio. On Mac OS X with Xcode installed, a .xcodeproj file will be generated. Incorporating Into An Existing CMake Project \u00b6 If you want to use gtest in a project which already uses CMake, then a more robust and flexible approach is to build gtest as part of that project directly. This is done by making the GoogleTest source code available to the main build and adding it using CMake's add_subdirectory() command. This has the significant advantage that the same compiler and linker settings are used between gtest and the rest of your project, so issues associated with using incompatible libraries (eg debug/release), etc. are avoided. This is particularly useful on Windows. Making GoogleTest's source code available to the main build can be done a few different ways: Download the GoogleTest source code manually and place it at a known location. This is the least flexible approach and can make it more difficult to use with continuous integration systems, etc. Embed the GoogleTest source code as a direct copy in the main project's source tree. This is often the simplest approach, but is also the hardest to keep up to date. Some organizations may not permit this method. Add GoogleTest as a git submodule or equivalent. This may not always be possible or appropriate. Git submodules, for example, have their own set of advantages and drawbacks. Use CMake to download GoogleTest as part of the build's configure step. This is just a little more complex, but doesn't have the limitations of the other methods. The last of the above methods is implemented with a small piece of CMake code in a separate file (e.g. CMakeLists.txt.in ) which is copied to the build area and then invoked as a sub-build during the CMake stage . That directory is then pulled into the main build with add_subdirectory() . For example: New file CMakeLists.txt.in : cmake_minimum_required ( VERSION 2.8.2 ) project ( googletest-download NONE ) include ( ExternalProject ) ExternalProject_Add ( googletest GIT_REPOSITORY https://github.com/google/googletest.git GIT_TAG master SOURCE_DIR \"${CMAKE_BINARY_DIR}/googletest-src\" BINARY_DIR \"${CMAKE_BINARY_DIR}/googletest-build\" CONFIGURE_COMMAND \"\" BUILD_COMMAND \"\" INSTALL_COMMAND \"\" TEST_COMMAND \"\" ) Existing build's CMakeLists.txt : # Download and unpack googletest at configure time configure_file(CMakeLists.txt.in googletest-download/CMakeLists.txt) execute_process(COMMAND ${ CMAKE_COMMAND } -G \" ${ CMAKE_GENERATOR } \" . RESULT_VARIABLE result WORKING_DIRECTORY ${ CMAKE_BINARY_DIR } /googletest-download ) if(result) message(FATAL_ERROR \"CMake step for googletest failed: ${ result } \") endif() execute_process(COMMAND ${ CMAKE_COMMAND } --build . RESULT_VARIABLE result WORKING_DIRECTORY ${ CMAKE_BINARY_DIR } /googletest-download ) if(result) message(FATAL_ERROR \"Build step for googletest failed: ${ result } \") endif() # Prevent overriding the parent project's compiler/linker # settings on Windows set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE) # Add googletest directly to our build. This defines # the gtest and gtest_main targets. add_subdirectory( ${ CMAKE_BINARY_DIR } /googletest-src ${ CMAKE_BINARY_DIR } /googletest-build) # The gtest/gtest_main targets carry header search path # dependencies automatically when using CMake 2.8.11 or # later. Otherwise we have to add them here ourselves. if (CMAKE_VERSION VERSION_LESS 2.8.11) include_directories(\" ${ gtest_SOURCE_DIR } /include\") endif() # Now simply link against gtest or gtest_main as needed. Eg add_executable(example example.cpp) target_link_libraries(example gtest_main) add_test(NAME example_test COMMAND example) Note that this approach requires CMake 2.8.2 or later due to its use of the ExternalProject_Add() command. The above technique is discussed in more detail in this separate article which also contains a link to a fully generalized implementation of the technique. Legacy Build Scripts \u00b6 Before settling on CMake, we have been providing hand-maintained build projects/scripts for Visual Studio, Xcode, and Autotools. While we continue to provide them for convenience, they are not actively maintained any more. We highly recommend that you follow the instructions in the above sections to integrate Google Test with your existing build system. If you still need to use the legacy build scripts, here's how: The msvc folder contains two solutions with Visual C++ projects. Open the gtest.sln or gtest-md.sln file using Visual Studio, and you are ready to build Google Test the same way you build any Visual Studio project. Files that have names ending with -md use DLL versions of Microsoft runtime libraries (the /MD or the /MDd compiler option). Files without that suffix use static versions of the runtime libraries (the /MT or the /MTd option). Please note that one must use the same option to compile both gtest and the test code. If you use Visual Studio 2005 or above, we recommend the -md version as /MD is the default for new projects in these versions of Visual Studio. On Mac OS X, open the gtest.xcodeproj in the xcode/ folder using Xcode. Build the \"gtest\" target. The universal binary framework will end up in your selected build directory (selected in the Xcode \"Preferences...\" -> \"Building\" pane and defaults to xcode/build). Alternatively, at the command line, enter: xcodebuild This will build the \"Release\" configuration of gtest.framework in your default build location. See the \"xcodebuild\" man page for more information about building different configurations and building in different locations. If you wish to use the Google Test Xcode project with Xcode 4.x and above, you need to either: update the SDK configuration options in xcode/Config/General.xconfig. Comment options SDKROOT , MACOS_DEPLOYMENT_TARGET , and GCC_VERSION . If you choose this route you lose the ability to target earlier versions of MacOS X. Install an SDK for an earlier version. This doesn't appear to be supported by Apple, but has been reported to work ( http://stackoverflow.com/questions/5378518 ). Tweaking Google Test \u00b6 Google Test can be used in diverse environments. The default configuration may not work (or may not work well) out of the box in some environments. However, you can easily tweak Google Test by defining control macros on the compiler command line. Generally, these macros are named like GTEST_XYZ and you define them to either 1 or 0 to enable or disable a certain feature. We list the most frequently used macros below. For a complete list, see file include/gtest/internal/gtest-port.h . Choosing a TR1 Tuple Library \u00b6 Some Google Test features require the C++ Technical Report 1 (TR1) tuple library, which is not yet available with all compilers. The good news is that Google Test implements a subset of TR1 tuple that's enough for its own need, and will automatically use this when the compiler doesn't provide TR1 tuple. Usually you don't need to care about which tuple library Google Test uses. However, if your project already uses TR1 tuple, you need to tell Google Test to use the same TR1 tuple library the rest of your project uses, or the two tuple implementations will clash. To do that, add -DGTEST_USE_OWN_TR1_TUPLE=0 to the compiler flags while compiling Google Test and your tests. If you want to force Google Test to use its own tuple library, just add -DGTEST_USE_OWN_TR1_TUPLE=1 to the compiler flags instead. If you don't want Google Test to use tuple at all, add -DGTEST_HAS_TR1_TUPLE=0 and all features using tuple will be disabled. Multi-threaded Tests \u00b6 Google Test is thread-safe where the pthread library is available. After #include \"gtest/gtest.h\" , you can check the GTEST_IS_THREADSAFE macro to see whether this is the case (yes if the macro is #defined to 1, no if it's undefined.). If Google Test doesn't correctly detect whether pthread is available in your environment, you can force it with -DGTEST_HAS_PTHREAD=1 or -DGTEST_HAS_PTHREAD=0 When Google Test uses pthread, you may need to add flags to your compiler and/or linker to select the pthread library, or you'll get link errors. If you use the CMake script or the deprecated Autotools script, this is taken care of for you. If you use your own build script, you'll need to read your compiler and linker's manual to figure out what flags to add. As a Shared Library (DLL) \u00b6 Google Test is compact, so most users can build and link it as a static library for the simplicity. You can choose to use Google Test as a shared library (known as a DLL on Windows) if you prefer. To compile gtest as a shared library, add -DGTEST_CREATE_SHARED_LIBRARY=1 to the compiler flags. You'll also need to tell the linker to produce a shared library instead - consult your linker's manual for how to do it. To compile your tests that use the gtest shared library, add -DGTEST_LINKED_AS_SHARED_LIBRARY=1 to the compiler flags. Note: while the above steps aren't technically necessary today when using some compilers (e.g. GCC), they may become necessary in the future, if we decide to improve the speed of loading the library (see http://gcc.gnu.org/wiki/Visibility for details). Therefore you are recommended to always add the above flags when using Google Test as a shared library. Otherwise a future release of Google Test may break your build script. Avoiding Macro Name Clashes \u00b6 In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO, you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . Currently FOO can be FAIL , SUCCEED , or TEST . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST(SomeTest, DoesThis) { ... } instead of TEST(SomeTest, DoesThis) { ... } in order to define a test. Developing Google Test \u00b6 This section discusses how to make your own changes to Google Test. Testing Google Test Itself \u00b6 To make sure your changes work as intended and don't break existing functionality, you'll want to compile and run Google Test's own tests. For that you can use CMake: mkdir mybuild cd mybuild cmake -Dgtest_build_tests=ON ${ GTEST_DIR } Make sure you have Python installed, as some of Google Test's tests are written in Python. If the cmake command complains about not being able to find Python ( Could NOT find PythonInterp (missing: PYTHON_EXECUTABLE) ), try telling it explicitly where your Python executable can be found: cmake -DPYTHON_EXECUTABLE=path/to/python -Dgtest_build_tests=ON ${ GTEST_DIR } Next, you can build Google Test and all of its own tests. On *nix, this is usually done by 'make'. To run the tests, do make test All tests should pass. Normally you don't need to worry about regenerating the source files, unless you need to modify them. In that case, you should modify the corresponding .pump files instead and run the pump.py Python script to regenerate them. You can find pump.py in the scripts/ directory. Read the Pump manual for how to use it.","title":"README"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#generic-build-instructions","text":"","title":"Generic Build Instructions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#setup","text":"To build Google Test and your tests that use it, you need to tell your build system where to find its headers and source files. The exact way to do it depends on which build system you use, and is usually straightforward.","title":"Setup"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#build","text":"Suppose you put Google Test in directory ${GTEST_DIR} . To build it, create a library build target (or a project as called by Visual Studio and Xcode) to compile ${ GTEST_DIR } /src/gtest-all.cc with ${GTEST_DIR}/include in the system header search path and ${GTEST_DIR} in the normal header search path. Assuming a Linux-like system and gcc, something like the following will do: g++ -isystem ${ GTEST_DIR } /include -I ${ GTEST_DIR } \\ -pthread -c ${ GTEST_DIR } /src/gtest-all.cc ar -rv libgtest.a gtest-all.o (We need -pthread as Google Test uses threads.) Next, you should compile your test source file with ${GTEST_DIR}/include in the system header search path, and link it with gtest and any other necessary libraries: g++ -isystem ${ GTEST_DIR } /include -pthread path/to/your_test.cc libgtest.a \\ -o your_test As an example, the make/ directory contains a Makefile that you can use to build Google Test on systems where GNU make is available (e.g. Linux, Mac OS X, and Cygwin). It doesn't try to build Google Test's own tests. Instead, it just builds the Google Test library and a sample test. You can use it as a starting point for your own build script. If the default settings are correct for your environment, the following commands should succeed: cd ${ GTEST_DIR } /make make ./sample1_unittest If you see errors, try to tweak the contents of make/Makefile to make them go away. There are instructions in make/Makefile on how to do it.","title":"Build"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#using-cmake","text":"Google Test comes with a CMake build script ( CMakeLists.txt ) that can be used on a wide range of platforms (\"C\" stands for cross-platform.). If you don't have CMake installed already, you can download it for free from http://www.cmake.org/ . CMake works by generating native makefiles or build projects that can be used in the compiler environment of your choice. You can either build Google Test as a standalone project or it can be incorporated into an existing CMake build for another project.","title":"Using CMake"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#standalone-cmake-project","text":"When building Google Test as a standalone project, the typical workflow starts with: mkdir mybuild # Create a directory to hold the build output. cd mybuild cmake ${ GTEST_DIR } # Generate native build scripts. If you want to build Google Test's samples, you should replace the last command with cmake -Dgtest_build_samples=ON ${ GTEST_DIR } If you are on a *nix system, you should now see a Makefile in the current directory. Just type 'make' to build gtest. If you use Windows and have Visual Studio installed, a gtest.sln file and several .vcproj files will be created. You can then build them using Visual Studio. On Mac OS X with Xcode installed, a .xcodeproj file will be generated.","title":"Standalone CMake Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#incorporating-into-an-existing-cmake-project","text":"If you want to use gtest in a project which already uses CMake, then a more robust and flexible approach is to build gtest as part of that project directly. This is done by making the GoogleTest source code available to the main build and adding it using CMake's add_subdirectory() command. This has the significant advantage that the same compiler and linker settings are used between gtest and the rest of your project, so issues associated with using incompatible libraries (eg debug/release), etc. are avoided. This is particularly useful on Windows. Making GoogleTest's source code available to the main build can be done a few different ways: Download the GoogleTest source code manually and place it at a known location. This is the least flexible approach and can make it more difficult to use with continuous integration systems, etc. Embed the GoogleTest source code as a direct copy in the main project's source tree. This is often the simplest approach, but is also the hardest to keep up to date. Some organizations may not permit this method. Add GoogleTest as a git submodule or equivalent. This may not always be possible or appropriate. Git submodules, for example, have their own set of advantages and drawbacks. Use CMake to download GoogleTest as part of the build's configure step. This is just a little more complex, but doesn't have the limitations of the other methods. The last of the above methods is implemented with a small piece of CMake code in a separate file (e.g. CMakeLists.txt.in ) which is copied to the build area and then invoked as a sub-build during the CMake stage . That directory is then pulled into the main build with add_subdirectory() . For example: New file CMakeLists.txt.in : cmake_minimum_required ( VERSION 2.8.2 ) project ( googletest-download NONE ) include ( ExternalProject ) ExternalProject_Add ( googletest GIT_REPOSITORY https://github.com/google/googletest.git GIT_TAG master SOURCE_DIR \"${CMAKE_BINARY_DIR}/googletest-src\" BINARY_DIR \"${CMAKE_BINARY_DIR}/googletest-build\" CONFIGURE_COMMAND \"\" BUILD_COMMAND \"\" INSTALL_COMMAND \"\" TEST_COMMAND \"\" ) Existing build's CMakeLists.txt : # Download and unpack googletest at configure time configure_file(CMakeLists.txt.in googletest-download/CMakeLists.txt) execute_process(COMMAND ${ CMAKE_COMMAND } -G \" ${ CMAKE_GENERATOR } \" . RESULT_VARIABLE result WORKING_DIRECTORY ${ CMAKE_BINARY_DIR } /googletest-download ) if(result) message(FATAL_ERROR \"CMake step for googletest failed: ${ result } \") endif() execute_process(COMMAND ${ CMAKE_COMMAND } --build . RESULT_VARIABLE result WORKING_DIRECTORY ${ CMAKE_BINARY_DIR } /googletest-download ) if(result) message(FATAL_ERROR \"Build step for googletest failed: ${ result } \") endif() # Prevent overriding the parent project's compiler/linker # settings on Windows set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE) # Add googletest directly to our build. This defines # the gtest and gtest_main targets. add_subdirectory( ${ CMAKE_BINARY_DIR } /googletest-src ${ CMAKE_BINARY_DIR } /googletest-build) # The gtest/gtest_main targets carry header search path # dependencies automatically when using CMake 2.8.11 or # later. Otherwise we have to add them here ourselves. if (CMAKE_VERSION VERSION_LESS 2.8.11) include_directories(\" ${ gtest_SOURCE_DIR } /include\") endif() # Now simply link against gtest or gtest_main as needed. Eg add_executable(example example.cpp) target_link_libraries(example gtest_main) add_test(NAME example_test COMMAND example) Note that this approach requires CMake 2.8.2 or later due to its use of the ExternalProject_Add() command. The above technique is discussed in more detail in this separate article which also contains a link to a fully generalized implementation of the technique.","title":"Incorporating Into An Existing CMake Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#legacy-build-scripts","text":"Before settling on CMake, we have been providing hand-maintained build projects/scripts for Visual Studio, Xcode, and Autotools. While we continue to provide them for convenience, they are not actively maintained any more. We highly recommend that you follow the instructions in the above sections to integrate Google Test with your existing build system. If you still need to use the legacy build scripts, here's how: The msvc folder contains two solutions with Visual C++ projects. Open the gtest.sln or gtest-md.sln file using Visual Studio, and you are ready to build Google Test the same way you build any Visual Studio project. Files that have names ending with -md use DLL versions of Microsoft runtime libraries (the /MD or the /MDd compiler option). Files without that suffix use static versions of the runtime libraries (the /MT or the /MTd option). Please note that one must use the same option to compile both gtest and the test code. If you use Visual Studio 2005 or above, we recommend the -md version as /MD is the default for new projects in these versions of Visual Studio. On Mac OS X, open the gtest.xcodeproj in the xcode/ folder using Xcode. Build the \"gtest\" target. The universal binary framework will end up in your selected build directory (selected in the Xcode \"Preferences...\" -> \"Building\" pane and defaults to xcode/build). Alternatively, at the command line, enter: xcodebuild This will build the \"Release\" configuration of gtest.framework in your default build location. See the \"xcodebuild\" man page for more information about building different configurations and building in different locations. If you wish to use the Google Test Xcode project with Xcode 4.x and above, you need to either: update the SDK configuration options in xcode/Config/General.xconfig. Comment options SDKROOT , MACOS_DEPLOYMENT_TARGET , and GCC_VERSION . If you choose this route you lose the ability to target earlier versions of MacOS X. Install an SDK for an earlier version. This doesn't appear to be supported by Apple, but has been reported to work ( http://stackoverflow.com/questions/5378518 ).","title":"Legacy Build Scripts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#tweaking-google-test","text":"Google Test can be used in diverse environments. The default configuration may not work (or may not work well) out of the box in some environments. However, you can easily tweak Google Test by defining control macros on the compiler command line. Generally, these macros are named like GTEST_XYZ and you define them to either 1 or 0 to enable or disable a certain feature. We list the most frequently used macros below. For a complete list, see file include/gtest/internal/gtest-port.h .","title":"Tweaking Google Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#choosing-a-tr1-tuple-library","text":"Some Google Test features require the C++ Technical Report 1 (TR1) tuple library, which is not yet available with all compilers. The good news is that Google Test implements a subset of TR1 tuple that's enough for its own need, and will automatically use this when the compiler doesn't provide TR1 tuple. Usually you don't need to care about which tuple library Google Test uses. However, if your project already uses TR1 tuple, you need to tell Google Test to use the same TR1 tuple library the rest of your project uses, or the two tuple implementations will clash. To do that, add -DGTEST_USE_OWN_TR1_TUPLE=0 to the compiler flags while compiling Google Test and your tests. If you want to force Google Test to use its own tuple library, just add -DGTEST_USE_OWN_TR1_TUPLE=1 to the compiler flags instead. If you don't want Google Test to use tuple at all, add -DGTEST_HAS_TR1_TUPLE=0 and all features using tuple will be disabled.","title":"Choosing a TR1 Tuple Library"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#multi-threaded-tests","text":"Google Test is thread-safe where the pthread library is available. After #include \"gtest/gtest.h\" , you can check the GTEST_IS_THREADSAFE macro to see whether this is the case (yes if the macro is #defined to 1, no if it's undefined.). If Google Test doesn't correctly detect whether pthread is available in your environment, you can force it with -DGTEST_HAS_PTHREAD=1 or -DGTEST_HAS_PTHREAD=0 When Google Test uses pthread, you may need to add flags to your compiler and/or linker to select the pthread library, or you'll get link errors. If you use the CMake script or the deprecated Autotools script, this is taken care of for you. If you use your own build script, you'll need to read your compiler and linker's manual to figure out what flags to add.","title":"Multi-threaded Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#as-a-shared-library-dll","text":"Google Test is compact, so most users can build and link it as a static library for the simplicity. You can choose to use Google Test as a shared library (known as a DLL on Windows) if you prefer. To compile gtest as a shared library, add -DGTEST_CREATE_SHARED_LIBRARY=1 to the compiler flags. You'll also need to tell the linker to produce a shared library instead - consult your linker's manual for how to do it. To compile your tests that use the gtest shared library, add -DGTEST_LINKED_AS_SHARED_LIBRARY=1 to the compiler flags. Note: while the above steps aren't technically necessary today when using some compilers (e.g. GCC), they may become necessary in the future, if we decide to improve the speed of loading the library (see http://gcc.gnu.org/wiki/Visibility for details). Therefore you are recommended to always add the above flags when using Google Test as a shared library. Otherwise a future release of Google Test may break your build script.","title":"As a Shared Library (DLL)"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#avoiding-macro-name-clashes","text":"In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO, you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . Currently FOO can be FAIL , SUCCEED , or TEST . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST(SomeTest, DoesThis) { ... } instead of TEST(SomeTest, DoesThis) { ... } in order to define a test.","title":"Avoiding Macro Name Clashes"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#developing-google-test","text":"This section discusses how to make your own changes to Google Test.","title":"Developing Google Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/#testing-google-test-itself","text":"To make sure your changes work as intended and don't break existing functionality, you'll want to compile and run Google Test's own tests. For that you can use CMake: mkdir mybuild cd mybuild cmake -Dgtest_build_tests=ON ${ GTEST_DIR } Make sure you have Python installed, as some of Google Test's tests are written in Python. If the cmake command complains about not being able to find Python ( Could NOT find PythonInterp (missing: PYTHON_EXECUTABLE) ), try telling it explicitly where your Python executable can be found: cmake -DPYTHON_EXECUTABLE=path/to/python -Dgtest_build_tests=ON ${ GTEST_DIR } Next, you can build Google Test and all of its own tests. On *nix, this is usually done by 'make'. To run the tests, do make test All tests should pass. Normally you don't need to worry about regenerating the source files, unless you need to modify them. In that case, you should modify the corresponding .pump files instead and run the pump.py Python script to regenerate them. You can find pump.py in the scripts/ directory. Read the Pump manual for how to use it.","title":"Testing Google Test Itself"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/","text":"Now that you have read Primer and learned how to write tests using Google Test, it's time to learn some new tricks. This document will show you more assertions as well as how to construct complex failure messages, propagate fatal failures, reuse and speed up your test fixtures, and use various flags with your tests. More Assertions \u00b6 This section covers some less frequently used, but still significant, assertions. Explicit Success and Failure \u00b6 These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); ADD_FAILURE_AT(\" file_path \", line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Note: you can only use FAIL() in functions that return void . See the Assertion Placement section for more information. Availability : Linux, Windows, Mac. Exception Assertions \u00b6 These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0. Predicate Assertions for Better Error Messages \u00b6 Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem: Using an Existing Boolean Function \u00b6 If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this FAQ for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac Using a Function That Returns an AssertionResult \u00b6 While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1. Using a Predicate-Formatter \u00b6 If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 ); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac. Floating-Point Comparison \u00b6 Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison . Floating-Point Macros \u00b6 Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( val1, val2 ); EXPECT_FLOAT_EQ( val1, val2 ); the two float values are almost equal ASSERT_DOUBLE_EQ( val1, val2 ); EXPECT_DOUBLE_EQ( val1, val2 ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac. Floating-Point Predicate-Format Functions \u00b6 Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac. Windows HRESULT assertions \u00b6 These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows. Type Assertions \u00b6 You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0. Assertion Placement \u00b6 You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations! Teaching Google Test How to Print Your Values \u00b6 When a test assertion such as EXPECT_EQ fails, Google Test prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <iostream> namespace foo { class Bar { ... }; // We want Google Test to be able to print instances of this. // It's important that the << operator is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. ::std::ostream& operator<<(::std::ostream& os, const Bar& bar) { return os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <iostream> namespace foo { class Bar { ... }; // It's important that PrintTo() is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. void PrintTo(const Bar& bar, ::std::ostream* os) { *os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when Google Test is concerned. This allows you to customize how the value appears in Google Test's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using Google Test's value printer yourself, just call ::testing::PrintToString( x ) , which returns an std::string : vector<pair<Bar, int> > bar_ints = GetBarIntVector(); EXPECT_TRUE(IsCorrectBarIntVector(bar_ints)) << \"bar_ints = \" << ::testing::PrintToString(bar_ints); Death Tests \u00b6 In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures . How to Write a Death Test \u00b6 Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); EXPECT_DEATH( statement, regex ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); EXPECT_DEATH_IF_SUPPORTED( statement, regex ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); EXPECT_EXIT( statement, predicate, regex ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(MyDeathTest, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0. Regular Expression Syntax \u00b6 On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support (Letter A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation \\\\. matches the . character . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only. How It Works \u00b6 Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails. Death Tests And Threads \u00b6 The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent. Death Test Styles \u00b6 The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); } Caveats \u00b6 The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some Google Test macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) . Using Assertions in Sub-routines \u00b6 Adding Traces to Assertions \u00b6 If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac. Propagating Fatal Failures \u00b6 A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections. Asserting on Subroutines \u00b6 As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported. Checking for Failures in the Current Test \u00b6 HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0. Logging Additional Information \u00b6 In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a string or an int . The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , classname , type_param , and value_param ). * Calling RecordProperty() outside of the lifespan of a test is allowed. If it's called outside of a test but between a test case's SetUpTestCase() and TearDownTestCase() methods, it will be attributed to the XML element for the test case. If it's called outside of all test cases (e.g. in a test environment), it will be attributed to the top-level XML element. Availability : Linux, Windows, Mac. Sharing Resources Between Tests in the Same Test Case \u00b6 Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac. Global Set-Up and Tear-Down \u00b6 Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac. Value Parameterized Tests \u00b6 Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFoo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it! How to Write Value-Parameterized Tests \u00b6 To write value-parameterized tests, first you should define a fixture class. It must be derived from both ::testing::Test and ::testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from ::testing::TestWithParam<T> , which itself is derived from both ::testing::Test and ::testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public ::testing::Test { ... }; class BarTest : public BaseTest, public ::testing::WithParamInterface<const char*> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . container , begin , and end can be expressions whose values are determined at run time. Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest_filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0. Creating Value-Parameterized Abstract Tests \u00b6 In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files. Typed Tests \u00b6 Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Type-Parameterized Tests \u00b6 Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Testing Private Code \u00b6 If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members Static Functions \u00b6 Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients. Private Class Members \u00b6 Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include \"gtest/gtest_prod.h\" // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace Catching Failures \u00b6 If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? \"gtest/gtest-spi.h\" contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in \"gtest/internal/gtest-port.h\" have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring ); Getting the Current Test's Name \u00b6 Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac. Extending Google Test by Handling Test Events \u00b6 Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0. Defining Event Listeners \u00b6 To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCEED() invocation. virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } }; Using Event Listeners \u00b6 To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier. Generating Failures in Listeners \u00b6 You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here . Running Test Programs: Advanced Options \u00b6 Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } Selecting Tests \u00b6 This section shows various options for choosing which tests to run. Listing Test Names \u00b6 Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac. Running a Subset of the Tests \u00b6 By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac. Temporarily Disabling Tests \u00b6 If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac. Temporarily Enabling Disabled Tests \u00b6 To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0. Repeating the Tests \u00b6 Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac. Shuffling the Tests \u00b6 You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0. Controlling Test Output \u00b6 This section teaches how to tweak the way test results are reported. Colored Terminal Output \u00b6 Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac. Suppressing the Elapsed Time \u00b6 By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.) Generating an XML Report \u00b6 Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Hudson . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac. Controlling How Failures Are Reported \u00b6 Turning Assertion Failures into Break-Points \u00b6 When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac. Disabling Catching Test-Thrown Exceptions \u00b6 Google Test can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default Google Test catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests. Availability : Linux, Windows, Mac. Letting Another Testing Framework Drive \u00b6 If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include \"gtest/gtest.h\" int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Death tests are not supported when other test framework is used to organize tests. Availability: Linux, Windows, Mac; since v1.3.0. Distributing Test Functions to Multiple Machines \u00b6 If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0. Fusing Google Test Source Files \u00b6 Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scripts/test/Makefile file as an example on how to compile your tests against them. Where to Go from Here \u00b6 Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the Frequently-Asked Questions .","title":"Advanced Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#more-assertions","text":"This section covers some less frequently used, but still significant, assertions.","title":"More Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#explicit-success-and-failure","text":"These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); ADD_FAILURE_AT(\" file_path \", line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Note: you can only use FAIL() in functions that return void . See the Assertion Placement section for more information. Availability : Linux, Windows, Mac.","title":"Explicit Success and Failure"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#exception-assertions","text":"These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0.","title":"Exception Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#predicate-assertions-for-better-error-messages","text":"Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem:","title":"Predicate Assertions for Better Error Messages"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#using-an-existing-boolean-function","text":"If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this FAQ for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac","title":"Using an Existing Boolean Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#using-a-function-that-returns-an-assertionresult","text":"While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1.","title":"Using a Function That Returns an AssertionResult"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#using-a-predicate-formatter","text":"If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 ); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac.","title":"Using a Predicate-Formatter"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#floating-point-comparison","text":"Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison .","title":"Floating-Point Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#floating-point-macros","text":"Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( val1, val2 ); EXPECT_FLOAT_EQ( val1, val2 ); the two float values are almost equal ASSERT_DOUBLE_EQ( val1, val2 ); EXPECT_DOUBLE_EQ( val1, val2 ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac.","title":"Floating-Point Macros"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#floating-point-predicate-format-functions","text":"Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac.","title":"Floating-Point Predicate-Format Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#windows-hresult-assertions","text":"These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows.","title":"Windows HRESULT assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#type-assertions","text":"You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Type Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#assertion-placement","text":"You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations!","title":"Assertion Placement"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#teaching-google-test-how-to-print-your-values","text":"When a test assertion such as EXPECT_EQ fails, Google Test prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <iostream> namespace foo { class Bar { ... }; // We want Google Test to be able to print instances of this. // It's important that the << operator is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. ::std::ostream& operator<<(::std::ostream& os, const Bar& bar) { return os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <iostream> namespace foo { class Bar { ... }; // It's important that PrintTo() is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. void PrintTo(const Bar& bar, ::std::ostream* os) { *os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when Google Test is concerned. This allows you to customize how the value appears in Google Test's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using Google Test's value printer yourself, just call ::testing::PrintToString( x ) , which returns an std::string : vector<pair<Bar, int> > bar_ints = GetBarIntVector(); EXPECT_TRUE(IsCorrectBarIntVector(bar_ints)) << \"bar_ints = \" << ::testing::PrintToString(bar_ints);","title":"Teaching Google Test How to Print Your Values"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#death-tests","text":"In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures .","title":"Death Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#how-to-write-a-death-test","text":"Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); EXPECT_DEATH( statement, regex ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); EXPECT_DEATH_IF_SUPPORTED( statement, regex ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); EXPECT_EXIT( statement, predicate, regex ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(MyDeathTest, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0.","title":"How to Write a Death Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#regular-expression-syntax","text":"On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support (Letter A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation \\\\. matches the . character . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only.","title":"Regular Expression Syntax"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#how-it-works","text":"Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails.","title":"How It Works"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#death-tests-and-threads","text":"The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent.","title":"Death Tests And Threads"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#death-test-styles","text":"The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); }","title":"Death Test Styles"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#caveats","text":"The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some Google Test macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) .","title":"Caveats"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#using-assertions-in-sub-routines","text":"","title":"Using Assertions in Sub-routines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#adding-traces-to-assertions","text":"If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac.","title":"Adding Traces to Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#propagating-fatal-failures","text":"A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections.","title":"Propagating Fatal Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#asserting-on-subroutines","text":"As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported.","title":"Asserting on Subroutines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#checking-for-failures-in-the-current-test","text":"HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0.","title":"Checking for Failures in the Current Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#logging-additional-information","text":"In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a string or an int . The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , classname , type_param , and value_param ). * Calling RecordProperty() outside of the lifespan of a test is allowed. If it's called outside of a test but between a test case's SetUpTestCase() and TearDownTestCase() methods, it will be attributed to the XML element for the test case. If it's called outside of all test cases (e.g. in a test environment), it will be attributed to the top-level XML element. Availability : Linux, Windows, Mac.","title":"Logging Additional Information"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#sharing-resources-between-tests-in-the-same-test-case","text":"Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac.","title":"Sharing Resources Between Tests in the Same Test Case"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#global-set-up-and-tear-down","text":"Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac.","title":"Global Set-Up and Tear-Down"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#value-parameterized-tests","text":"Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFoo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it!","title":"Value Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#how-to-write-value-parameterized-tests","text":"To write value-parameterized tests, first you should define a fixture class. It must be derived from both ::testing::Test and ::testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from ::testing::TestWithParam<T> , which itself is derived from both ::testing::Test and ::testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public ::testing::Test { ... }; class BarTest : public BaseTest, public ::testing::WithParamInterface<const char*> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . container , begin , and end can be expressions whose values are determined at run time. Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest_filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0.","title":"How to Write Value-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#creating-value-parameterized-abstract-tests","text":"In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files.","title":"Creating Value-Parameterized Abstract Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#typed-tests","text":"Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Typed Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#type-parameterized-tests","text":"Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Type-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#testing-private-code","text":"If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members","title":"Testing Private Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#static-functions","text":"Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients.","title":"Static Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#private-class-members","text":"Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include \"gtest/gtest_prod.h\" // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace","title":"Private Class Members"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#catching-failures","text":"If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? \"gtest/gtest-spi.h\" contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in \"gtest/internal/gtest-port.h\" have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring );","title":"Catching Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#getting-the-current-tests-name","text":"Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac.","title":"Getting the Current Test's Name"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#extending-google-test-by-handling-test-events","text":"Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Extending Google Test by Handling Test Events"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#defining-event-listeners","text":"To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCEED() invocation. virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } };","title":"Defining Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#using-event-listeners","text":"To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier.","title":"Using Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#generating-failures-in-listeners","text":"You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here .","title":"Generating Failures in Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#running-test-programs-advanced-options","text":"Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); }","title":"Running Test Programs: Advanced Options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#selecting-tests","text":"This section shows various options for choosing which tests to run.","title":"Selecting Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#listing-test-names","text":"Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac.","title":"Listing Test Names"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#running-a-subset-of-the-tests","text":"By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac.","title":"Running a Subset of the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#temporarily-disabling-tests","text":"If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac.","title":"Temporarily Disabling Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#temporarily-enabling-disabled-tests","text":"To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Temporarily Enabling Disabled Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#repeating-the-tests","text":"Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac.","title":"Repeating the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#shuffling-the-tests","text":"You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Shuffling the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#controlling-test-output","text":"This section teaches how to tweak the way test results are reported.","title":"Controlling Test Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#colored-terminal-output","text":"Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac.","title":"Colored Terminal Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#suppressing-the-elapsed-time","text":"By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.)","title":"Suppressing the Elapsed Time"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#generating-an-xml-report","text":"Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Hudson . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac.","title":"Generating an XML Report"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#controlling-how-failures-are-reported","text":"","title":"Controlling How Failures Are Reported"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#turning-assertion-failures-into-break-points","text":"When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac.","title":"Turning Assertion Failures into Break-Points"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#disabling-catching-test-thrown-exceptions","text":"Google Test can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default Google Test catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests. Availability : Linux, Windows, Mac.","title":"Disabling Catching Test-Thrown Exceptions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#letting-another-testing-framework-drive","text":"If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include \"gtest/gtest.h\" int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Death tests are not supported when other test framework is used to organize tests. Availability: Linux, Windows, Mac; since v1.3.0.","title":"Letting Another Testing Framework Drive"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#distributing-test-functions-to-multiple-machines","text":"If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Distributing Test Functions to Multiple Machines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#fusing-google-test-source-files","text":"Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scripts/test/Makefile file as an example on how to compile your tests against them.","title":"Fusing Google Test Source Files"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/AdvancedGuide/#where-to-go-from-here","text":"Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the Frequently-Asked Questions .","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/","text":"If you are interested in understanding the internals of Google Test, building from source, or contributing ideas or modifications to the project, then this document is for you. Introduction \u00b6 First, let's give you some background of the project. Licensing \u00b6 All Google Test source and pre-built packages are provided under the New BSD License . The Google Test Community \u00b6 The Google Test community exists primarily through the discussion group and the GitHub repository. You are definitely encouraged to contribute to the discussion and you can also help us to keep the effectiveness of the group high by following and promoting the guidelines listed here. Please Be Friendly \u00b6 Showing courtesy and respect to others is a vital part of the Google culture, and we strongly encourage everyone participating in Google Test development to join us in accepting nothing less. Of course, being courteous is not the same as failing to constructively disagree with each other, but it does mean that we should be respectful of each other when enumerating the 42 technical reasons that a particular proposal may not be the best choice. There's never a reason to be antagonistic or dismissive toward anyone who is sincerely trying to contribute to a discussion. Sure, C++ testing is serious business and all that, but it's also a lot of fun. Let's keep it that way. Let's strive to be one of the friendliest communities in all of open source. As always, discuss Google Test in the official GoogleTest discussion group. You don't have to actually submit code in order to sign up. Your participation itself is a valuable contribution. Working with the Code \u00b6 If you want to get your hands dirty with the code inside Google Test, this is the section for you. Compiling from Source \u00b6 Once you check out the code, you can find instructions on how to compile it in the README file. Testing \u00b6 A testing framework is of no good if itself is not thoroughly tested. Tests should be written for any new code, and changes should be verified to not break existing tests before they are submitted for review. To perform the tests, follow the instructions in README and verify that there are no failures. Contributing Code \u00b6 We are excited that Google Test is now open source, and hope to get great patches from the community. Before you fire up your favorite IDE and begin hammering away at that new feature, though, please take the time to read this section and understand the process. While it seems rigorous, we want to keep a high standard of quality in the code base. Contributor License Agreements \u00b6 You must sign a Contributor License Agreement (CLA) before we can accept any code. The CLA protects you and us. If you are an individual writing original source code and you're sure you own the intellectual property, then you'll need to sign an individual CLA . If you work for a company that wants to allow you to contribute your work to Google Test, then you'll need to sign a corporate CLA . Follow either of the two links above to access the appropriate CLA and instructions for how to sign and return it. Coding Style \u00b6 To keep the source consistent, readable, diffable and easy to merge, we use a fairly rigid coding style, as defined by the google-styleguide project. All patches will be expected to conform to the style outlined here . Updating Generated Code \u00b6 Some of Google Test's source files are generated by the Pump tool (a Python script). If you need to update such files, please modify the source ( foo.h.pump ) and re-generate the C++ file using Pump. You can read the PumpManual for details. Submitting Patches \u00b6 Please do submit code. Here's what you need to do: A submission should be a set of changes that addresses one issue in the issue tracker . Please don't mix more than one logical change per submittal, because it makes the history hard to follow. If you want to make a change that doesn't have a corresponding issue in the issue tracker, please create one. Also, coordinate with team members that are listed on the issue in question. This ensures that work isn't being duplicated and communicating your plan early also generally leads to better patches. Ensure that your code adheres to the Google Test source code style . Ensure that there are unit tests for your code. Sign a Contributor License Agreement. Create a Pull Request in the usual way. Google Test Committers \u00b6 The current members of the Google Test engineering team are the only committers at present. In the great tradition of eating one's own dogfood, we will be requiring each new Google Test engineering team member to earn the right to become a committer by following the procedures in this document, writing consistently great code, and demonstrating repeatedly that he or she truly gets the zen of Google Test. Release Process \u00b6 We follow a typical release process: A release branch named release-X.Y is created. Bugs are fixed and features are added in trunk; those individual patches are merged into the release branch until it's stable. An individual point release (the Z in X.Y.Z ) is made by creating a tag from the branch. Repeat steps 2 and 3 throughout one release cycle (as determined by features or time). Go back to step 1 to create another release branch and so on. This page is based on the Making GWT Better guide from the Google Web Toolkit project. Except as otherwise noted , the content of this page is licensed under the Creative Commons Attribution 2.5 License .","title":"Dev Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#introduction","text":"First, let's give you some background of the project.","title":"Introduction"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#licensing","text":"All Google Test source and pre-built packages are provided under the New BSD License .","title":"Licensing"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#the-google-test-community","text":"The Google Test community exists primarily through the discussion group and the GitHub repository. You are definitely encouraged to contribute to the discussion and you can also help us to keep the effectiveness of the group high by following and promoting the guidelines listed here.","title":"The Google Test Community"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#please-be-friendly","text":"Showing courtesy and respect to others is a vital part of the Google culture, and we strongly encourage everyone participating in Google Test development to join us in accepting nothing less. Of course, being courteous is not the same as failing to constructively disagree with each other, but it does mean that we should be respectful of each other when enumerating the 42 technical reasons that a particular proposal may not be the best choice. There's never a reason to be antagonistic or dismissive toward anyone who is sincerely trying to contribute to a discussion. Sure, C++ testing is serious business and all that, but it's also a lot of fun. Let's keep it that way. Let's strive to be one of the friendliest communities in all of open source. As always, discuss Google Test in the official GoogleTest discussion group. You don't have to actually submit code in order to sign up. Your participation itself is a valuable contribution.","title":"Please Be Friendly"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#working-with-the-code","text":"If you want to get your hands dirty with the code inside Google Test, this is the section for you.","title":"Working with the Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#compiling-from-source","text":"Once you check out the code, you can find instructions on how to compile it in the README file.","title":"Compiling from Source"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#testing","text":"A testing framework is of no good if itself is not thoroughly tested. Tests should be written for any new code, and changes should be verified to not break existing tests before they are submitted for review. To perform the tests, follow the instructions in README and verify that there are no failures.","title":"Testing"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#contributing-code","text":"We are excited that Google Test is now open source, and hope to get great patches from the community. Before you fire up your favorite IDE and begin hammering away at that new feature, though, please take the time to read this section and understand the process. While it seems rigorous, we want to keep a high standard of quality in the code base.","title":"Contributing Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#contributor-license-agreements","text":"You must sign a Contributor License Agreement (CLA) before we can accept any code. The CLA protects you and us. If you are an individual writing original source code and you're sure you own the intellectual property, then you'll need to sign an individual CLA . If you work for a company that wants to allow you to contribute your work to Google Test, then you'll need to sign a corporate CLA . Follow either of the two links above to access the appropriate CLA and instructions for how to sign and return it.","title":"Contributor License Agreements"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#coding-style","text":"To keep the source consistent, readable, diffable and easy to merge, we use a fairly rigid coding style, as defined by the google-styleguide project. All patches will be expected to conform to the style outlined here .","title":"Coding Style"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#updating-generated-code","text":"Some of Google Test's source files are generated by the Pump tool (a Python script). If you need to update such files, please modify the source ( foo.h.pump ) and re-generate the C++ file using Pump. You can read the PumpManual for details.","title":"Updating Generated Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#submitting-patches","text":"Please do submit code. Here's what you need to do: A submission should be a set of changes that addresses one issue in the issue tracker . Please don't mix more than one logical change per submittal, because it makes the history hard to follow. If you want to make a change that doesn't have a corresponding issue in the issue tracker, please create one. Also, coordinate with team members that are listed on the issue in question. This ensures that work isn't being duplicated and communicating your plan early also generally leads to better patches. Ensure that your code adheres to the Google Test source code style . Ensure that there are unit tests for your code. Sign a Contributor License Agreement. Create a Pull Request in the usual way.","title":"Submitting Patches"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#google-test-committers","text":"The current members of the Google Test engineering team are the only committers at present. In the great tradition of eating one's own dogfood, we will be requiring each new Google Test engineering team member to earn the right to become a committer by following the procedures in this document, writing consistently great code, and demonstrating repeatedly that he or she truly gets the zen of Google Test.","title":"Google Test Committers"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/DevGuide/#release-process","text":"We follow a typical release process: A release branch named release-X.Y is created. Bugs are fixed and features are added in trunk; those individual patches are merged into the release branch until it's stable. An individual point release (the Z in X.Y.Z ) is made by creating a tag from the branch. Repeat steps 2 and 3 throughout one release cycle (as determined by features or time). Go back to step 1 to create another release branch and so on. This page is based on the Making GWT Better guide from the Google Web Toolkit project. Except as otherwise noted , the content of this page is licensed under the Creative Commons Attribution 2.5 License .","title":"Release Process"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Documentation/","text":"This page lists all documentation wiki pages for Google Test (the SVN trunk version) -- if you use a released version of Google Test, please read the documentation for that specific version instead. Primer -- start here if you are new to Google Test. Samples -- learn from examples. AdvancedGuide -- learn more about Google Test. XcodeGuide -- how to use Google Test in Xcode on Mac. Frequently-Asked Questions -- check here before asking a question on the mailing list. To contribute code to Google Test, read: DevGuide -- read this before writing your first patch. PumpManual -- how we generate some of Google Test's source files.","title":"Documentation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/","text":"If you cannot find the answer to your question here, and you have read Primer and AdvancedGuide , send it to googletestframework@googlegroups.com . Why should I use Google Test instead of my favorite C++ testing framework? \u00b6 First, let us say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable: it doesn't require exceptions or RTTI; it works around various bugs in various compilers and environments; etc. As a result, it works on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. Google Test can generate XML test result reports that can be parsed by popular continuous build system like Hudson. Simple things are easy in Google Test, while hard things are possible: in addition to advanced features like global test environments and tests parameterized by values or types , Google Test supports various ways for the user to extend the framework -- if Google Test doesn't do something out of the box, chances are that a user can implement the feature using Google Test's public API, without changing Google Test itself. In particular, you can: expand your testing vocabulary by defining custom predicates , teach Google Test how to print your types , define your own testing macros or utilities and verify them using Google Test's Service Provider Interface , and reflect on the test cases or change the test output format by intercepting the test events . I'm getting warnings when compiling Google Test. Would you fix them? \u00b6 We strive to minimize compiler warnings Google Test generates. Before releasing a new version, we test to make sure that it doesn't generate warnings when compiled using its CMake script on Windows, Linux, and Mac OS. Unfortunately, this doesn't mean you are guaranteed to see no warnings when compiling Google Test in your environment: You may be using a different compiler as we use, or a different version of the same compiler. We cannot possibly test for all compilers. You may be compiling on a different platform as we do. Your project may be using different compiler flags as we do. It is not always possible to make Google Test warning-free for everyone. Or, it may not be desirable if the warning is rarely enabled and fixing the violations makes the code more complex. If you see warnings when compiling Google Test, we suggest that you use the -isystem flag (assuming your are using GCC) to mark Google Test headers as system headers. That'll suppress warnings from Google Test headers. Why should not test case names and test names contain underscore? \u00b6 Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that containers two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestCaseName, TestName) generates a class named TestCaseName_TestName_Test . What happens if TestCaseName or TestName contains _ ? If TestCaseName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestCaseName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestCaseName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestCaseName_Bar__Test , which is invalid. So clearly TestCaseName and TestName cannot start or end with _ (Actually, TestCaseName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestCaseName and TestName to contain _ in the middle. However, consider this: TEST ( Time , Flies_Like_An_Arrow ) { ... } TEST ( Time_Flies , Like_An_Arrow ) { ... } Now, the two TEST s will both generate the same class ( Time_Files_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestCaseName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives Google Test some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediately consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of Google Test. Therefore it's best to follow the rule. Why is it not recommended to install a pre-compiled copy of Google Test (for example, into /usr/local)? \u00b6 In the early days, we said that you could install compiled Google Test libraries on * nix systems using make install . Then every user of your machine can write tests without recompiling Google Test. This seemed like a good idea, but it has a got-cha: every user needs to compile his tests using the same compiler flags used to compile the installed Google Test libraries; otherwise he may run into undefined behaviors (i.e. the tests can behave strangely and may even crash for no obvious reasons). Why? Because C++ has this thing called the One-Definition Rule: if two C++ source files contain different definitions of the same class/function/variable, and you link them together, you violate the rule. The linker may or may not catch the error (in many cases it's not required by the C++ standard to catch the violation). If it doesn't, you get strange run-time behaviors that are unexpected and hard to debug. If you compile Google Test and your test code using different compiler flags, they may see different definitions of the same class/function/variable (e.g. due to the use of #if in Google Test). Therefore, for your sanity, we recommend to avoid installing pre-compiled Google Test libraries. Instead, each project should compile Google Test itself such that it can be sure that the same flags are used for both Google Test and the tests. How do I generate 64-bit binaries on Windows (using Visual Studio 2008)? \u00b6 (Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory. Can I use Google Test on MinGW? \u00b6 We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW. Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)? \u00b6 Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr != NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros. Does Google Test support running tests in parallel? \u00b6 Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel. Why don't Google Test run the tests in different threads to speed things up? \u00b6 It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes. Why aren't Google Test assertions implemented using exceptions? \u00b6 Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE (...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST . Why do we use two different macros for tests with and without fixtures? \u00b6 Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public :: testing :: Test {}; TEST_F ( FooTest , DoesThis ) { ... } or typedef :: testing :: Test FooTest ; TEST_F ( FooTest , DoesThat ) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way. Why don't we use structs as test fixtures? \u00b6 We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes. Why are death tests implemented as assertions instead of using a test runner? \u00b6 Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if ( FooCondition ()) { ASSERT_DEATH ( Bar (), \"blah\" ); } else { ASSERT_EQ ( 5 , Bar ()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount (); // Only known at run time. for ( int i = 1 ; i <= count ; i ++ ) { ASSERT_DEATH ({ double * buffer = new double [ i ]; ... initializes buffer ... Foo ( buffer , i ) }, \"blah blah\" ); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically. My death test modifies some state, but the change seems lost after the death test finishes. Why? \u00b6 Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less. The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong? \u00b6 If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100 ; }; You also need to define it outside of the class body in foo.cc : const int Foo :: kBar ; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error. I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations? \u00b6 Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon. Can I derive a test fixture from another? \u00b6 Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public :: testing :: Test { protected : ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected : virtual void SetUp () { BaseTest :: SetUp (); // Sets up the base fixture first. ... additional set - up work ... } virtual void TearDown () { ... clean - up work for FooTest ... BaseTest :: TearDown (); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F ( FooTest , Bar ) { ... } TEST_F ( FooTest , Baz ) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5 . My compiler complains \"void value not ignored as it ought to be.\" What does this mean? \u00b6 You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions. My death test hangs (or seg-faults). How do I fix it? \u00b6 In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry! Should I use the constructor/destructor of the test fixture or the set-up/tear-down function? \u00b6 The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test body, call TearDown() , and then immediately delete the test fixture object. When you need to write per-test set-up and tear-down logic, you have the choice between using the test fixture constructor/destructor or SetUp()/TearDown() . The former is usually preferred, as it has the following benefits: By initializing a member variable in the constructor, we have the option to make it const , which helps prevent accidental changes to its value and makes the tests more obviously correct. In case we need to subclass the test fixture class, the subclass' constructor is guaranteed to call the base class' constructor first, and the subclass' destructor is guaranteed to call the base class' destructor afterward. With SetUp()/TearDown() , a subclass may make the mistake of forgetting to call the base class' SetUp()/TearDown() or call them at the wrong moment. You may still want to use SetUp()/TearDown() in the following rare cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The assertion macros throw an exception when flag --gtest_throw_on_failure is specified. Therefore, you shouldn't use Google Test assertions in a destructor if you plan to run your tests with this flag. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() . The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it? \u00b6 If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive ( int n ) { return n > 0 ; } bool IsPositive ( double x ) { return x > 0 ; } you will get a compiler error if you write EXPECT_PRED1 ( IsPositive , 5 ); However, this will work: EXPECT_PRED1 ( * static_cast < bool ( * )( int ) >* ( IsPositive ), 5 ); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template < typename T > bool IsNegative ( T x ) { return x < 0 ; } you can use it in a predicate assertion like this: ASSERT_PRED1 ( IsNegative *< int >* , - 5 ); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2 ( * GreaterThan < int , int >* , 5 , 0 ); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2 ( * ( GreaterThan < int , int > ) * , 5 , 0 ); My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why? \u00b6 Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS (); they write RUN_ALL_TESTS (); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() . My compiler complains that a constructor (or destructor) cannot return a value. What's going on? \u00b6 Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ ( 1 , Foo ()) << \"blah blah\" << foo ; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it. My set-up function is not called. Why? \u00b6 C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called. How do I jump to the line of a failure in Emacs directly? \u00b6 Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure. I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious. \u00b6 You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } class BarTest : public BaseTest {}; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest ; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } typedef BaseTest BarTest ; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... } The Google Test output is buried in a whole bunch of log messages. What do I do? \u00b6 The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt Why should I prefer test fixtures over global variables? \u00b6 There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common. How do I test private class members without writing FRIEND_TEST()s? \u00b6 You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest ; ... }; class FooTest : public :: testing :: Test { protected : ... void Test1 () {...} // This accesses private members of class Foo. void Test2 () {...} // So does this one. }; TEST_F ( FooTest , Test1 ) { Test1 (); } TEST_F ( FooTest , Test2 ) { Test2 (); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest ; ... }; class FooTest : public :: testing :: Test { protected : ... T1 get_private_member1 ( Foo * obj ) { return obj -> private_member1_ ; } }; TEST_F ( FooTest , Test1 ) { ... get_private_member1 ( x ) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected : // protected access for testability. int DoSomethingReturningInt (); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public : using YourClass :: DoSomethingReturningInt ; // changes access rights ... }; TEST_F ( YourClassTest , DoSomethingTest ) { TestableYourClass obj ; assertEquals ( expected_value , obj . DoSomethingReturningInt ()); } How do I test private class static members without writing FRIEND_TEST()s? \u00b6 We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private : static bool Func ( int n ); }; // foo.cc bool Foo :: Func ( int n ) { ... } // foo_test.cc EXPECT_TRUE ( Foo :: Func ( 12345 )); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func ( int n ) { ... } } // foo_test.cc namespace internal { bool Func ( int n ); } EXPECT_TRUE ( internal :: Func ( 12345 )); I would like to run a test several times with different parameters. Do I need to write several similar copies of it? \u00b6 No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once. How do I test a file that defines main()? \u00b6 To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort. What can the statement argument in ASSERT_DEATH() be? \u00b6 ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST ( MyDeathTest , FunctionCall ) { ASSERT_DEATH ( Xyz ( 5 ), \"Xyz failed\" ); } // Or a complex expression that references variables and functions. TEST ( MyDeathTest , ComplexExpression ) { const bool c = Condition (); ASSERT_DEATH (( c ? Func1 ( 0 ) : object2 . Method ( \"test\" )), \"(Func1|Method) failed\" ); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST ( MyDeathTest , InsideLoop ) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for ( int i = 0 ; i < 5 ; i ++ ) { EXPECT_DEATH_M ( Foo ( i ), \"Foo has \\\\ d+ errors\" , :: testing :: Message () << \"where i is \" << i ); } } // A death assertion can contain a compound statement. TEST ( MyDeathTest , CompoundStatement ) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH ({ for ( int i = 0 ; i < 5 ; i ++ ) { Bar ( i ); } }, \"Bar has \\\\ d+ errors\" );} googletest_unittest.cc contains more examples if you are interested. What syntax does the regular expression in ASSERT_DEATH use? \u00b6 On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax . I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why? \u00b6 Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .) Why does ASSERT_DEATH complain about previous threads that were already joined? \u00b6 With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this. Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH? \u00b6 Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F ( FooTest , AbcDeathTest ) { ... } TEST_F ( FooTest , Uvw ) { ... } TEST_F ( BarTest , DefDeathTest ) { ... } TEST_F ( BarTest , Xyz ) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw . But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do? \u00b6 You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public :: testing :: Test { ... }; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } typedef FooTest FooDeathTest ; TEST_F ( FooDeathTest , Uvw ) { ... EXPECT_DEATH (...) ... } TEST_F ( FooDeathTest , Xyz ) { ... ASSERT_DEATH (...) ... } The compiler complains about \"no match for 'operator<<'\" when I use an assertion. What gives? \u00b6 If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space. How do I suppress the memory leak messages on Windows? \u00b6 Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines. I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help! \u00b6 You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj. I put my tests in a library and Google Test doesn't run them. What's happening? \u00b6 Have you read a warning on the Google Test Primer page? I want to use Google Test with Visual Studio but don't know where to start. \u00b6 Many people are in your position and one of the posted his solution to our mailing list. I am seeing compile errors mentioning std::type_traits when I try to use Google Test on Solaris. \u00b6 Google Test uses parts of the standard C++ library that SunStudio does not support. Our users reported success using alternative implementations. Try running the build after runing this commad: export CC=cc CXX=CC CXXFLAGS='-library=stlport4' How can my code detect if it is running in a test? \u00b6 If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and Google Test doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is dependency injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all, there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test. Google Test defines a macro that clashes with one defined by another library. How do I deal with that? \u00b6 In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO , you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST ( SomeTest , DoesThis ) { ... } instead of TEST ( SomeTest , DoesThis ) { ... } in order to define a test. Currently, the following TEST , FAIL , SUCCEED , and the basic comparison assertion macros can have alternative names. You can see the full list of covered macros here . More information can be found in the \"Avoiding Macro Name Clashes\" section of the README file. Is it OK if I have two separate TEST(Foo, Bar) test methods defined in different namespaces? \u00b6 Yes. The rule is all test methods in the same test case must use the same fixture class . This means that the following is allowed because both tests use the same fixture class ( ::testing::Test ). namespace foo { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo However, the following code is not allowed and will produce a runtime error from Google Test because the test methods are using different test fixture classes with the same test case name. namespace foo { class CoolTest : public :: testing :: Test {}; // Fixture foo::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { class CoolTest : public :: testing :: Test {}; // Fixture: bar::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo How do I build Google Testing Framework with Xcode 4? \u00b6 If you try to build Google Test's Xcode project with Xcode 4.0 or later, you may encounter an error message that looks like \"Missing SDK in target gtest_framework: /Developer/SDKs/MacOSX10.4u.sdk\". That means that Xcode does not support the SDK the project is targeting. See the Xcode section in the README file on how to resolve this. My question is not covered in your FAQ! \u00b6 If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the commit hash if you check out from Git directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.","title":"FAQ"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-should-i-use-google-test-instead-of-my-favorite-c-testing-framework","text":"First, let us say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable: it doesn't require exceptions or RTTI; it works around various bugs in various compilers and environments; etc. As a result, it works on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. Google Test can generate XML test result reports that can be parsed by popular continuous build system like Hudson. Simple things are easy in Google Test, while hard things are possible: in addition to advanced features like global test environments and tests parameterized by values or types , Google Test supports various ways for the user to extend the framework -- if Google Test doesn't do something out of the box, chances are that a user can implement the feature using Google Test's public API, without changing Google Test itself. In particular, you can: expand your testing vocabulary by defining custom predicates , teach Google Test how to print your types , define your own testing macros or utilities and verify them using Google Test's Service Provider Interface , and reflect on the test cases or change the test output format by intercepting the test events .","title":"Why should I use Google Test instead of my favorite C++ testing framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#im-getting-warnings-when-compiling-google-test-would-you-fix-them","text":"We strive to minimize compiler warnings Google Test generates. Before releasing a new version, we test to make sure that it doesn't generate warnings when compiled using its CMake script on Windows, Linux, and Mac OS. Unfortunately, this doesn't mean you are guaranteed to see no warnings when compiling Google Test in your environment: You may be using a different compiler as we use, or a different version of the same compiler. We cannot possibly test for all compilers. You may be compiling on a different platform as we do. Your project may be using different compiler flags as we do. It is not always possible to make Google Test warning-free for everyone. Or, it may not be desirable if the warning is rarely enabled and fixing the violations makes the code more complex. If you see warnings when compiling Google Test, we suggest that you use the -isystem flag (assuming your are using GCC) to mark Google Test headers as system headers. That'll suppress warnings from Google Test headers.","title":"I'm getting warnings when compiling Google Test.  Would you fix them?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-should-not-test-case-names-and-test-names-contain-underscore","text":"Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that containers two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestCaseName, TestName) generates a class named TestCaseName_TestName_Test . What happens if TestCaseName or TestName contains _ ? If TestCaseName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestCaseName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestCaseName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestCaseName_Bar__Test , which is invalid. So clearly TestCaseName and TestName cannot start or end with _ (Actually, TestCaseName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestCaseName and TestName to contain _ in the middle. However, consider this: TEST ( Time , Flies_Like_An_Arrow ) { ... } TEST ( Time_Flies , Like_An_Arrow ) { ... } Now, the two TEST s will both generate the same class ( Time_Files_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestCaseName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives Google Test some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediately consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of Google Test. Therefore it's best to follow the rule.","title":"Why should not test case names and test names contain underscore?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-is-it-not-recommended-to-install-a-pre-compiled-copy-of-google-test-for-example-into-usrlocal","text":"In the early days, we said that you could install compiled Google Test libraries on * nix systems using make install . Then every user of your machine can write tests without recompiling Google Test. This seemed like a good idea, but it has a got-cha: every user needs to compile his tests using the same compiler flags used to compile the installed Google Test libraries; otherwise he may run into undefined behaviors (i.e. the tests can behave strangely and may even crash for no obvious reasons). Why? Because C++ has this thing called the One-Definition Rule: if two C++ source files contain different definitions of the same class/function/variable, and you link them together, you violate the rule. The linker may or may not catch the error (in many cases it's not required by the C++ standard to catch the violation). If it doesn't, you get strange run-time behaviors that are unexpected and hard to debug. If you compile Google Test and your test code using different compiler flags, they may see different definitions of the same class/function/variable (e.g. due to the use of #if in Google Test). Therefore, for your sanity, we recommend to avoid installing pre-compiled Google Test libraries. Instead, each project should compile Google Test itself such that it can be sure that the same flags are used for both Google Test and the tests.","title":"Why is it not recommended to install a pre-compiled copy of Google Test (for example, into /usr/local)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-generate-64-bit-binaries-on-windows-using-visual-studio-2008","text":"(Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory.","title":"How do I generate 64-bit binaries on Windows (using Visual Studio 2008)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#can-i-use-google-test-on-mingw","text":"We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW.","title":"Can I use Google Test on MinGW?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-does-google-test-support-expect_eqnull-ptr-and-assert_eqnull-ptr-but-not-expect_nenull-ptr-and-assert_nenull-ptr","text":"Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr != NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros.","title":"Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#does-google-test-support-running-tests-in-parallel","text":"Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel.","title":"Does Google Test support running tests in parallel?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-dont-google-test-run-the-tests-in-different-threads-to-speed-things-up","text":"It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes.","title":"Why don't Google Test run the tests in different threads to speed things up?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-arent-google-test-assertions-implemented-using-exceptions","text":"Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE (...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST .","title":"Why aren't Google Test assertions implemented using exceptions?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-do-we-use-two-different-macros-for-tests-with-and-without-fixtures","text":"Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public :: testing :: Test {}; TEST_F ( FooTest , DoesThis ) { ... } or typedef :: testing :: Test FooTest ; TEST_F ( FooTest , DoesThat ) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way.","title":"Why do we use two different macros for tests with and without fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-dont-we-use-structs-as-test-fixtures","text":"We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes.","title":"Why don't we use structs as test fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-are-death-tests-implemented-as-assertions-instead-of-using-a-test-runner","text":"Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if ( FooCondition ()) { ASSERT_DEATH ( Bar (), \"blah\" ); } else { ASSERT_EQ ( 5 , Bar ()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount (); // Only known at run time. for ( int i = 1 ; i <= count ; i ++ ) { ASSERT_DEATH ({ double * buffer = new double [ i ]; ... initializes buffer ... Foo ( buffer , i ) }, \"blah blah\" ); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically.","title":"Why are death tests implemented as assertions instead of using a test runner?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-death-test-modifies-some-state-but-the-change-seems-lost-after-the-death-test-finishes-why","text":"Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less.","title":"My death test modifies some state, but the change seems lost after the death test finishes. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#the-compiler-complains-about-undefined-references-to-some-static-const-member-variables-but-i-did-define-them-in-the-class-body-whats-wrong","text":"If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100 ; }; You also need to define it outside of the class body in foo.cc : const int Foo :: kBar ; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error.","title":"The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-have-an-interface-that-has-several-implementations-can-i-write-a-set-of-tests-once-and-repeat-them-over-all-the-implementations","text":"Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon.","title":"I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#can-i-derive-a-test-fixture-from-another","text":"Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public :: testing :: Test { protected : ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected : virtual void SetUp () { BaseTest :: SetUp (); // Sets up the base fixture first. ... additional set - up work ... } virtual void TearDown () { ... clean - up work for FooTest ... BaseTest :: TearDown (); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F ( FooTest , Bar ) { ... } TEST_F ( FooTest , Baz ) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5 .","title":"Can I derive a test fixture from another?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-compiler-complains-void-value-not-ignored-as-it-ought-to-be-what-does-this-mean","text":"You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions.","title":"My compiler complains \"void value not ignored as it ought to be.\" What does this mean?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-death-test-hangs-or-seg-faults-how-do-i-fix-it","text":"In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry!","title":"My death test hangs (or seg-faults). How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#should-i-use-the-constructordestructor-of-the-test-fixture-or-the-set-uptear-down-function","text":"The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test body, call TearDown() , and then immediately delete the test fixture object. When you need to write per-test set-up and tear-down logic, you have the choice between using the test fixture constructor/destructor or SetUp()/TearDown() . The former is usually preferred, as it has the following benefits: By initializing a member variable in the constructor, we have the option to make it const , which helps prevent accidental changes to its value and makes the tests more obviously correct. In case we need to subclass the test fixture class, the subclass' constructor is guaranteed to call the base class' constructor first, and the subclass' destructor is guaranteed to call the base class' destructor afterward. With SetUp()/TearDown() , a subclass may make the mistake of forgetting to call the base class' SetUp()/TearDown() or call them at the wrong moment. You may still want to use SetUp()/TearDown() in the following rare cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The assertion macros throw an exception when flag --gtest_throw_on_failure is specified. Therefore, you shouldn't use Google Test assertions in a destructor if you plan to run your tests with this flag. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() .","title":"Should I use the constructor/destructor of the test fixture or the set-up/tear-down function?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#the-compiler-complains-no-matching-function-to-call-when-i-use-assert_predn-how-do-i-fix-it","text":"If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive ( int n ) { return n > 0 ; } bool IsPositive ( double x ) { return x > 0 ; } you will get a compiler error if you write EXPECT_PRED1 ( IsPositive , 5 ); However, this will work: EXPECT_PRED1 ( * static_cast < bool ( * )( int ) >* ( IsPositive ), 5 ); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template < typename T > bool IsNegative ( T x ) { return x < 0 ; } you can use it in a predicate assertion like this: ASSERT_PRED1 ( IsNegative *< int >* , - 5 ); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2 ( * GreaterThan < int , int >* , 5 , 0 ); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2 ( * ( GreaterThan < int , int > ) * , 5 , 0 );","title":"The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-compiler-complains-about-ignoring-return-value-when-i-call-run_all_tests-why","text":"Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS (); they write RUN_ALL_TESTS (); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() .","title":"My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-compiler-complains-that-a-constructor-or-destructor-cannot-return-a-value-whats-going-on","text":"Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ ( 1 , Foo ()) << \"blah blah\" << foo ; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it.","title":"My compiler complains that a constructor (or destructor) cannot return a value. What's going on?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-set-up-function-is-not-called-why","text":"C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called.","title":"My set-up function is not called. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-jump-to-the-line-of-a-failure-in-emacs-directly","text":"Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure.","title":"How do I jump to the line of a failure in Emacs directly?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-have-several-test-cases-which-share-the-same-test-fixture-logic-do-i-have-to-define-a-new-test-fixture-class-for-each-of-them-this-seems-pretty-tedious","text":"You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } class BarTest : public BaseTest {}; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest ; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } typedef BaseTest BarTest ; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... }","title":"I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#the-google-test-output-is-buried-in-a-whole-bunch-of-log-messages-what-do-i-do","text":"The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt","title":"The Google Test output is buried in a whole bunch of log messages. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-should-i-prefer-test-fixtures-over-global-variables","text":"There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common.","title":"Why should I prefer test fixtures over global variables?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-test-private-class-members-without-writing-friend_tests","text":"You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest ; ... }; class FooTest : public :: testing :: Test { protected : ... void Test1 () {...} // This accesses private members of class Foo. void Test2 () {...} // So does this one. }; TEST_F ( FooTest , Test1 ) { Test1 (); } TEST_F ( FooTest , Test2 ) { Test2 (); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest ; ... }; class FooTest : public :: testing :: Test { protected : ... T1 get_private_member1 ( Foo * obj ) { return obj -> private_member1_ ; } }; TEST_F ( FooTest , Test1 ) { ... get_private_member1 ( x ) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected : // protected access for testability. int DoSomethingReturningInt (); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public : using YourClass :: DoSomethingReturningInt ; // changes access rights ... }; TEST_F ( YourClassTest , DoSomethingTest ) { TestableYourClass obj ; assertEquals ( expected_value , obj . DoSomethingReturningInt ()); }","title":"How do I test private class members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-test-private-class-static-members-without-writing-friend_tests","text":"We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private : static bool Func ( int n ); }; // foo.cc bool Foo :: Func ( int n ) { ... } // foo_test.cc EXPECT_TRUE ( Foo :: Func ( 12345 )); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func ( int n ) { ... } } // foo_test.cc namespace internal { bool Func ( int n ); } EXPECT_TRUE ( internal :: Func ( 12345 ));","title":"How do I test private class static members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-would-like-to-run-a-test-several-times-with-different-parameters-do-i-need-to-write-several-similar-copies-of-it","text":"No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once.","title":"I would like to run a test several times with different parameters. Do I need to write several similar copies of it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-test-a-file-that-defines-main","text":"To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort.","title":"How do I test a file that defines main()?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#what-can-the-statement-argument-in-assert_death-be","text":"ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST ( MyDeathTest , FunctionCall ) { ASSERT_DEATH ( Xyz ( 5 ), \"Xyz failed\" ); } // Or a complex expression that references variables and functions. TEST ( MyDeathTest , ComplexExpression ) { const bool c = Condition (); ASSERT_DEATH (( c ? Func1 ( 0 ) : object2 . Method ( \"test\" )), \"(Func1|Method) failed\" ); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST ( MyDeathTest , InsideLoop ) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for ( int i = 0 ; i < 5 ; i ++ ) { EXPECT_DEATH_M ( Foo ( i ), \"Foo has \\\\ d+ errors\" , :: testing :: Message () << \"where i is \" << i ); } } // A death assertion can contain a compound statement. TEST ( MyDeathTest , CompoundStatement ) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH ({ for ( int i = 0 ; i < 5 ; i ++ ) { Bar ( i ); } }, \"Bar has \\\\ d+ errors\" );} googletest_unittest.cc contains more examples if you are interested.","title":"What can the statement argument in ASSERT_DEATH() be?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#what-syntax-does-the-regular-expression-in-assert_death-use","text":"On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax .","title":"What syntax does the regular expression in ASSERT_DEATH use?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-have-a-fixture-class-foo-but-test_ffoo-bar-gives-me-error-no-matching-function-for-call-to-foofoo-why","text":"Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .)","title":"I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-does-assert_death-complain-about-previous-threads-that-were-already-joined","text":"With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this.","title":"Why does ASSERT_DEATH complain about previous threads that were already joined?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#why-does-google-test-require-the-entire-test-case-instead-of-individual-tests-to-be-named-foodeathtest-when-it-uses-assert_death","text":"Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F ( FooTest , AbcDeathTest ) { ... } TEST_F ( FooTest , Uvw ) { ... } TEST_F ( BarTest , DefDeathTest ) { ... } TEST_F ( BarTest , Xyz ) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw .","title":"Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#but-i-dont-like-calling-my-entire-test-case-foodeathtest-when-it-contains-both-death-tests-and-non-death-tests-what-do-i-do","text":"You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public :: testing :: Test { ... }; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } typedef FooTest FooDeathTest ; TEST_F ( FooDeathTest , Uvw ) { ... EXPECT_DEATH (...) ... } TEST_F ( FooDeathTest , Xyz ) { ... ASSERT_DEATH (...) ... }","title":"But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#the-compiler-complains-about-no-match-for-operator-when-i-use-an-assertion-what-gives","text":"If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space.","title":"The compiler complains about \"no match for 'operator&lt;&lt;'\" when I use an assertion. What gives?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-suppress-the-memory-leak-messages-on-windows","text":"Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines.","title":"How do I suppress the memory leak messages on Windows?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-am-building-my-project-with-google-test-in-visual-studio-and-all-im-getting-is-a-bunch-of-linker-errors-or-warnings-help","text":"You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj.","title":"I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-put-my-tests-in-a-library-and-google-test-doesnt-run-them-whats-happening","text":"Have you read a warning on the Google Test Primer page?","title":"I put my tests in a library and Google Test doesn't run them. What's happening?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-want-to-use-google-test-with-visual-studio-but-dont-know-where-to-start","text":"Many people are in your position and one of the posted his solution to our mailing list.","title":"I want to use Google Test with Visual Studio but don't know where to start."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#i-am-seeing-compile-errors-mentioning-stdtype_traits-when-i-try-to-use-google-test-on-solaris","text":"Google Test uses parts of the standard C++ library that SunStudio does not support. Our users reported success using alternative implementations. Try running the build after runing this commad: export CC=cc CXX=CC CXXFLAGS='-library=stlport4'","title":"I am seeing compile errors mentioning std::type_traits when I try to use Google Test on Solaris."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-can-my-code-detect-if-it-is-running-in-a-test","text":"If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and Google Test doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is dependency injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all, there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test.","title":"How can my code detect if it is running in a test?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#google-test-defines-a-macro-that-clashes-with-one-defined-by-another-library-how-do-i-deal-with-that","text":"In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO , you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST ( SomeTest , DoesThis ) { ... } instead of TEST ( SomeTest , DoesThis ) { ... } in order to define a test. Currently, the following TEST , FAIL , SUCCEED , and the basic comparison assertion macros can have alternative names. You can see the full list of covered macros here . More information can be found in the \"Avoiding Macro Name Clashes\" section of the README file.","title":"Google Test defines a macro that clashes with one defined by another library. How do I deal with that?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#is-it-ok-if-i-have-two-separate-testfoo-bar-test-methods-defined-in-different-namespaces","text":"Yes. The rule is all test methods in the same test case must use the same fixture class . This means that the following is allowed because both tests use the same fixture class ( ::testing::Test ). namespace foo { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo However, the following code is not allowed and will produce a runtime error from Google Test because the test methods are using different test fixture classes with the same test case name. namespace foo { class CoolTest : public :: testing :: Test {}; // Fixture foo::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { class CoolTest : public :: testing :: Test {}; // Fixture: bar::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo","title":"Is it OK if I have two separate TEST(Foo, Bar) test methods defined in different namespaces?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#how-do-i-build-google-testing-framework-with-xcode-4","text":"If you try to build Google Test's Xcode project with Xcode 4.0 or later, you may encounter an error message that looks like \"Missing SDK in target gtest_framework: /Developer/SDKs/MacOSX10.4u.sdk\". That means that Xcode does not support the SDK the project is targeting. See the Xcode section in the README file on how to resolve this.","title":"How do I build Google Testing Framework with Xcode 4?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/FAQ/#my-question-is-not-covered-in-your-faq","text":"If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the commit hash if you check out from Git directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.","title":"My question is not covered in your FAQ!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/","text":"Introduction: Why Google C++ Testing Framework? \u00b6 Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test . Setting up a New Test Project \u00b6 To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems: msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script (deprecated) and CMakeLists.txt for CMake (recommended) in the Google Test root directory. If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find \"gtest/gtest.h\" when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples. Basic Concepts \u00b6 When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases. Assertions \u00b6 Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed. Basic Assertions \u00b6 These assertions do basic true/false condition testing. Fatal assertion Nonfatal assertion Verifies ASSERT_TRUE( condition ) ; EXPECT_TRUE( condition ) ; condition is true ASSERT_FALSE( condition ) ; EXPECT_FALSE( condition ) ; condition is false Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac. Binary Comparison \u00b6 This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( val1 , val2 ); EXPECT_EQ( val1 , val2 ); val1 == val2 ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but it's no longer necessary since v1.6.0 (if << is supported, it will be called to print the arguments when the assertion fails; otherwise Google Test will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see this Google Mock recipe .). These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. Historical note : Before February 2016 *_EQ had a convention of calling it as ASSERT_EQ(expected, actual) , so lots of existing code uses this order. Now *_EQ treats both parameters in the same way. String Comparison \u00b6 The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( str1 , str2 ); EXPECT_STREQ( str1 , _str_2 ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( str1 , str2 ); EXPECT_STRCASEEQ( str1 , str2 ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the Advanced Google Test Guide . Simple Tests \u00b6 To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain underscore ( _ ). A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac. Test Fixtures: Using the Same Data Configuration for Multiple Tests \u00b6 If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed. Invoking the Tests \u00b6 TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac. Writing the main() Function \u00b6 You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go. Important note for Visual C++ users \u00b6 If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries! Where to Go from Here \u00b6 Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features. Known Limitations \u00b6 Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Primer"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#introduction-why-google-c-testing-framework","text":"Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test .","title":"Introduction: Why Google C++ Testing Framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#setting-up-a-new-test-project","text":"To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems: msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script (deprecated) and CMakeLists.txt for CMake (recommended) in the Google Test root directory. If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find \"gtest/gtest.h\" when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples.","title":"Setting up a New Test Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#basic-concepts","text":"When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases.","title":"Basic Concepts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#assertions","text":"Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed.","title":"Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#basic-assertions","text":"These assertions do basic true/false condition testing. Fatal assertion Nonfatal assertion Verifies ASSERT_TRUE( condition ) ; EXPECT_TRUE( condition ) ; condition is true ASSERT_FALSE( condition ) ; EXPECT_FALSE( condition ) ; condition is false Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac.","title":"Basic Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#binary-comparison","text":"This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( val1 , val2 ); EXPECT_EQ( val1 , val2 ); val1 == val2 ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but it's no longer necessary since v1.6.0 (if << is supported, it will be called to print the arguments when the assertion fails; otherwise Google Test will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see this Google Mock recipe .). These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. Historical note : Before February 2016 *_EQ had a convention of calling it as ASSERT_EQ(expected, actual) , so lots of existing code uses this order. Now *_EQ treats both parameters in the same way.","title":"Binary Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#string-comparison","text":"The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( str1 , str2 ); EXPECT_STREQ( str1 , _str_2 ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( str1 , str2 ); EXPECT_STRCASEEQ( str1 , str2 ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the Advanced Google Test Guide .","title":"String Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#simple-tests","text":"To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain underscore ( _ ). A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac.","title":"Simple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#test-fixtures-using-the-same-data-configuration-for-multiple-tests","text":"If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed.","title":"Test Fixtures: Using the Same Data Configuration for Multiple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#invoking-the-tests","text":"TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac.","title":"Invoking the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#writing-the-main-function","text":"You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go.","title":"Writing the main() Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#important-note-for-visual-c-users","text":"If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries!","title":"Important note for Visual C++ users"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#where-to-go-from-here","text":"Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features.","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Primer/#known-limitations","text":"Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Known Limitations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/","text":"P ump is U seful for M eta P rogramming. The Problem \u00b6 Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code. Our Solution \u00b6 Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain. Highlights \u00b6 The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode. Examples \u00b6 The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on... Constructs \u00b6 We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [[ code ]] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output. Grammar \u00b6 code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax Code \u00b6 You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump. Real Examples \u00b6 You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h . Tips \u00b6 If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Pump Manual"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#the-problem","text":"Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code.","title":"The Problem"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#our-solution","text":"Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain.","title":"Our Solution"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#highlights","text":"The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode.","title":"Highlights"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#examples","text":"The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on...","title":"Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#constructs","text":"We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [[ code ]] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output.","title":"Constructs"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#grammar","text":"code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax","title":"Grammar"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#code","text":"You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump.","title":"Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#real-examples","text":"You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h .","title":"Real Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/PumpManual/#tips","text":"If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Tips"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/Samples/","text":"If you're like us, you'd like to look at some Google Test sample code. The samples folder has a number of well-commented samples showing how to use a variety of Google Test features. Sample #1 shows the basic steps of using Google Test to test C++ functions. Sample #2 shows a more complex unit test for a class with multiple member functions. Sample #3 uses a test fixture. Sample #4 is another basic example of using Google Test. Sample #5 teaches how to reuse a test fixture in multiple test cases by deriving sub-fixtures from it. Sample #6 demonstrates type-parameterized tests. Sample #7 teaches the basics of value-parameterized tests. Sample #8 shows using Combine() in value-parameterized tests. Sample #9 shows use of the listener API to modify Google Test's console output and the use of its reflection API to inspect test results. Sample #10 shows use of the listener API to implement a primitive memory leak checker.","title":"Samples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/","text":"Now that you have read Primer and learned how to write tests using Google Test, it's time to learn some new tricks. This document will show you more assertions as well as how to construct complex failure messages, propagate fatal failures, reuse and speed up your test fixtures, and use various flags with your tests. More Assertions \u00b6 This section covers some less frequently used, but still significant, assertions. Explicit Success and Failure \u00b6 These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); FAIL* generates a fatal failure while ADD_FAILURE* generates a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Availability : Linux, Windows, Mac. Exception Assertions \u00b6 These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0. Predicate Assertions for Better Error Messages \u00b6 Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem: Using an Existing Boolean Function \u00b6 If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac Using a Function That Returns an AssertionResult \u00b6 While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: !IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: !IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: !IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1. Using a Predicate-Formatter \u00b6 If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 `); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac. Floating-Point Comparison \u00b6 Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison . Floating-Point Macros \u00b6 Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( expected, actual ); EXPECT_FLOAT_EQ( expected, actual ); the two float values are almost equal ASSERT_DOUBLE_EQ( expected, actual ); EXPECT_DOUBLE_EQ( expected, actual ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac. Floating-Point Predicate-Format Functions \u00b6 Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac. Windows HRESULT assertions \u00b6 These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows. Type Assertions \u00b6 You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0. Assertion Placement \u00b6 You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations! Death Tests \u00b6 In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates in an expected fashion is also a death test. If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures . How to Write a Death Test \u00b6 Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); | EXPECT_DEATH( _statement, regex_ ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); | EXPECT_DEATH_IF_SUPPORTED( _statement, regex_ ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); | EXPECT_EXIT( _statement, predicate, regex_ ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(My*DeathTest*, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0. Regular Expression Syntax \u00b6 On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support ( A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only. How It Works \u00b6 Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails. Death Tests And Threads \u00b6 The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent. Death Test Styles \u00b6 The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); } Caveats \u00b6 The statement argument of ASSERT_EXIT() can be any valid C++ statement except that it can not return from the current function. This means statement should not contain return or a macro that might return (e.g. ASSERT_TRUE() ). If statement returns before it crashes, Google Test will print an error message, and the test will fail. Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) . Using Assertions in Sub-routines \u00b6 Adding Traces to Assertions \u00b6 If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac. Propagating Fatal Failures \u00b6 A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections. Asserting on Subroutines \u00b6 As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported. Checking for Failures in the Current Test \u00b6 HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0. Logging Additional Information \u00b6 In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a C string or a 32-bit integer. The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , and classname ). Availability : Linux, Windows, Mac. Sharing Resources Between Tests in the Same Test Case \u00b6 Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac. Global Set-Up and Tear-Down \u00b6 Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac. Value Parameterized Tests \u00b6 Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFooo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You wan to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it! How to Write Value-Parameterized Tests \u00b6 To write value-parameterized tests, first you should define a fixture class. It must be derived from ::testing::TestWithParam<T> , where T is the type of your parameter values. TestWithParam<T> is itself derived from ::testing::Test . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest-filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0. Creating Value-Parameterized Abstract Tests \u00b6 In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files. Typed Tests \u00b6 Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Type-Parameterized Tests \u00b6 Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Testing Private Code \u00b6 If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members Static Functions \u00b6 Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients. Private Class Members \u00b6 Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include <gtest/gtest_prod.h> // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace Catching Failures \u00b6 If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? <gtest/gtest-spi.h> contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in <gtest/internal/gtest-port.h> have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring ); Getting the Current Test's Name \u00b6 Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac. Extending Google Test by Handling Test Events \u00b6 Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0. Defining Event Listeners \u00b6 To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCESS(). virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } }; Using Event Listeners \u00b6 To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier. Generating Failures in Listeners \u00b6 You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here . Running Test Programs: Advanced Options \u00b6 Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } Selecting Tests \u00b6 This section shows various options for choosing which tests to run. Listing Test Names \u00b6 Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac. Running a Subset of the Tests \u00b6 By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac. Temporarily Disabling Tests \u00b6 If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac. Temporarily Enabling Disabled Tests \u00b6 To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0. Repeating the Tests \u00b6 Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac. Shuffling the Tests \u00b6 You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0. Controlling Test Output \u00b6 This section teaches how to tweak the way test results are reported. Colored Terminal Output \u00b6 Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac. Suppressing the Elapsed Time \u00b6 By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.) Generating an XML Report \u00b6 Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Hudson . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\"* errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac. Controlling How Failures Are Reported \u00b6 Turning Assertion Failures into Break-Points \u00b6 When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac. Suppressing Pop-ups Caused by Exceptions \u00b6 On Windows, Google Test may be used with exceptions enabled. Even when exceptions are disabled, an application can still throw structured exceptions (SEH's). If a test throws an exception, by default Google Test doesn't try to catch it. Instead, you'll see a pop-up dialog, at which point you can attach the process to a debugger and easily find out what went wrong. However, if you don't want to see the pop-ups (for example, if you run the tests in a batch job), set the GTEST_CATCH_EXCEPTIONS environment variable to a non- 0 value, or use the --gtest_catch_exceptions flag. Google Test now catches all test-thrown exceptions and logs them as failures. Availability: Windows. GTEST_CATCH_EXCEPTIONS and --gtest_catch_exceptions have no effect on Google Test's behavior on Linux or Mac, even if exceptions are enabled. It is possible to add support for catching exceptions on these platforms, but it is not implemented yet. Letting Another Testing Framework Drive \u00b6 If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include <gtest/gtest.h> int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Availability: Linux, Windows, Mac; since v1.3.0. Distributing Test Functions to Multiple Machines \u00b6 If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0. Fusing Google Test Source Files \u00b6 Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scrpts/test/Makefile file as an example on how to compile your tests against them. Where to Go from Here \u00b6 Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the FAQ .","title":"V1 5 Advanced Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#more-assertions","text":"This section covers some less frequently used, but still significant, assertions.","title":"More Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#explicit-success-and-failure","text":"These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); FAIL* generates a fatal failure while ADD_FAILURE* generates a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Availability : Linux, Windows, Mac.","title":"Explicit Success and Failure"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#exception-assertions","text":"These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0.","title":"Exception Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#predicate-assertions-for-better-error-messages","text":"Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem:","title":"Predicate Assertions for Better Error Messages"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#using-an-existing-boolean-function","text":"If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac","title":"Using an Existing Boolean Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#using-a-function-that-returns-an-assertionresult","text":"While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: !IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: !IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: !IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1.","title":"Using a Function That Returns an AssertionResult"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#using-a-predicate-formatter","text":"If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 `); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac.","title":"Using a Predicate-Formatter"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#floating-point-comparison","text":"Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison .","title":"Floating-Point Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#floating-point-macros","text":"Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( expected, actual ); EXPECT_FLOAT_EQ( expected, actual ); the two float values are almost equal ASSERT_DOUBLE_EQ( expected, actual ); EXPECT_DOUBLE_EQ( expected, actual ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac.","title":"Floating-Point Macros"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#floating-point-predicate-format-functions","text":"Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac.","title":"Floating-Point Predicate-Format Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#windows-hresult-assertions","text":"These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows.","title":"Windows HRESULT assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#type-assertions","text":"You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Type Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#assertion-placement","text":"You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations!","title":"Assertion Placement"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#death-tests","text":"In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates in an expected fashion is also a death test. If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures .","title":"Death Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#how-to-write-a-death-test","text":"Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); | EXPECT_DEATH( _statement, regex_ ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); | EXPECT_DEATH_IF_SUPPORTED( _statement, regex_ ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); | EXPECT_EXIT( _statement, predicate, regex_ ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(My*DeathTest*, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0.","title":"How to Write a Death Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#regular-expression-syntax","text":"On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support ( A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only.","title":"Regular Expression Syntax"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#how-it-works","text":"Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails.","title":"How It Works"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#death-tests-and-threads","text":"The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent.","title":"Death Tests And Threads"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#death-test-styles","text":"The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); }","title":"Death Test Styles"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#caveats","text":"The statement argument of ASSERT_EXIT() can be any valid C++ statement except that it can not return from the current function. This means statement should not contain return or a macro that might return (e.g. ASSERT_TRUE() ). If statement returns before it crashes, Google Test will print an error message, and the test will fail. Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) .","title":"Caveats"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#using-assertions-in-sub-routines","text":"","title":"Using Assertions in Sub-routines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#adding-traces-to-assertions","text":"If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac.","title":"Adding Traces to Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#propagating-fatal-failures","text":"A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections.","title":"Propagating Fatal Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#asserting-on-subroutines","text":"As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported.","title":"Asserting on Subroutines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#checking-for-failures-in-the-current-test","text":"HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0.","title":"Checking for Failures in the Current Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#logging-additional-information","text":"In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a C string or a 32-bit integer. The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , and classname ). Availability : Linux, Windows, Mac.","title":"Logging Additional Information"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#sharing-resources-between-tests-in-the-same-test-case","text":"Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac.","title":"Sharing Resources Between Tests in the Same Test Case"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#global-set-up-and-tear-down","text":"Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac.","title":"Global Set-Up and Tear-Down"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#value-parameterized-tests","text":"Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFooo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You wan to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it!","title":"Value Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#how-to-write-value-parameterized-tests","text":"To write value-parameterized tests, first you should define a fixture class. It must be derived from ::testing::TestWithParam<T> , where T is the type of your parameter values. TestWithParam<T> is itself derived from ::testing::Test . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest-filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0.","title":"How to Write Value-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#creating-value-parameterized-abstract-tests","text":"In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files.","title":"Creating Value-Parameterized Abstract Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#typed-tests","text":"Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Typed Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#type-parameterized-tests","text":"Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Type-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#testing-private-code","text":"If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members","title":"Testing Private Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#static-functions","text":"Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients.","title":"Static Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#private-class-members","text":"Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include <gtest/gtest_prod.h> // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace","title":"Private Class Members"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#catching-failures","text":"If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? <gtest/gtest-spi.h> contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in <gtest/internal/gtest-port.h> have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring );","title":"Catching Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#getting-the-current-tests-name","text":"Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac.","title":"Getting the Current Test's Name"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#extending-google-test-by-handling-test-events","text":"Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Extending Google Test by Handling Test Events"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#defining-event-listeners","text":"To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCESS(). virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } };","title":"Defining Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#using-event-listeners","text":"To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier.","title":"Using Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#generating-failures-in-listeners","text":"You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here .","title":"Generating Failures in Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#running-test-programs-advanced-options","text":"Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); }","title":"Running Test Programs: Advanced Options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#selecting-tests","text":"This section shows various options for choosing which tests to run.","title":"Selecting Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#listing-test-names","text":"Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac.","title":"Listing Test Names"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#running-a-subset-of-the-tests","text":"By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac.","title":"Running a Subset of the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#temporarily-disabling-tests","text":"If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac.","title":"Temporarily Disabling Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#temporarily-enabling-disabled-tests","text":"To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Temporarily Enabling Disabled Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#repeating-the-tests","text":"Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac.","title":"Repeating the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#shuffling-the-tests","text":"You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Shuffling the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#controlling-test-output","text":"This section teaches how to tweak the way test results are reported.","title":"Controlling Test Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#colored-terminal-output","text":"Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac.","title":"Colored Terminal Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#suppressing-the-elapsed-time","text":"By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.)","title":"Suppressing the Elapsed Time"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#generating-an-xml-report","text":"Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Hudson . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\"* errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac.","title":"Generating an XML Report"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#controlling-how-failures-are-reported","text":"","title":"Controlling How Failures Are Reported"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#turning-assertion-failures-into-break-points","text":"When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac.","title":"Turning Assertion Failures into Break-Points"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#suppressing-pop-ups-caused-by-exceptions","text":"On Windows, Google Test may be used with exceptions enabled. Even when exceptions are disabled, an application can still throw structured exceptions (SEH's). If a test throws an exception, by default Google Test doesn't try to catch it. Instead, you'll see a pop-up dialog, at which point you can attach the process to a debugger and easily find out what went wrong. However, if you don't want to see the pop-ups (for example, if you run the tests in a batch job), set the GTEST_CATCH_EXCEPTIONS environment variable to a non- 0 value, or use the --gtest_catch_exceptions flag. Google Test now catches all test-thrown exceptions and logs them as failures. Availability: Windows. GTEST_CATCH_EXCEPTIONS and --gtest_catch_exceptions have no effect on Google Test's behavior on Linux or Mac, even if exceptions are enabled. It is possible to add support for catching exceptions on these platforms, but it is not implemented yet.","title":"Suppressing Pop-ups Caused by Exceptions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#letting-another-testing-framework-drive","text":"If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include <gtest/gtest.h> int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Availability: Linux, Windows, Mac; since v1.3.0.","title":"Letting Another Testing Framework Drive"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#distributing-test-functions-to-multiple-machines","text":"If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Distributing Test Functions to Multiple Machines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#fusing-google-test-source-files","text":"Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scrpts/test/Makefile file as an example on how to compile your tests against them.","title":"Fusing Google Test Source Files"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_AdvancedGuide/#where-to-go-from-here","text":"Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the FAQ .","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Documentation/","text":"This page lists all official documentation wiki pages for Google Test 1.5.0 -- if you use a different version of Google Test, make sure to read the documentation for that version instead. Primer -- start here if you are new to Google Test. Samples -- learn from examples. AdvancedGuide -- learn more about Google Test. XcodeGuide -- how to use Google Test in Xcode on Mac. Frequently-Asked Questions -- check here before asking a question on the mailing list. To contribute code to Google Test, read: DevGuide -- read this before writing your first patch. PumpManual -- how we generate some of Google Test's source files.","title":"V1 5 Documentation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/","text":"If you cannot find the answer to your question here, and you have read Primer and AdvancedGuide , send it to googletestframework@googlegroups.com . Why should I use Google Test instead of my favorite C++ testing framework? \u00b6 First, let's say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable. It works where many STL types (e.g. std::string and std::vector ) don't compile. It doesn't require exceptions or RTTI. As a result, it runs on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. No framework can anticipate all your needs, so Google Test provides EXPECT_PRED* to make it easy to extend your assertion vocabulary. For a nicer syntax, you can define your own assertion macros trivially in terms of EXPECT_PRED* . Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. How do I generate 64-bit binaries on Windows (using Visual Studio 2008)? \u00b6 (Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory. Can I use Google Test on MinGW? \u00b6 We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW. Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)? \u00b6 Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr ! NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros. Does Google Test support running tests in parallel? \u00b6 Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel. Why don't Google Test run the tests in different threads to speed things up? \u00b6 It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes. Why aren't Google Test assertions implemented using exceptions? \u00b6 Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE(...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST . Why do we use two different macros for tests with and without fixtures? \u00b6 Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public ::testing::Test {}; TEST_F(FooTest, DoesThis) { ... } or typedef ::testing::Test FooTest; TEST_F(FooTest, DoesThat) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way. Why don't we use structs as test fixtures? \u00b6 We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes. Why are death tests implemented as assertions instead of using a test runner? \u00b6 Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if (FooCondition()) { ASSERT_DEATH(Bar(), \"blah\"); } else { ASSERT_EQ(5, Bar()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount(); // Only known at run time. for (int i = 1; i <= count; i++) { ASSERT_DEATH({ double* buffer = new double[i]; ... initializes buffer ... Foo(buffer, i) }, \"blah blah\"); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically. My death test modifies some state, but the change seems lost after the death test finishes. Why? \u00b6 Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less. The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong? \u00b6 If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100; }; You also need to define it outside of the class body in foo.cc : const int Foo::kBar; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error. I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations? \u00b6 Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon. Can I derive a test fixture from another? \u00b6 Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public ::testing::Test { protected: ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected: virtual void SetUp() { BaseTest::SetUp(); // Sets up the base fixture first. ... additional set-up work ... } virtual void TearDown() { ... clean-up work for FooTest ... BaseTest::TearDown(); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see samples/sample5_unittest.cc . My compiler complains \"void value not ignored as it ought to be.\" What does this mean? \u00b6 You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions. My death test hangs (or seg-faults). How do I fix it? \u00b6 In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry! Should I use the constructor/destructor of the test fixture or the set-up/tear-down function? \u00b6 The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test, call TearDown() , and then immediately delete the test fixture object. Therefore, there is no need to write a SetUp() or TearDown() function if the constructor or destructor already does the job. You may still want to use SetUp()/TearDown() in the following cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The Google Test team is considering making the assertion macros throw on platforms where exceptions are enabled (e.g. Windows, Mac OS, and Linux client-side), which will eliminate the need for the user to propagate failures from a subroutine to its caller. Therefore, you shouldn't use Google Test assertions in a destructor if your code could run on such a platform. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() . The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it? \u00b6 If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive(int n) { return n > 0; } bool IsPositive(double x) { return x > 0; } you will get a compiler error if you write EXPECT_PRED1(IsPositive, 5); However, this will work: EXPECT_PRED1(*static_cast<bool (*)(int)>*(IsPositive), 5); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template <typename T> bool IsNegative(T x) { return x < 0; } you can use it in a predicate assertion like this: ASSERT_PRED1(IsNegative*<int>*, -5); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2(*GreaterThan<int, int>*, 5, 0); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2(*(GreaterThan<int, int>)*, 5, 0); My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why? \u00b6 Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS(); they write RUN_ALL_TESTS(); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() . My compiler complains that a constructor (or destructor) cannot return a value. What's going on? \u00b6 Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ(1, Foo()) << \"blah blah\" << foo; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it. My set-up function is not called. Why? \u00b6 C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called. How do I jump to the line of a failure in Emacs directly? \u00b6 Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure. I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious. \u00b6 You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } class BarTest : public BaseTest {}; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef BaseTest BarTest; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } The Google Test output is buried in a whole bunch of log messages. What do I do? \u00b6 The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt Why should I prefer test fixtures over global variables? \u00b6 There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common. How do I test private class members without writing FRIEND_TEST()s? \u00b6 You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... void Test1() {...} // This accesses private members of class Foo. void Test2() {...} // So does this one. }; TEST_F(FooTest, Test1) { Test1(); } TEST_F(FooTest, Test2) { Test2(); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... T1 get_private_member1(Foo* obj) { return obj->private_member1_; } }; TEST_F(FooTest, Test1) { ... get_private_member1(x) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected: // protected access for testability. int DoSomethingReturningInt(); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public: using YourClass::DoSomethingReturningInt; // changes access rights ... }; TEST_F(YourClassTest, DoSomethingTest) { TestableYourClass obj; assertEquals(expected_value, obj.DoSomethingReturningInt()); } How do I test private class static members without writing FRIEND_TEST()s? \u00b6 We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private: static bool Func(int n); }; // foo.cc bool Foo::Func(int n) { ... } // foo_test.cc EXPECT_TRUE(Foo::Func(12345)); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func(int n) { ... } } // foo_test.cc namespace internal { bool Func(int n); } EXPECT_TRUE(internal::Func(12345)); I would like to run a test several times with different parameters. Do I need to write several similar copies of it? \u00b6 No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once. How do I test a file that defines main()? \u00b6 To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort. What can the statement argument in ASSERT_DEATH() be? \u00b6 ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST(MyDeathTest, FunctionCall) { ASSERT_DEATH(Xyz(5), \"Xyz failed\"); } // Or a complex expression that references variables and functions. TEST(MyDeathTest, ComplexExpression) { const bool c = Condition(); ASSERT_DEATH((c ? Func1(0) : object2.Method(\"test\")), \"(Func1|Method) failed\"); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST(MyDeathTest, InsideLoop) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for (int i = 0; i < 5; i++) { EXPECT_DEATH_M(Foo(i), \"Foo has \\\\d+ errors\", ::testing::Message() << \"where i is \" << i); } } // A death assertion can contain a compound statement. TEST(MyDeathTest, CompoundStatement) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH({ for (int i = 0; i < 5; i++) { Bar(i); } }, \"Bar has \\\\d+ errors\");} googletest_unittest.cc contains more examples if you are interested. What syntax does the regular expression in ASSERT_DEATH use? \u00b6 On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax . I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why? \u00b6 Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .) Why does ASSERT_DEATH complain about previous threads that were already joined? \u00b6 With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this. Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH? \u00b6 Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F(FooTest, AbcDeathTest) { ... } TEST_F(FooTest, Uvw) { ... } TEST_F(BarTest, DefDeathTest) { ... } TEST_F(BarTest, Xyz) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw . But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do? \u00b6 You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public ::testing::Test { ... }; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef FooTest FooDeathTest; TEST_F(FooDeathTest, Uvw) { ... EXPECT_DEATH(...) ... } TEST_F(FooDeathTest, Xyz) { ... ASSERT_DEATH(...) ... } The compiler complains about \"no match for 'operator<<'\" when I use an assertion. What gives? \u00b6 If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space. How do I suppress the memory leak messages on Windows? \u00b6 Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines. I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help! \u00b6 You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj. I put my tests in a library and Google Test doesn't run them. What's happening? \u00b6 Have you read a warning on the Google Test Primer page? I want to use Google Test with Visual Studio but don't know where to start. \u00b6 Many people are in your position and one of the posted his solution to our mailing list. Here is his link: http://hassanjamilahmad.blogspot.com/2009/07/gtest-starters-help.html . My question is not covered in your FAQ! \u00b6 If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the revision number if you check out from SVN directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.\uf701","title":"V1 5 FAQ"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-should-i-use-google-test-instead-of-my-favorite-c-testing-framework","text":"First, let's say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable. It works where many STL types (e.g. std::string and std::vector ) don't compile. It doesn't require exceptions or RTTI. As a result, it runs on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. No framework can anticipate all your needs, so Google Test provides EXPECT_PRED* to make it easy to extend your assertion vocabulary. For a nicer syntax, you can define your own assertion macros trivially in terms of EXPECT_PRED* . Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure.","title":"Why should I use Google Test instead of my favorite C++ testing framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#how-do-i-generate-64-bit-binaries-on-windows-using-visual-studio-2008","text":"(Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory.","title":"How do I generate 64-bit binaries on Windows (using Visual Studio 2008)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#can-i-use-google-test-on-mingw","text":"We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW.","title":"Can I use Google Test on MinGW?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-does-google-test-support-expect_eqnull-ptr-and-assert_eqnull-ptr-but-not-expect_nenull-ptr-and-assert_nenull-ptr","text":"Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr ! NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros.","title":"Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#does-google-test-support-running-tests-in-parallel","text":"Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel.","title":"Does Google Test support running tests in parallel?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-dont-google-test-run-the-tests-in-different-threads-to-speed-things-up","text":"It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes.","title":"Why don't Google Test run the tests in different threads to speed things up?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-arent-google-test-assertions-implemented-using-exceptions","text":"Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE(...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST .","title":"Why aren't Google Test assertions implemented using exceptions?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-do-we-use-two-different-macros-for-tests-with-and-without-fixtures","text":"Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public ::testing::Test {}; TEST_F(FooTest, DoesThis) { ... } or typedef ::testing::Test FooTest; TEST_F(FooTest, DoesThat) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way.","title":"Why do we use two different macros for tests with and without fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-dont-we-use-structs-as-test-fixtures","text":"We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes.","title":"Why don't we use structs as test fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-are-death-tests-implemented-as-assertions-instead-of-using-a-test-runner","text":"Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if (FooCondition()) { ASSERT_DEATH(Bar(), \"blah\"); } else { ASSERT_EQ(5, Bar()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount(); // Only known at run time. for (int i = 1; i <= count; i++) { ASSERT_DEATH({ double* buffer = new double[i]; ... initializes buffer ... Foo(buffer, i) }, \"blah blah\"); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically.","title":"Why are death tests implemented as assertions instead of using a test runner?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-death-test-modifies-some-state-but-the-change-seems-lost-after-the-death-test-finishes-why","text":"Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less.","title":"My death test modifies some state, but the change seems lost after the death test finishes. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#the-compiler-complains-about-undefined-references-to-some-static-const-member-variables-but-i-did-define-them-in-the-class-body-whats-wrong","text":"If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100; }; You also need to define it outside of the class body in foo.cc : const int Foo::kBar; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error.","title":"The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-have-an-interface-that-has-several-implementations-can-i-write-a-set-of-tests-once-and-repeat-them-over-all-the-implementations","text":"Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon.","title":"I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#can-i-derive-a-test-fixture-from-another","text":"Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public ::testing::Test { protected: ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected: virtual void SetUp() { BaseTest::SetUp(); // Sets up the base fixture first. ... additional set-up work ... } virtual void TearDown() { ... clean-up work for FooTest ... BaseTest::TearDown(); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see samples/sample5_unittest.cc .","title":"Can I derive a test fixture from another?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-compiler-complains-void-value-not-ignored-as-it-ought-to-be-what-does-this-mean","text":"You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions.","title":"My compiler complains \"void value not ignored as it ought to be.\" What does this mean?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-death-test-hangs-or-seg-faults-how-do-i-fix-it","text":"In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry!","title":"My death test hangs (or seg-faults). How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#should-i-use-the-constructordestructor-of-the-test-fixture-or-the-set-uptear-down-function","text":"The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test, call TearDown() , and then immediately delete the test fixture object. Therefore, there is no need to write a SetUp() or TearDown() function if the constructor or destructor already does the job. You may still want to use SetUp()/TearDown() in the following cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The Google Test team is considering making the assertion macros throw on platforms where exceptions are enabled (e.g. Windows, Mac OS, and Linux client-side), which will eliminate the need for the user to propagate failures from a subroutine to its caller. Therefore, you shouldn't use Google Test assertions in a destructor if your code could run on such a platform. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() .","title":"Should I use the constructor/destructor of the test fixture or the set-up/tear-down function?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#the-compiler-complains-no-matching-function-to-call-when-i-use-assert_predn-how-do-i-fix-it","text":"If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive(int n) { return n > 0; } bool IsPositive(double x) { return x > 0; } you will get a compiler error if you write EXPECT_PRED1(IsPositive, 5); However, this will work: EXPECT_PRED1(*static_cast<bool (*)(int)>*(IsPositive), 5); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template <typename T> bool IsNegative(T x) { return x < 0; } you can use it in a predicate assertion like this: ASSERT_PRED1(IsNegative*<int>*, -5); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2(*GreaterThan<int, int>*, 5, 0); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2(*(GreaterThan<int, int>)*, 5, 0);","title":"The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-compiler-complains-about-ignoring-return-value-when-i-call-run_all_tests-why","text":"Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS(); they write RUN_ALL_TESTS(); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() .","title":"My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-compiler-complains-that-a-constructor-or-destructor-cannot-return-a-value-whats-going-on","text":"Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ(1, Foo()) << \"blah blah\" << foo; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it.","title":"My compiler complains that a constructor (or destructor) cannot return a value. What's going on?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-set-up-function-is-not-called-why","text":"C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called.","title":"My set-up function is not called. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#how-do-i-jump-to-the-line-of-a-failure-in-emacs-directly","text":"Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure.","title":"How do I jump to the line of a failure in Emacs directly?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-have-several-test-cases-which-share-the-same-test-fixture-logic-do-i-have-to-define-a-new-test-fixture-class-for-each-of-them-this-seems-pretty-tedious","text":"You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } class BarTest : public BaseTest {}; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef BaseTest BarTest; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... }","title":"I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#the-google-test-output-is-buried-in-a-whole-bunch-of-log-messages-what-do-i-do","text":"The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt","title":"The Google Test output is buried in a whole bunch of log messages. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-should-i-prefer-test-fixtures-over-global-variables","text":"There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common.","title":"Why should I prefer test fixtures over global variables?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#how-do-i-test-private-class-members-without-writing-friend_tests","text":"You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... void Test1() {...} // This accesses private members of class Foo. void Test2() {...} // So does this one. }; TEST_F(FooTest, Test1) { Test1(); } TEST_F(FooTest, Test2) { Test2(); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... T1 get_private_member1(Foo* obj) { return obj->private_member1_; } }; TEST_F(FooTest, Test1) { ... get_private_member1(x) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected: // protected access for testability. int DoSomethingReturningInt(); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public: using YourClass::DoSomethingReturningInt; // changes access rights ... }; TEST_F(YourClassTest, DoSomethingTest) { TestableYourClass obj; assertEquals(expected_value, obj.DoSomethingReturningInt()); }","title":"How do I test private class members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#how-do-i-test-private-class-static-members-without-writing-friend_tests","text":"We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private: static bool Func(int n); }; // foo.cc bool Foo::Func(int n) { ... } // foo_test.cc EXPECT_TRUE(Foo::Func(12345)); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func(int n) { ... } } // foo_test.cc namespace internal { bool Func(int n); } EXPECT_TRUE(internal::Func(12345));","title":"How do I test private class static members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-would-like-to-run-a-test-several-times-with-different-parameters-do-i-need-to-write-several-similar-copies-of-it","text":"No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once.","title":"I would like to run a test several times with different parameters. Do I need to write several similar copies of it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#how-do-i-test-a-file-that-defines-main","text":"To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort.","title":"How do I test a file that defines main()?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#what-can-the-statement-argument-in-assert_death-be","text":"ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST(MyDeathTest, FunctionCall) { ASSERT_DEATH(Xyz(5), \"Xyz failed\"); } // Or a complex expression that references variables and functions. TEST(MyDeathTest, ComplexExpression) { const bool c = Condition(); ASSERT_DEATH((c ? Func1(0) : object2.Method(\"test\")), \"(Func1|Method) failed\"); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST(MyDeathTest, InsideLoop) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for (int i = 0; i < 5; i++) { EXPECT_DEATH_M(Foo(i), \"Foo has \\\\d+ errors\", ::testing::Message() << \"where i is \" << i); } } // A death assertion can contain a compound statement. TEST(MyDeathTest, CompoundStatement) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH({ for (int i = 0; i < 5; i++) { Bar(i); } }, \"Bar has \\\\d+ errors\");} googletest_unittest.cc contains more examples if you are interested.","title":"What can the statement argument in ASSERT_DEATH() be?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#what-syntax-does-the-regular-expression-in-assert_death-use","text":"On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax .","title":"What syntax does the regular expression in ASSERT_DEATH use?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-have-a-fixture-class-foo-but-test_ffoo-bar-gives-me-error-no-matching-function-for-call-to-foofoo-why","text":"Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .)","title":"I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-does-assert_death-complain-about-previous-threads-that-were-already-joined","text":"With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this.","title":"Why does ASSERT_DEATH complain about previous threads that were already joined?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#why-does-google-test-require-the-entire-test-case-instead-of-individual-tests-to-be-named-foodeathtest-when-it-uses-assert_death","text":"Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F(FooTest, AbcDeathTest) { ... } TEST_F(FooTest, Uvw) { ... } TEST_F(BarTest, DefDeathTest) { ... } TEST_F(BarTest, Xyz) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw .","title":"Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#but-i-dont-like-calling-my-entire-test-case-foodeathtest-when-it-contains-both-death-tests-and-non-death-tests-what-do-i-do","text":"You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public ::testing::Test { ... }; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef FooTest FooDeathTest; TEST_F(FooDeathTest, Uvw) { ... EXPECT_DEATH(...) ... } TEST_F(FooDeathTest, Xyz) { ... ASSERT_DEATH(...) ... }","title":"But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#the-compiler-complains-about-no-match-for-operator-when-i-use-an-assertion-what-gives","text":"If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space.","title":"The compiler complains about \"no match for 'operator&lt;&lt;'\" when I use an assertion. What gives?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#how-do-i-suppress-the-memory-leak-messages-on-windows","text":"Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines.","title":"How do I suppress the memory leak messages on Windows?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-am-building-my-project-with-google-test-in-visual-studio-and-all-im-getting-is-a-bunch-of-linker-errors-or-warnings-help","text":"You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj.","title":"I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-put-my-tests-in-a-library-and-google-test-doesnt-run-them-whats-happening","text":"Have you read a warning on the Google Test Primer page?","title":"I put my tests in a library and Google Test doesn't run them. What's happening?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#i-want-to-use-google-test-with-visual-studio-but-dont-know-where-to-start","text":"Many people are in your position and one of the posted his solution to our mailing list. Here is his link: http://hassanjamilahmad.blogspot.com/2009/07/gtest-starters-help.html .","title":"I want to use Google Test with Visual Studio but don't know where to start."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_FAQ/#my-question-is-not-covered-in-your-faq","text":"If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the revision number if you check out from SVN directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.\uf701","title":"My question is not covered in your FAQ!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/","text":"Introduction: Why Google C++ Testing Framework? \u00b6 Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test . Setting up a New Test Project \u00b6 To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems ( msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script in the Google Test root directory). If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find <gtest/gtest.h> when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples. Basic Concepts \u00b6 When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases. Assertions \u00b6 Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed. Basic Assertions \u00b6 These assertions do basic true/false condition testing. | Fatal assertion | Nonfatal assertion | Verifies | |:--------------------|:-----------------------|:-------------| | ASSERT_TRUE( condition ) ; | EXPECT_TRUE( condition ) ; | condition is true | | ASSERT_FALSE( condition ) ; | EXPECT_FALSE( condition ) ; | condition is false | Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac. Binary Comparison \u00b6 This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( expected , actual ); EXPECT_EQ( expected , actual ); expected == actual ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll introduce later), you should put the expression you want to test in the position of actual , and put its expected value in expected , as Google Test's failure messages are optimized for this convention. Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. Values must also support the << operator for streaming to an ostream . All built-in types support this. These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. String Comparison \u00b6 The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( expected_str , actual_str ); EXPECT_STREQ( expected_str , actual_str ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( expected_str , actual_str ); EXPECT_STRCASEEQ( expected_str , actual_str ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the [AdvancedGuide Advanced Google Test Guide]. Simple Tests \u00b6 To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Remember that a test case can contain any number of individual tests. A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac. Test Fixtures: Using the Same Data Configuration for Multiple Tests \u00b6 If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed. Invoking the Tests \u00b6 TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac. Writing the main() Function \u00b6 You can start from this boilerplate: #include \"this/package/foo.h\" #include <gtest/gtest.h> namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go. Important note for Visual C++ users \u00b6 If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries! Where to Go from Here \u00b6 Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features. Known Limitations \u00b6 Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"V1 5 Primer"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#introduction-why-google-c-testing-framework","text":"Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test .","title":"Introduction: Why Google C++ Testing Framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#setting-up-a-new-test-project","text":"To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems ( msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script in the Google Test root directory). If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find <gtest/gtest.h> when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples.","title":"Setting up a New Test Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#basic-concepts","text":"When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases.","title":"Basic Concepts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#assertions","text":"Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed.","title":"Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#basic-assertions","text":"These assertions do basic true/false condition testing. | Fatal assertion | Nonfatal assertion | Verifies | |:--------------------|:-----------------------|:-------------| | ASSERT_TRUE( condition ) ; | EXPECT_TRUE( condition ) ; | condition is true | | ASSERT_FALSE( condition ) ; | EXPECT_FALSE( condition ) ; | condition is false | Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac.","title":"Basic Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#binary-comparison","text":"This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( expected , actual ); EXPECT_EQ( expected , actual ); expected == actual ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll introduce later), you should put the expression you want to test in the position of actual , and put its expected value in expected , as Google Test's failure messages are optimized for this convention. Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. Values must also support the << operator for streaming to an ostream . All built-in types support this. These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac.","title":"Binary Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#string-comparison","text":"The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( expected_str , actual_str ); EXPECT_STREQ( expected_str , actual_str ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( expected_str , actual_str ); EXPECT_STRCASEEQ( expected_str , actual_str ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the [AdvancedGuide Advanced Google Test Guide].","title":"String Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#simple-tests","text":"To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Remember that a test case can contain any number of individual tests. A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac.","title":"Simple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#test-fixtures-using-the-same-data-configuration-for-multiple-tests","text":"If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed.","title":"Test Fixtures: Using the Same Data Configuration for Multiple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#invoking-the-tests","text":"TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac.","title":"Invoking the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#writing-the-main-function","text":"You can start from this boilerplate: #include \"this/package/foo.h\" #include <gtest/gtest.h> namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go.","title":"Writing the main() Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#important-note-for-visual-c-users","text":"If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries!","title":"Important note for Visual C++ users"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#where-to-go-from-here","text":"Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features.","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_Primer/#known-limitations","text":"Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Known Limitations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/","text":"P ump is U seful for M eta P rogramming. The Problem \u00b6 Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code. Our Solution \u00b6 Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain. Highlights \u00b6 The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode. Examples \u00b6 The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on... Constructs \u00b6 We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [ code ] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output. Grammar \u00b6 code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax Code \u00b6 You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump. Real Examples \u00b6 You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h . Tips \u00b6 If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"V1 5 Pump Manual"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#the-problem","text":"Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code.","title":"The Problem"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#our-solution","text":"Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain.","title":"Our Solution"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#highlights","text":"The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode.","title":"Highlights"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#examples","text":"The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on...","title":"Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#constructs","text":"We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [ code ] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output.","title":"Constructs"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#grammar","text":"code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax","title":"Grammar"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#code","text":"You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump.","title":"Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#real-examples","text":"You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h .","title":"Real Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_PumpManual/#tips","text":"If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Tips"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/","text":"This guide will explain how to use the Google Testing Framework in your Xcode projects on Mac OS X. This tutorial begins by quickly explaining what to do for experienced users. After the quick start, the guide goes provides additional explanation about each step. Quick Start \u00b6 Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations. Get the Source \u00b6 Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk Add the Framework to Your Project \u00b6 The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below). Make a Test Target \u00b6 To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase. Set Up the Executable Run Environment \u00b6 Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH. Build and Go \u00b6 Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0. Summary \u00b6 Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"V1 5 Xcode Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#quick-start","text":"Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations.","title":"Quick Start"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#get-the-source","text":"Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk","title":"Get the Source"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#add-the-framework-to-your-project","text":"The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below).","title":"Add the Framework to Your Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#make-a-test-target","text":"To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase.","title":"Make a Test Target"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#set-up-the-executable-run-environment","text":"Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH.","title":"Set Up the Executable Run Environment"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#build-and-go","text":"Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0.","title":"Build and Go"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_5_XcodeGuide/#summary","text":"Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"Summary"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/","text":"Now that you have read Primer and learned how to write tests using Google Test, it's time to learn some new tricks. This document will show you more assertions as well as how to construct complex failure messages, propagate fatal failures, reuse and speed up your test fixtures, and use various flags with your tests. More Assertions \u00b6 This section covers some less frequently used, but still significant, assertions. Explicit Success and Failure \u00b6 These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); ADD_FAILURE_AT(\" file_path \", line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Availability : Linux, Windows, Mac. Exception Assertions \u00b6 These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0. Predicate Assertions for Better Error Messages \u00b6 Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem: Using an Existing Boolean Function \u00b6 If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac Using a Function That Returns an AssertionResult \u00b6 While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: !IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: !IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: !IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1. Using a Predicate-Formatter \u00b6 If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 `); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac. Floating-Point Comparison \u00b6 Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison . Floating-Point Macros \u00b6 Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( expected, actual ); EXPECT_FLOAT_EQ( expected, actual ); the two float values are almost equal ASSERT_DOUBLE_EQ( expected, actual ); EXPECT_DOUBLE_EQ( expected, actual ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac. Floating-Point Predicate-Format Functions \u00b6 Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac. Windows HRESULT assertions \u00b6 These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows. Type Assertions \u00b6 You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0. Assertion Placement \u00b6 You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations! Teaching Google Test How to Print Your Values \u00b6 When a test assertion such as EXPECT_EQ fails, Google Test prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <iostream> namespace foo { class Bar { ... }; // We want Google Test to be able to print instances of this. // It's important that the << operator is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. ::std::ostream& operator<<(::std::ostream& os, const Bar& bar) { return os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <iostream> namespace foo { class Bar { ... }; // It's important that PrintTo() is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. void PrintTo(const Bar& bar, ::std::ostream* os) { *os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when Google Test is concerned. This allows you to customize how the value appears in Google Test's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using Google Test's value printer yourself, just call ::testing::PrintToString( x ) , which returns an std::string : vector<pair<Bar, int> > bar_ints = GetBarIntVector(); EXPECT_TRUE(IsCorrectBarIntVector(bar_ints)) << \"bar_ints = \" << ::testing::PrintToString(bar_ints); Death Tests \u00b6 In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures . How to Write a Death Test \u00b6 Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); | EXPECT_DEATH( _statement, regex_ ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); | EXPECT_DEATH_IF_SUPPORTED( _statement, regex_ ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); | EXPECT_EXIT( _statement, predicate, regex_ ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(My*DeathTest*, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0. Regular Expression Syntax \u00b6 On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support ( A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only. How It Works \u00b6 Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails. Death Tests And Threads \u00b6 The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent. Death Test Styles \u00b6 The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); } Caveats \u00b6 The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some Google Test macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) . Using Assertions in Sub-routines \u00b6 Adding Traces to Assertions \u00b6 If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac. Propagating Fatal Failures \u00b6 A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections. Asserting on Subroutines \u00b6 As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported. Checking for Failures in the Current Test \u00b6 HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0. Logging Additional Information \u00b6 In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a C string or a 32-bit integer. The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , and classname ). Availability : Linux, Windows, Mac. Sharing Resources Between Tests in the Same Test Case \u00b6 Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac. Global Set-Up and Tear-Down \u00b6 Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac. Value Parameterized Tests \u00b6 Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFooo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it! How to Write Value-Parameterized Tests \u00b6 To write value-parameterized tests, first you should define a fixture class. It must be derived from both ::testing::Test and ::testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from ::testing::TestWithParam<T> , which itself is derived from both ::testing::Test and ::testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public ::testing::Test { ... }; class BarTest : public BaseTest, public ::testing::WithParamInterface<const char*> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . container , begin , and end can be expressions whose values are determined at run time. Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest-filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0. Creating Value-Parameterized Abstract Tests \u00b6 In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files. Typed Tests \u00b6 Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Type-Parameterized Tests \u00b6 Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Testing Private Code \u00b6 If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members Static Functions \u00b6 Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients. Private Class Members \u00b6 Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include \"gtest/gtest_prod.h\" // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace Catching Failures \u00b6 If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? \"gtest/gtest-spi.h\" contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in \"gtest/internal/gtest-port.h\" have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring ); Getting the Current Test's Name \u00b6 Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac. Extending Google Test by Handling Test Events \u00b6 Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0. Defining Event Listeners \u00b6 To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCEED() invocation. virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } }; Using Event Listeners \u00b6 To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier. Generating Failures in Listeners \u00b6 You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here . Running Test Programs: Advanced Options \u00b6 Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } Selecting Tests \u00b6 This section shows various options for choosing which tests to run. Listing Test Names \u00b6 Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac. Running a Subset of the Tests \u00b6 By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac. Temporarily Disabling Tests \u00b6 If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac. Temporarily Enabling Disabled Tests \u00b6 To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest-filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0. Repeating the Tests \u00b6 Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac. Shuffling the Tests \u00b6 You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0. Controlling Test Output \u00b6 This section teaches how to tweak the way test results are reported. Colored Terminal Output \u00b6 Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac. Suppressing the Elapsed Time \u00b6 By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.) Generating an XML Report \u00b6 Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Hudson . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac. Controlling How Failures Are Reported \u00b6 Turning Assertion Failures into Break-Points \u00b6 When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac. Disabling Catching Test-Thrown Exceptions \u00b6 Google Test can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default Google Test catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests. Availability : Linux, Windows, Mac. Letting Another Testing Framework Drive \u00b6 If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include \"gtest/gtest.h\" int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Availability: Linux, Windows, Mac; since v1.3.0. Distributing Test Functions to Multiple Machines \u00b6 If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0. Fusing Google Test Source Files \u00b6 Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scripts/test/Makefile file as an example on how to compile your tests against them. Where to Go from Here \u00b6 Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the Frequently-Asked Questions .","title":"V1 6 Advanced Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#more-assertions","text":"This section covers some less frequently used, but still significant, assertions.","title":"More Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#explicit-success-and-failure","text":"These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); ADD_FAILURE_AT(\" file_path \", line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Availability : Linux, Windows, Mac.","title":"Explicit Success and Failure"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#exception-assertions","text":"These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0.","title":"Exception Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#predicate-assertions-for-better-error-messages","text":"Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem:","title":"Predicate Assertions for Better Error Messages"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#using-an-existing-boolean-function","text":"If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac","title":"Using an Existing Boolean Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#using-a-function-that-returns-an-assertionresult","text":"While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: !IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: !IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: !IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1.","title":"Using a Function That Returns an AssertionResult"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#using-a-predicate-formatter","text":"If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 `); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac.","title":"Using a Predicate-Formatter"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#floating-point-comparison","text":"Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison .","title":"Floating-Point Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#floating-point-macros","text":"Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( expected, actual ); EXPECT_FLOAT_EQ( expected, actual ); the two float values are almost equal ASSERT_DOUBLE_EQ( expected, actual ); EXPECT_DOUBLE_EQ( expected, actual ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac.","title":"Floating-Point Macros"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#floating-point-predicate-format-functions","text":"Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac.","title":"Floating-Point Predicate-Format Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#windows-hresult-assertions","text":"These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows.","title":"Windows HRESULT assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#type-assertions","text":"You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Type Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#assertion-placement","text":"You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations!","title":"Assertion Placement"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#teaching-google-test-how-to-print-your-values","text":"When a test assertion such as EXPECT_EQ fails, Google Test prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <iostream> namespace foo { class Bar { ... }; // We want Google Test to be able to print instances of this. // It's important that the << operator is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. ::std::ostream& operator<<(::std::ostream& os, const Bar& bar) { return os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <iostream> namespace foo { class Bar { ... }; // It's important that PrintTo() is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. void PrintTo(const Bar& bar, ::std::ostream* os) { *os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when Google Test is concerned. This allows you to customize how the value appears in Google Test's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using Google Test's value printer yourself, just call ::testing::PrintToString( x ) , which returns an std::string : vector<pair<Bar, int> > bar_ints = GetBarIntVector(); EXPECT_TRUE(IsCorrectBarIntVector(bar_ints)) << \"bar_ints = \" << ::testing::PrintToString(bar_ints);","title":"Teaching Google Test How to Print Your Values"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#death-tests","text":"In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures .","title":"Death Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#how-to-write-a-death-test","text":"Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); | EXPECT_DEATH( _statement, regex_ ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); | EXPECT_DEATH_IF_SUPPORTED( _statement, regex_ ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); | EXPECT_EXIT( _statement, predicate, regex_ ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(My*DeathTest*, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0.","title":"How to Write a Death Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#regular-expression-syntax","text":"On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support ( A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only.","title":"Regular Expression Syntax"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#how-it-works","text":"Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails.","title":"How It Works"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#death-tests-and-threads","text":"The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent.","title":"Death Tests And Threads"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#death-test-styles","text":"The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); }","title":"Death Test Styles"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#caveats","text":"The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some Google Test macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) .","title":"Caveats"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#using-assertions-in-sub-routines","text":"","title":"Using Assertions in Sub-routines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#adding-traces-to-assertions","text":"If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac.","title":"Adding Traces to Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#propagating-fatal-failures","text":"A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections.","title":"Propagating Fatal Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#asserting-on-subroutines","text":"As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported.","title":"Asserting on Subroutines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#checking-for-failures-in-the-current-test","text":"HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0.","title":"Checking for Failures in the Current Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#logging-additional-information","text":"In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a C string or a 32-bit integer. The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , and classname ). Availability : Linux, Windows, Mac.","title":"Logging Additional Information"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#sharing-resources-between-tests-in-the-same-test-case","text":"Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac.","title":"Sharing Resources Between Tests in the Same Test Case"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#global-set-up-and-tear-down","text":"Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac.","title":"Global Set-Up and Tear-Down"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#value-parameterized-tests","text":"Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFooo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it!","title":"Value Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#how-to-write-value-parameterized-tests","text":"To write value-parameterized tests, first you should define a fixture class. It must be derived from both ::testing::Test and ::testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from ::testing::TestWithParam<T> , which itself is derived from both ::testing::Test and ::testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public ::testing::Test { ... }; class BarTest : public BaseTest, public ::testing::WithParamInterface<const char*> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . container , begin , and end can be expressions whose values are determined at run time. Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest-filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0.","title":"How to Write Value-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#creating-value-parameterized-abstract-tests","text":"In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files.","title":"Creating Value-Parameterized Abstract Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#typed-tests","text":"Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Typed Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#type-parameterized-tests","text":"Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Type-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#testing-private-code","text":"If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members","title":"Testing Private Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#static-functions","text":"Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients.","title":"Static Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#private-class-members","text":"Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include \"gtest/gtest_prod.h\" // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace","title":"Private Class Members"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#catching-failures","text":"If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? \"gtest/gtest-spi.h\" contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in \"gtest/internal/gtest-port.h\" have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring );","title":"Catching Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#getting-the-current-tests-name","text":"Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac.","title":"Getting the Current Test's Name"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#extending-google-test-by-handling-test-events","text":"Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Extending Google Test by Handling Test Events"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#defining-event-listeners","text":"To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCEED() invocation. virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } };","title":"Defining Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#using-event-listeners","text":"To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier.","title":"Using Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#generating-failures-in-listeners","text":"You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here .","title":"Generating Failures in Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#running-test-programs-advanced-options","text":"Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); }","title":"Running Test Programs: Advanced Options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#selecting-tests","text":"This section shows various options for choosing which tests to run.","title":"Selecting Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#listing-test-names","text":"Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac.","title":"Listing Test Names"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#running-a-subset-of-the-tests","text":"By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac.","title":"Running a Subset of the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#temporarily-disabling-tests","text":"If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac.","title":"Temporarily Disabling Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#temporarily-enabling-disabled-tests","text":"To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest-filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Temporarily Enabling Disabled Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#repeating-the-tests","text":"Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac.","title":"Repeating the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#shuffling-the-tests","text":"You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Shuffling the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#controlling-test-output","text":"This section teaches how to tweak the way test results are reported.","title":"Controlling Test Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#colored-terminal-output","text":"Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac.","title":"Colored Terminal Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#suppressing-the-elapsed-time","text":"By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.)","title":"Suppressing the Elapsed Time"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#generating-an-xml-report","text":"Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Hudson . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac.","title":"Generating an XML Report"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#controlling-how-failures-are-reported","text":"","title":"Controlling How Failures Are Reported"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#turning-assertion-failures-into-break-points","text":"When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac.","title":"Turning Assertion Failures into Break-Points"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#disabling-catching-test-thrown-exceptions","text":"Google Test can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default Google Test catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests. Availability : Linux, Windows, Mac.","title":"Disabling Catching Test-Thrown Exceptions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#letting-another-testing-framework-drive","text":"If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include \"gtest/gtest.h\" int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Availability: Linux, Windows, Mac; since v1.3.0.","title":"Letting Another Testing Framework Drive"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#distributing-test-functions-to-multiple-machines","text":"If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Distributing Test Functions to Multiple Machines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#fusing-google-test-source-files","text":"Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scripts/test/Makefile file as an example on how to compile your tests against them.","title":"Fusing Google Test Source Files"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_AdvancedGuide/#where-to-go-from-here","text":"Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the Frequently-Asked Questions .","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Documentation/","text":"This page lists all documentation wiki pages for Google Test 1.6 -- if you use a released version of Google Test, please read the documentation for that specific version instead. Primer -- start here if you are new to Google Test. Samples -- learn from examples. AdvancedGuide -- learn more about Google Test. XcodeGuide -- how to use Google Test in Xcode on Mac. Frequently-Asked Questions -- check here before asking a question on the mailing list. To contribute code to Google Test, read: DevGuide -- read this before writing your first patch. PumpManual -- how we generate some of Google Test's source files.","title":"V1 6 Documentation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/","text":"If you cannot find the answer to your question here, and you have read Primer and AdvancedGuide , send it to googletestframework@googlegroups.com . Why should I use Google Test instead of my favorite C++ testing framework? \u00b6 First, let us say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable: it doesn't require exceptions or RTTI; it works around various bugs in various compilers and environments; etc. As a result, it works on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. Google Test can generate XML test result reports that can be parsed by popular continuous build system like Hudson. Simple things are easy in Google Test, while hard things are possible: in addition to advanced features like global test environments and tests parameterized by values or types , Google Test supports various ways for the user to extend the framework -- if Google Test doesn't do something out of the box, chances are that a user can implement the feature using Google Test's public API, without changing Google Test itself. In particular, you can: expand your testing vocabulary by defining custom predicates , teach Google Test how to print your types , define your own testing macros or utilities and verify them using Google Test's Service Provider Interface , and reflect on the test cases or change the test output format by intercepting the test events . I'm getting warnings when compiling Google Test. Would you fix them? \u00b6 We strive to minimize compiler warnings Google Test generates. Before releasing a new version, we test to make sure that it doesn't generate warnings when compiled using its CMake script on Windows, Linux, and Mac OS. Unfortunately, this doesn't mean you are guaranteed to see no warnings when compiling Google Test in your environment: You may be using a different compiler as we use, or a different version of the same compiler. We cannot possibly test for all compilers. You may be compiling on a different platform as we do. Your project may be using different compiler flags as we do. It is not always possible to make Google Test warning-free for everyone. Or, it may not be desirable if the warning is rarely enabled and fixing the violations makes the code more complex. If you see warnings when compiling Google Test, we suggest that you use the -isystem flag (assuming your are using GCC) to mark Google Test headers as system headers. That'll suppress warnings from Google Test headers. Why should not test case names and test names contain underscore? \u00b6 Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that containers two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestCaseName, TestName) generates a class named TestCaseName_TestName_Test . What happens if TestCaseName or TestName contains _ ? If TestCaseName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestCaseName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestCaseName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestCaseName_Bar__Test , which is invalid. So clearly TestCaseName and TestName cannot start or end with _ (Actually, TestCaseName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestCaseName and TestName to contain _ in the middle. However, consider this: TEST(Time, Flies_Like_An_Arrow) { ... } TEST(Time_Flies, Like_An_Arrow) { ... } Now, the two TEST s will both generate the same class ( Time_Files_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestCaseName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives Google Test some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediately consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of Google Test. Therefore it's best to follow the rule. Why is it not recommended to install a pre-compiled copy of Google Test (for example, into /usr/local)? \u00b6 In the early days, we said that you could install compiled Google Test libraries on * nix systems using make install . Then every user of your machine can write tests without recompiling Google Test. This seemed like a good idea, but it has a got-cha: every user needs to compile his tests using the same compiler flags used to compile the installed Google Test libraries; otherwise he may run into undefined behaviors (i.e. the tests can behave strangely and may even crash for no obvious reasons). Why? Because C++ has this thing called the One-Definition Rule: if two C++ source files contain different definitions of the same class/function/variable, and you link them together, you violate the rule. The linker may or may not catch the error (in many cases it's not required by the C++ standard to catch the violation). If it doesn't, you get strange run-time behaviors that are unexpected and hard to debug. If you compile Google Test and your test code using different compiler flags, they may see different definitions of the same class/function/variable (e.g. due to the use of #if in Google Test). Therefore, for your sanity, we recommend to avoid installing pre-compiled Google Test libraries. Instead, each project should compile Google Test itself such that it can be sure that the same flags are used for both Google Test and the tests. How do I generate 64-bit binaries on Windows (using Visual Studio 2008)? \u00b6 (Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory. Can I use Google Test on MinGW? \u00b6 We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW. Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)? \u00b6 Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr ! NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros. Does Google Test support running tests in parallel? \u00b6 Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel. Why don't Google Test run the tests in different threads to speed things up? \u00b6 It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes. Why aren't Google Test assertions implemented using exceptions? \u00b6 Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE(...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST . Why do we use two different macros for tests with and without fixtures? \u00b6 Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public ::testing::Test {}; TEST_F(FooTest, DoesThis) { ... } or typedef ::testing::Test FooTest; TEST_F(FooTest, DoesThat) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way. Why don't we use structs as test fixtures? \u00b6 We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes. Why are death tests implemented as assertions instead of using a test runner? \u00b6 Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if (FooCondition()) { ASSERT_DEATH(Bar(), \"blah\"); } else { ASSERT_EQ(5, Bar()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount(); // Only known at run time. for (int i = 1; i <= count; i++) { ASSERT_DEATH({ double* buffer = new double[i]; ... initializes buffer ... Foo(buffer, i) }, \"blah blah\"); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically. My death test modifies some state, but the change seems lost after the death test finishes. Why? \u00b6 Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less. The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong? \u00b6 If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100; }; You also need to define it outside of the class body in foo.cc : const int Foo::kBar; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error. I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations? \u00b6 Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon. Can I derive a test fixture from another? \u00b6 Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public ::testing::Test { protected: ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected: virtual void SetUp() { BaseTest::SetUp(); // Sets up the base fixture first. ... additional set-up work ... } virtual void TearDown() { ... clean-up work for FooTest ... BaseTest::TearDown(); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5 . My compiler complains \"void value not ignored as it ought to be.\" What does this mean? \u00b6 You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions. My death test hangs (or seg-faults). How do I fix it? \u00b6 In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry! Should I use the constructor/destructor of the test fixture or the set-up/tear-down function? \u00b6 The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test, call TearDown() , and then immediately delete the test fixture object. Therefore, there is no need to write a SetUp() or TearDown() function if the constructor or destructor already does the job. You may still want to use SetUp()/TearDown() in the following cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The Google Test team is considering making the assertion macros throw on platforms where exceptions are enabled (e.g. Windows, Mac OS, and Linux client-side), which will eliminate the need for the user to propagate failures from a subroutine to its caller. Therefore, you shouldn't use Google Test assertions in a destructor if your code could run on such a platform. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() . The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it? \u00b6 If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive(int n) { return n > 0; } bool IsPositive(double x) { return x > 0; } you will get a compiler error if you write EXPECT_PRED1(IsPositive, 5); However, this will work: EXPECT_PRED1(*static_cast<bool (*)(int)>*(IsPositive), 5); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template <typename T> bool IsNegative(T x) { return x < 0; } you can use it in a predicate assertion like this: ASSERT_PRED1(IsNegative*<int>*, -5); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2(*GreaterThan<int, int>*, 5, 0); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2(*(GreaterThan<int, int>)*, 5, 0); My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why? \u00b6 Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS(); they write RUN_ALL_TESTS(); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() . My compiler complains that a constructor (or destructor) cannot return a value. What's going on? \u00b6 Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ(1, Foo()) << \"blah blah\" << foo; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it. My set-up function is not called. Why? \u00b6 C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called. How do I jump to the line of a failure in Emacs directly? \u00b6 Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure. I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious. \u00b6 You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } class BarTest : public BaseTest {}; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef BaseTest BarTest; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } The Google Test output is buried in a whole bunch of log messages. What do I do? \u00b6 The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt Why should I prefer test fixtures over global variables? \u00b6 There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common. How do I test private class members without writing FRIEND_TEST()s? \u00b6 You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... void Test1() {...} // This accesses private members of class Foo. void Test2() {...} // So does this one. }; TEST_F(FooTest, Test1) { Test1(); } TEST_F(FooTest, Test2) { Test2(); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... T1 get_private_member1(Foo* obj) { return obj->private_member1_; } }; TEST_F(FooTest, Test1) { ... get_private_member1(x) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected: // protected access for testability. int DoSomethingReturningInt(); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public: using YourClass::DoSomethingReturningInt; // changes access rights ... }; TEST_F(YourClassTest, DoSomethingTest) { TestableYourClass obj; assertEquals(expected_value, obj.DoSomethingReturningInt()); } How do I test private class static members without writing FRIEND_TEST()s? \u00b6 We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private: static bool Func(int n); }; // foo.cc bool Foo::Func(int n) { ... } // foo_test.cc EXPECT_TRUE(Foo::Func(12345)); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func(int n) { ... } } // foo_test.cc namespace internal { bool Func(int n); } EXPECT_TRUE(internal::Func(12345)); I would like to run a test several times with different parameters. Do I need to write several similar copies of it? \u00b6 No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once. How do I test a file that defines main()? \u00b6 To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort. What can the statement argument in ASSERT_DEATH() be? \u00b6 ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST(MyDeathTest, FunctionCall) { ASSERT_DEATH(Xyz(5), \"Xyz failed\"); } // Or a complex expression that references variables and functions. TEST(MyDeathTest, ComplexExpression) { const bool c = Condition(); ASSERT_DEATH((c ? Func1(0) : object2.Method(\"test\")), \"(Func1|Method) failed\"); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST(MyDeathTest, InsideLoop) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for (int i = 0; i < 5; i++) { EXPECT_DEATH_M(Foo(i), \"Foo has \\\\d+ errors\", ::testing::Message() << \"where i is \" << i); } } // A death assertion can contain a compound statement. TEST(MyDeathTest, CompoundStatement) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH({ for (int i = 0; i < 5; i++) { Bar(i); } }, \"Bar has \\\\d+ errors\");} googletest_unittest.cc contains more examples if you are interested. What syntax does the regular expression in ASSERT_DEATH use? \u00b6 On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax . I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why? \u00b6 Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .) Why does ASSERT_DEATH complain about previous threads that were already joined? \u00b6 With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this. Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH? \u00b6 Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F(FooTest, AbcDeathTest) { ... } TEST_F(FooTest, Uvw) { ... } TEST_F(BarTest, DefDeathTest) { ... } TEST_F(BarTest, Xyz) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw . But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do? \u00b6 You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public ::testing::Test { ... }; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef FooTest FooDeathTest; TEST_F(FooDeathTest, Uvw) { ... EXPECT_DEATH(...) ... } TEST_F(FooDeathTest, Xyz) { ... ASSERT_DEATH(...) ... } The compiler complains about \"no match for 'operator<<'\" when I use an assertion. What gives? \u00b6 If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space. How do I suppress the memory leak messages on Windows? \u00b6 Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines. I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help! \u00b6 You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj. I put my tests in a library and Google Test doesn't run them. What's happening? \u00b6 Have you read a warning on the Google Test Primer page? I want to use Google Test with Visual Studio but don't know where to start. \u00b6 Many people are in your position and one of the posted his solution to our mailing list. Here is his link: http://hassanjamilahmad.blogspot.com/2009/07/gtest-starters-help.html . I am seeing compile errors mentioning std::type_traits when I try to use Google Test on Solaris. \u00b6 Google Test uses parts of the standard C++ library that SunStudio does not support. Our users reported success using alternative implementations. Try running the build after runing this commad: export CC=cc CXX=CC CXXFLAGS='-library=stlport4' How can my code detect if it is running in a test? \u00b6 If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and Google Test doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is dependency injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all, there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test. Google Test defines a macro that clashes with one defined by another library. How do I deal with that? \u00b6 In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO , you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST(SomeTest, DoesThis) { ... } instead of TEST(SomeTest, DoesThis) { ... } in order to define a test. Currently, the following TEST , FAIL , SUCCEED , and the basic comparison assertion macros can have alternative names. You can see the full list of covered macros here . More information can be found in the \"Avoiding Macro Name Clashes\" section of the README file. My question is not covered in your FAQ! \u00b6 If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the revision number if you check out from SVN directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.","title":"V1 6 FAQ"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-should-i-use-google-test-instead-of-my-favorite-c-testing-framework","text":"First, let us say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable: it doesn't require exceptions or RTTI; it works around various bugs in various compilers and environments; etc. As a result, it works on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. Google Test can generate XML test result reports that can be parsed by popular continuous build system like Hudson. Simple things are easy in Google Test, while hard things are possible: in addition to advanced features like global test environments and tests parameterized by values or types , Google Test supports various ways for the user to extend the framework -- if Google Test doesn't do something out of the box, chances are that a user can implement the feature using Google Test's public API, without changing Google Test itself. In particular, you can: expand your testing vocabulary by defining custom predicates , teach Google Test how to print your types , define your own testing macros or utilities and verify them using Google Test's Service Provider Interface , and reflect on the test cases or change the test output format by intercepting the test events .","title":"Why should I use Google Test instead of my favorite C++ testing framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#im-getting-warnings-when-compiling-google-test-would-you-fix-them","text":"We strive to minimize compiler warnings Google Test generates. Before releasing a new version, we test to make sure that it doesn't generate warnings when compiled using its CMake script on Windows, Linux, and Mac OS. Unfortunately, this doesn't mean you are guaranteed to see no warnings when compiling Google Test in your environment: You may be using a different compiler as we use, or a different version of the same compiler. We cannot possibly test for all compilers. You may be compiling on a different platform as we do. Your project may be using different compiler flags as we do. It is not always possible to make Google Test warning-free for everyone. Or, it may not be desirable if the warning is rarely enabled and fixing the violations makes the code more complex. If you see warnings when compiling Google Test, we suggest that you use the -isystem flag (assuming your are using GCC) to mark Google Test headers as system headers. That'll suppress warnings from Google Test headers.","title":"I'm getting warnings when compiling Google Test.  Would you fix them?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-should-not-test-case-names-and-test-names-contain-underscore","text":"Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that containers two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestCaseName, TestName) generates a class named TestCaseName_TestName_Test . What happens if TestCaseName or TestName contains _ ? If TestCaseName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestCaseName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestCaseName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestCaseName_Bar__Test , which is invalid. So clearly TestCaseName and TestName cannot start or end with _ (Actually, TestCaseName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestCaseName and TestName to contain _ in the middle. However, consider this: TEST(Time, Flies_Like_An_Arrow) { ... } TEST(Time_Flies, Like_An_Arrow) { ... } Now, the two TEST s will both generate the same class ( Time_Files_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestCaseName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives Google Test some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediately consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of Google Test. Therefore it's best to follow the rule.","title":"Why should not test case names and test names contain underscore?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-is-it-not-recommended-to-install-a-pre-compiled-copy-of-google-test-for-example-into-usrlocal","text":"In the early days, we said that you could install compiled Google Test libraries on * nix systems using make install . Then every user of your machine can write tests without recompiling Google Test. This seemed like a good idea, but it has a got-cha: every user needs to compile his tests using the same compiler flags used to compile the installed Google Test libraries; otherwise he may run into undefined behaviors (i.e. the tests can behave strangely and may even crash for no obvious reasons). Why? Because C++ has this thing called the One-Definition Rule: if two C++ source files contain different definitions of the same class/function/variable, and you link them together, you violate the rule. The linker may or may not catch the error (in many cases it's not required by the C++ standard to catch the violation). If it doesn't, you get strange run-time behaviors that are unexpected and hard to debug. If you compile Google Test and your test code using different compiler flags, they may see different definitions of the same class/function/variable (e.g. due to the use of #if in Google Test). Therefore, for your sanity, we recommend to avoid installing pre-compiled Google Test libraries. Instead, each project should compile Google Test itself such that it can be sure that the same flags are used for both Google Test and the tests.","title":"Why is it not recommended to install a pre-compiled copy of Google Test (for example, into /usr/local)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-do-i-generate-64-bit-binaries-on-windows-using-visual-studio-2008","text":"(Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory.","title":"How do I generate 64-bit binaries on Windows (using Visual Studio 2008)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#can-i-use-google-test-on-mingw","text":"We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW.","title":"Can I use Google Test on MinGW?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-does-google-test-support-expect_eqnull-ptr-and-assert_eqnull-ptr-but-not-expect_nenull-ptr-and-assert_nenull-ptr","text":"Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr ! NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros.","title":"Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#does-google-test-support-running-tests-in-parallel","text":"Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel.","title":"Does Google Test support running tests in parallel?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-dont-google-test-run-the-tests-in-different-threads-to-speed-things-up","text":"It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes.","title":"Why don't Google Test run the tests in different threads to speed things up?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-arent-google-test-assertions-implemented-using-exceptions","text":"Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE(...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST .","title":"Why aren't Google Test assertions implemented using exceptions?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-do-we-use-two-different-macros-for-tests-with-and-without-fixtures","text":"Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public ::testing::Test {}; TEST_F(FooTest, DoesThis) { ... } or typedef ::testing::Test FooTest; TEST_F(FooTest, DoesThat) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way.","title":"Why do we use two different macros for tests with and without fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-dont-we-use-structs-as-test-fixtures","text":"We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes.","title":"Why don't we use structs as test fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-are-death-tests-implemented-as-assertions-instead-of-using-a-test-runner","text":"Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if (FooCondition()) { ASSERT_DEATH(Bar(), \"blah\"); } else { ASSERT_EQ(5, Bar()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount(); // Only known at run time. for (int i = 1; i <= count; i++) { ASSERT_DEATH({ double* buffer = new double[i]; ... initializes buffer ... Foo(buffer, i) }, \"blah blah\"); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically.","title":"Why are death tests implemented as assertions instead of using a test runner?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-death-test-modifies-some-state-but-the-change-seems-lost-after-the-death-test-finishes-why","text":"Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less.","title":"My death test modifies some state, but the change seems lost after the death test finishes. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#the-compiler-complains-about-undefined-references-to-some-static-const-member-variables-but-i-did-define-them-in-the-class-body-whats-wrong","text":"If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100; }; You also need to define it outside of the class body in foo.cc : const int Foo::kBar; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error.","title":"The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-have-an-interface-that-has-several-implementations-can-i-write-a-set-of-tests-once-and-repeat-them-over-all-the-implementations","text":"Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon.","title":"I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#can-i-derive-a-test-fixture-from-another","text":"Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public ::testing::Test { protected: ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected: virtual void SetUp() { BaseTest::SetUp(); // Sets up the base fixture first. ... additional set-up work ... } virtual void TearDown() { ... clean-up work for FooTest ... BaseTest::TearDown(); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5 .","title":"Can I derive a test fixture from another?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-compiler-complains-void-value-not-ignored-as-it-ought-to-be-what-does-this-mean","text":"You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions.","title":"My compiler complains \"void value not ignored as it ought to be.\" What does this mean?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-death-test-hangs-or-seg-faults-how-do-i-fix-it","text":"In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry!","title":"My death test hangs (or seg-faults). How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#should-i-use-the-constructordestructor-of-the-test-fixture-or-the-set-uptear-down-function","text":"The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test, call TearDown() , and then immediately delete the test fixture object. Therefore, there is no need to write a SetUp() or TearDown() function if the constructor or destructor already does the job. You may still want to use SetUp()/TearDown() in the following cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The Google Test team is considering making the assertion macros throw on platforms where exceptions are enabled (e.g. Windows, Mac OS, and Linux client-side), which will eliminate the need for the user to propagate failures from a subroutine to its caller. Therefore, you shouldn't use Google Test assertions in a destructor if your code could run on such a platform. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() .","title":"Should I use the constructor/destructor of the test fixture or the set-up/tear-down function?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#the-compiler-complains-no-matching-function-to-call-when-i-use-assert_predn-how-do-i-fix-it","text":"If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive(int n) { return n > 0; } bool IsPositive(double x) { return x > 0; } you will get a compiler error if you write EXPECT_PRED1(IsPositive, 5); However, this will work: EXPECT_PRED1(*static_cast<bool (*)(int)>*(IsPositive), 5); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template <typename T> bool IsNegative(T x) { return x < 0; } you can use it in a predicate assertion like this: ASSERT_PRED1(IsNegative*<int>*, -5); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2(*GreaterThan<int, int>*, 5, 0); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2(*(GreaterThan<int, int>)*, 5, 0);","title":"The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-compiler-complains-about-ignoring-return-value-when-i-call-run_all_tests-why","text":"Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS(); they write RUN_ALL_TESTS(); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() .","title":"My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-compiler-complains-that-a-constructor-or-destructor-cannot-return-a-value-whats-going-on","text":"Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ(1, Foo()) << \"blah blah\" << foo; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it.","title":"My compiler complains that a constructor (or destructor) cannot return a value. What's going on?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-set-up-function-is-not-called-why","text":"C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called.","title":"My set-up function is not called. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-do-i-jump-to-the-line-of-a-failure-in-emacs-directly","text":"Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure.","title":"How do I jump to the line of a failure in Emacs directly?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-have-several-test-cases-which-share-the-same-test-fixture-logic-do-i-have-to-define-a-new-test-fixture-class-for-each-of-them-this-seems-pretty-tedious","text":"You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } class BarTest : public BaseTest {}; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef BaseTest BarTest; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... }","title":"I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#the-google-test-output-is-buried-in-a-whole-bunch-of-log-messages-what-do-i-do","text":"The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt","title":"The Google Test output is buried in a whole bunch of log messages. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-should-i-prefer-test-fixtures-over-global-variables","text":"There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common.","title":"Why should I prefer test fixtures over global variables?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-do-i-test-private-class-members-without-writing-friend_tests","text":"You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... void Test1() {...} // This accesses private members of class Foo. void Test2() {...} // So does this one. }; TEST_F(FooTest, Test1) { Test1(); } TEST_F(FooTest, Test2) { Test2(); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... T1 get_private_member1(Foo* obj) { return obj->private_member1_; } }; TEST_F(FooTest, Test1) { ... get_private_member1(x) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected: // protected access for testability. int DoSomethingReturningInt(); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public: using YourClass::DoSomethingReturningInt; // changes access rights ... }; TEST_F(YourClassTest, DoSomethingTest) { TestableYourClass obj; assertEquals(expected_value, obj.DoSomethingReturningInt()); }","title":"How do I test private class members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-do-i-test-private-class-static-members-without-writing-friend_tests","text":"We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private: static bool Func(int n); }; // foo.cc bool Foo::Func(int n) { ... } // foo_test.cc EXPECT_TRUE(Foo::Func(12345)); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func(int n) { ... } } // foo_test.cc namespace internal { bool Func(int n); } EXPECT_TRUE(internal::Func(12345));","title":"How do I test private class static members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-would-like-to-run-a-test-several-times-with-different-parameters-do-i-need-to-write-several-similar-copies-of-it","text":"No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once.","title":"I would like to run a test several times with different parameters. Do I need to write several similar copies of it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-do-i-test-a-file-that-defines-main","text":"To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort.","title":"How do I test a file that defines main()?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#what-can-the-statement-argument-in-assert_death-be","text":"ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST(MyDeathTest, FunctionCall) { ASSERT_DEATH(Xyz(5), \"Xyz failed\"); } // Or a complex expression that references variables and functions. TEST(MyDeathTest, ComplexExpression) { const bool c = Condition(); ASSERT_DEATH((c ? Func1(0) : object2.Method(\"test\")), \"(Func1|Method) failed\"); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST(MyDeathTest, InsideLoop) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for (int i = 0; i < 5; i++) { EXPECT_DEATH_M(Foo(i), \"Foo has \\\\d+ errors\", ::testing::Message() << \"where i is \" << i); } } // A death assertion can contain a compound statement. TEST(MyDeathTest, CompoundStatement) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH({ for (int i = 0; i < 5; i++) { Bar(i); } }, \"Bar has \\\\d+ errors\");} googletest_unittest.cc contains more examples if you are interested.","title":"What can the statement argument in ASSERT_DEATH() be?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#what-syntax-does-the-regular-expression-in-assert_death-use","text":"On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax .","title":"What syntax does the regular expression in ASSERT_DEATH use?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-have-a-fixture-class-foo-but-test_ffoo-bar-gives-me-error-no-matching-function-for-call-to-foofoo-why","text":"Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .)","title":"I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-does-assert_death-complain-about-previous-threads-that-were-already-joined","text":"With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this.","title":"Why does ASSERT_DEATH complain about previous threads that were already joined?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#why-does-google-test-require-the-entire-test-case-instead-of-individual-tests-to-be-named-foodeathtest-when-it-uses-assert_death","text":"Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F(FooTest, AbcDeathTest) { ... } TEST_F(FooTest, Uvw) { ... } TEST_F(BarTest, DefDeathTest) { ... } TEST_F(BarTest, Xyz) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw .","title":"Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#but-i-dont-like-calling-my-entire-test-case-foodeathtest-when-it-contains-both-death-tests-and-non-death-tests-what-do-i-do","text":"You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public ::testing::Test { ... }; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef FooTest FooDeathTest; TEST_F(FooDeathTest, Uvw) { ... EXPECT_DEATH(...) ... } TEST_F(FooDeathTest, Xyz) { ... ASSERT_DEATH(...) ... }","title":"But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#the-compiler-complains-about-no-match-for-operator-when-i-use-an-assertion-what-gives","text":"If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space.","title":"The compiler complains about \"no match for 'operator&lt;&lt;'\" when I use an assertion. What gives?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-do-i-suppress-the-memory-leak-messages-on-windows","text":"Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines.","title":"How do I suppress the memory leak messages on Windows?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-am-building-my-project-with-google-test-in-visual-studio-and-all-im-getting-is-a-bunch-of-linker-errors-or-warnings-help","text":"You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj.","title":"I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-put-my-tests-in-a-library-and-google-test-doesnt-run-them-whats-happening","text":"Have you read a warning on the Google Test Primer page?","title":"I put my tests in a library and Google Test doesn't run them. What's happening?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-want-to-use-google-test-with-visual-studio-but-dont-know-where-to-start","text":"Many people are in your position and one of the posted his solution to our mailing list. Here is his link: http://hassanjamilahmad.blogspot.com/2009/07/gtest-starters-help.html .","title":"I want to use Google Test with Visual Studio but don't know where to start."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#i-am-seeing-compile-errors-mentioning-stdtype_traits-when-i-try-to-use-google-test-on-solaris","text":"Google Test uses parts of the standard C++ library that SunStudio does not support. Our users reported success using alternative implementations. Try running the build after runing this commad: export CC=cc CXX=CC CXXFLAGS='-library=stlport4'","title":"I am seeing compile errors mentioning std::type_traits when I try to use Google Test on Solaris."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#how-can-my-code-detect-if-it-is-running-in-a-test","text":"If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and Google Test doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is dependency injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all, there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test.","title":"How can my code detect if it is running in a test?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#google-test-defines-a-macro-that-clashes-with-one-defined-by-another-library-how-do-i-deal-with-that","text":"In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO , you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST(SomeTest, DoesThis) { ... } instead of TEST(SomeTest, DoesThis) { ... } in order to define a test. Currently, the following TEST , FAIL , SUCCEED , and the basic comparison assertion macros can have alternative names. You can see the full list of covered macros here . More information can be found in the \"Avoiding Macro Name Clashes\" section of the README file.","title":"Google Test defines a macro that clashes with one defined by another library. How do I deal with that?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_FAQ/#my-question-is-not-covered-in-your-faq","text":"If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the revision number if you check out from SVN directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.","title":"My question is not covered in your FAQ!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/","text":"Introduction: Why Google C++ Testing Framework? \u00b6 Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test . Setting up a New Test Project \u00b6 To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems: msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script (deprecated) and CMakeLists.txt for CMake (recommended) in the Google Test root directory. If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find \"gtest/gtest.h\" when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples. Basic Concepts \u00b6 When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases. Assertions \u00b6 Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed. Basic Assertions \u00b6 These assertions do basic true/false condition testing. | Fatal assertion | Nonfatal assertion | Verifies | |:--------------------|:-----------------------|:-------------| | ASSERT_TRUE( condition ) ; | EXPECT_TRUE( condition ) ; | condition is true | | ASSERT_FALSE( condition ) ; | EXPECT_FALSE( condition ) ; | condition is false | Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac. Binary Comparison \u00b6 This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( expected , actual ); EXPECT_EQ( expected , actual ); expected == actual ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll introduce later), you should put the expression you want to test in the position of actual , and put its expected value in expected , as Google Test's failure messages are optimized for this convention. Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but it's no longer necessary since v1.6.0 (if << is supported, it will be called to print the arguments when the assertion fails; otherwise Google Test will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see this Google Mock recipe .). These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. String Comparison \u00b6 The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( expected_str , actual_str ); EXPECT_STREQ( expected_str , actual_str ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( expected_str , actual_str ); EXPECT_STRCASEEQ( expected_str , actual_str ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the Advanced Google Test Guide . Simple Tests \u00b6 To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain underscore ( _ ). A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac. Test Fixtures: Using the Same Data Configuration for Multiple Tests \u00b6 If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed. Invoking the Tests \u00b6 TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac. Writing the main() Function \u00b6 You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go. Important note for Visual C++ users \u00b6 If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries! Where to Go from Here \u00b6 Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features. Known Limitations \u00b6 Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"V1 6 Primer"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#introduction-why-google-c-testing-framework","text":"Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test .","title":"Introduction: Why Google C++ Testing Framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#setting-up-a-new-test-project","text":"To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems: msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script (deprecated) and CMakeLists.txt for CMake (recommended) in the Google Test root directory. If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find \"gtest/gtest.h\" when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples.","title":"Setting up a New Test Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#basic-concepts","text":"When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases.","title":"Basic Concepts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#assertions","text":"Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed.","title":"Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#basic-assertions","text":"These assertions do basic true/false condition testing. | Fatal assertion | Nonfatal assertion | Verifies | |:--------------------|:-----------------------|:-------------| | ASSERT_TRUE( condition ) ; | EXPECT_TRUE( condition ) ; | condition is true | | ASSERT_FALSE( condition ) ; | EXPECT_FALSE( condition ) ; | condition is false | Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac.","title":"Basic Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#binary-comparison","text":"This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( expected , actual ); EXPECT_EQ( expected , actual ); expected == actual ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll introduce later), you should put the expression you want to test in the position of actual , and put its expected value in expected , as Google Test's failure messages are optimized for this convention. Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but it's no longer necessary since v1.6.0 (if << is supported, it will be called to print the arguments when the assertion fails; otherwise Google Test will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see this Google Mock recipe .). These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac.","title":"Binary Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#string-comparison","text":"The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( expected_str , actual_str ); EXPECT_STREQ( expected_str , actual_str ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( expected_str , actual_str ); EXPECT_STRCASEEQ( expected_str , actual_str ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the Advanced Google Test Guide .","title":"String Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#simple-tests","text":"To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain underscore ( _ ). A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac.","title":"Simple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#test-fixtures-using-the-same-data-configuration-for-multiple-tests","text":"If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed.","title":"Test Fixtures: Using the Same Data Configuration for Multiple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#invoking-the-tests","text":"TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac.","title":"Invoking the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#writing-the-main-function","text":"You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go.","title":"Writing the main() Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#important-note-for-visual-c-users","text":"If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries!","title":"Important note for Visual C++ users"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#where-to-go-from-here","text":"Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features.","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Primer/#known-limitations","text":"Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Known Limitations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/","text":"P ump is U seful for M eta P rogramming. The Problem \u00b6 Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code. Our Solution \u00b6 Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain. Highlights \u00b6 The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode. Examples \u00b6 The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on... Constructs \u00b6 We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [[ code ]] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output. Grammar \u00b6 code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax Code \u00b6 You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump. Real Examples \u00b6 You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h . Tips \u00b6 If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"V1 6 Pump Manual"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#the-problem","text":"Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code.","title":"The Problem"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#our-solution","text":"Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain.","title":"Our Solution"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#highlights","text":"The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode.","title":"Highlights"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#examples","text":"The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on...","title":"Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#constructs","text":"We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [[ code ]] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output.","title":"Constructs"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#grammar","text":"code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax","title":"Grammar"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#code","text":"You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump.","title":"Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#real-examples","text":"You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h .","title":"Real Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_PumpManual/#tips","text":"If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Tips"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_Samples/","text":"If you're like us, you'd like to look at some Google Test sample code. The samples folder has a number of well-commented samples showing how to use a variety of Google Test features. Sample #1 shows the basic steps of using Google Test to test C++ functions. Sample #2 shows a more complex unit test for a class with multiple member functions. Sample #3 uses a test fixture. Sample #4 is another basic example of using Google Test. Sample #5 teaches how to reuse a test fixture in multiple test cases by deriving sub-fixtures from it. Sample #6 demonstrates type-parameterized tests. Sample #7 teaches the basics of value-parameterized tests. Sample #8 shows using Combine() in value-parameterized tests. Sample #9 shows use of the listener API to modify Google Test's console output and the use of its reflection API to inspect test results. Sample #10 shows use of the listener API to implement a primitive memory leak checker.","title":"V1 6 Samples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/","text":"This guide will explain how to use the Google Testing Framework in your Xcode projects on Mac OS X. This tutorial begins by quickly explaining what to do for experienced users. After the quick start, the guide goes provides additional explanation about each step. Quick Start \u00b6 Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations. Get the Source \u00b6 Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk Add the Framework to Your Project \u00b6 The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below). Make a Test Target \u00b6 To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase. Set Up the Executable Run Environment \u00b6 Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH. Build and Go \u00b6 Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0. Summary \u00b6 Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"V1 6 Xcode Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#quick-start","text":"Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations.","title":"Quick Start"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#get-the-source","text":"Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk","title":"Get the Source"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#add-the-framework-to-your-project","text":"The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below).","title":"Add the Framework to Your Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#make-a-test-target","text":"To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase.","title":"Make a Test Target"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#set-up-the-executable-run-environment","text":"Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH.","title":"Set Up the Executable Run Environment"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#build-and-go","text":"Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0.","title":"Build and Go"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_6_XcodeGuide/#summary","text":"Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"Summary"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/","text":"Now that you have read Primer and learned how to write tests using Google Test, it's time to learn some new tricks. This document will show you more assertions as well as how to construct complex failure messages, propagate fatal failures, reuse and speed up your test fixtures, and use various flags with your tests. More Assertions \u00b6 This section covers some less frequently used, but still significant, assertions. Explicit Success and Failure \u00b6 These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); ADD_FAILURE_AT(\" file_path \", line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Availability : Linux, Windows, Mac. Exception Assertions \u00b6 These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0. Predicate Assertions for Better Error Messages \u00b6 Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem: Using an Existing Boolean Function \u00b6 If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac Using a Function That Returns an AssertionResult \u00b6 While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1. Using a Predicate-Formatter \u00b6 If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 `); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac. Floating-Point Comparison \u00b6 Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison . Floating-Point Macros \u00b6 Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( expected, actual ); EXPECT_FLOAT_EQ( expected, actual ); the two float values are almost equal ASSERT_DOUBLE_EQ( expected, actual ); EXPECT_DOUBLE_EQ( expected, actual ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac. Floating-Point Predicate-Format Functions \u00b6 Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac. Windows HRESULT assertions \u00b6 These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows. Type Assertions \u00b6 You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0. Assertion Placement \u00b6 You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations! Teaching Google Test How to Print Your Values \u00b6 When a test assertion such as EXPECT_EQ fails, Google Test prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <iostream> namespace foo { class Bar { ... }; // We want Google Test to be able to print instances of this. // It's important that the << operator is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. ::std::ostream& operator<<(::std::ostream& os, const Bar& bar) { return os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <iostream> namespace foo { class Bar { ... }; // It's important that PrintTo() is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. void PrintTo(const Bar& bar, ::std::ostream* os) { *os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when Google Test is concerned. This allows you to customize how the value appears in Google Test's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using Google Test's value printer yourself, just call ::testing::PrintToString( x ) , which returns an std::string : vector<pair<Bar, int> > bar_ints = GetBarIntVector(); EXPECT_TRUE(IsCorrectBarIntVector(bar_ints)) << \"bar_ints = \" << ::testing::PrintToString(bar_ints); Death Tests \u00b6 In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures . How to Write a Death Test \u00b6 Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); | EXPECT_DEATH( _statement, regex_ ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); | EXPECT_DEATH_IF_SUPPORTED( _statement, regex_ ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); | EXPECT_EXIT( _statement, predicate, regex_ ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(MyDeathTest, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0. Regular Expression Syntax \u00b6 On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support (Letter A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation \\\\. matches the . character . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only. How It Works \u00b6 Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails. Death Tests And Threads \u00b6 The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent. Death Test Styles \u00b6 The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); } Caveats \u00b6 The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some Google Test macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) . Using Assertions in Sub-routines \u00b6 Adding Traces to Assertions \u00b6 If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac. Propagating Fatal Failures \u00b6 A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections. Asserting on Subroutines \u00b6 As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported. Checking for Failures in the Current Test \u00b6 HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0. Logging Additional Information \u00b6 In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a string or an int . The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , classname , type_param , and value_param ). * Calling RecordProperty() outside of the lifespan of a test is allowed. If it's called outside of a test but between a test case's SetUpTestCase() and TearDownTestCase() methods, it will be attributed to the XML element for the test case. If it's called outside of all test cases (e.g. in a test environment), it will be attributed to the top-level XML element. Availability : Linux, Windows, Mac. Sharing Resources Between Tests in the Same Test Case \u00b6 Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac. Global Set-Up and Tear-Down \u00b6 Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac. Value Parameterized Tests \u00b6 Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFoo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it! How to Write Value-Parameterized Tests \u00b6 To write value-parameterized tests, first you should define a fixture class. It must be derived from both ::testing::Test and ::testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from ::testing::TestWithParam<T> , which itself is derived from both ::testing::Test and ::testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public ::testing::Test { ... }; class BarTest : public BaseTest, public ::testing::WithParamInterface<const char*> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . container , begin , and end can be expressions whose values are determined at run time. Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest_filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0. Creating Value-Parameterized Abstract Tests \u00b6 In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files. Typed Tests \u00b6 Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Type-Parameterized Tests \u00b6 Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0. Testing Private Code \u00b6 If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members Static Functions \u00b6 Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients. Private Class Members \u00b6 Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include \"gtest/gtest_prod.h\" // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace Catching Failures \u00b6 If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? \"gtest/gtest-spi.h\" contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in \"gtest/internal/gtest-port.h\" have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring ); Getting the Current Test's Name \u00b6 Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac. Extending Google Test by Handling Test Events \u00b6 Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0. Defining Event Listeners \u00b6 To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCEED() invocation. virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } }; Using Event Listeners \u00b6 To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier. Generating Failures in Listeners \u00b6 You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here . Running Test Programs: Advanced Options \u00b6 Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } Selecting Tests \u00b6 This section shows various options for choosing which tests to run. Listing Test Names \u00b6 Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac. Running a Subset of the Tests \u00b6 By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac. Temporarily Disabling Tests \u00b6 If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac. Temporarily Enabling Disabled Tests \u00b6 To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0. Repeating the Tests \u00b6 Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac. Shuffling the Tests \u00b6 You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0. Controlling Test Output \u00b6 This section teaches how to tweak the way test results are reported. Colored Terminal Output \u00b6 Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac. Suppressing the Elapsed Time \u00b6 By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.) Generating an XML Report \u00b6 Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Jenkins . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac. Controlling How Failures Are Reported \u00b6 Turning Assertion Failures into Break-Points \u00b6 When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac. Disabling Catching Test-Thrown Exceptions \u00b6 Google Test can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default Google Test catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests. Availability : Linux, Windows, Mac. Letting Another Testing Framework Drive \u00b6 If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include \"gtest/gtest.h\" int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Death tests are not supported when other test framework is used to organize tests. Availability: Linux, Windows, Mac; since v1.3.0. Distributing Test Functions to Multiple Machines \u00b6 If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0. Fusing Google Test Source Files \u00b6 Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scripts/test/Makefile file as an example on how to compile your tests against them. Where to Go from Here \u00b6 Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the Frequently-Asked Questions .","title":"V1 7 Advanced Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#more-assertions","text":"This section covers some less frequently used, but still significant, assertions.","title":"More Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#explicit-success-and-failure","text":"These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into the them. SUCCEED(); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. Note: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to Google Test's output in the future. FAIL(); ADD_FAILURE(); ADD_FAILURE_AT(\" file_path \", line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, deteremines the test's success or failure. For example, you might want to write something like: switch(expression) { case 1: ... some checks ... case 2: ... some other checks ... default: FAIL() << \"We shouldn't get here.\"; } Availability : Linux, Windows, Mac.","title":"Explicit Success and Failure"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#exception-assertions","text":"These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW( statement , exception_type ); EXPECT_THROW( statement , exception_type ); statement throws an exception of the given type ASSERT_ANY_THROW( statement ); EXPECT_ANY_THROW( statement ); statement throws an exception of any type ASSERT_NO_THROW( statement ); EXPECT_NO_THROW( statement ); statement doesn't throw any exception Examples: ASSERT_THROW(Foo(5), bar_exception); EXPECT_NO_THROW({ int n = 5; Bar(&n); }); Availability : Linux, Windows, Mac; since version 1.1.0.","title":"Exception Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#predicate-assertions-for-better-error-messages","text":"Even though Google Test has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all the scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. Google Test gives you three different options to solve this problem:","title":"Predicate Assertions for Better Error Messages"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#using-an-existing-boolean-function","text":"If you already have a function or a functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1( pred1, val1 ); EXPECT_PRED1( pred1, val1 ); pred1(val1) returns true ASSERT_PRED2( pred2, val1, val2 ); EXPECT_PRED2( pred2, val1, val2 ); pred2(val1, val2) returns true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true iff m and n have no common divisors except 1. bool MutuallyPrime(int m, int n) { ... } const int a = 3; const int b = 4; const int c = 10; the assertion EXPECT_PRED2(MutuallyPrime, a, b); will succeed, while the assertion EXPECT_PRED2(MutuallyPrime, b, c); will fail with the message !MutuallyPrime(b, c) is false, where b is 4 c is 10 Notes: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Currently we only provide predicate assertions of arity <= 5. If you need a higher-arity assertion, let us know. Availability : Linux, Windows, Mac","title":"Using an Existing Boolean Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#using-a-function-that-returns-an-assertionresult","text":"While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess(); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure(); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess(); else return ::testing::AssertionFailure() << n << \" is odd\"; } instead of: bool IsEven(int n) { return (n % 2) == 0; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: IsEven(Fib(4)) Actual: false (*3 is odd*) Expected: true instead of a more opaque Value of: IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well, and are fine with making the predicate slower in the success case, you can supply a success message: ::testing::AssertionResult IsEven(int n) { if ((n % 2) == 0) return ::testing::AssertionSuccess() << n << \" is even\"; else return ::testing::AssertionFailure() << n << \" is odd\"; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: IsEven(Fib(6)) Actual: true (8 is even) Expected: false Availability : Linux, Windows, Mac; since version 1.4.1.","title":"Using a Function That Returns an AssertionResult"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#using-a-predicate-formatter","text":"If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1( pred_format1, val1 ); EXPECT_PRED_FORMAT1( pred_format1, val1 `); pred_format1(val1) is successful ASSERT_PRED_FORMAT2( pred_format2, val1, val2 ); EXPECT_PRED_FORMAT2( pred_format2, val1, val2 ); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous two groups of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: ::testing::AssertionResult PredicateFormattern(const char* expr1 , const char* expr2 , ... const char* exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. A predicate-formatter returns a ::testing::AssertionResult object to indicate whether the assertion has succeeded or not. The only way to create such an object is to call one of these factory functions: As an example, let's improve the failure message in the previous example, which uses EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor(int m, int n) { ... } // A predicate-formatter for asserting that two integers are mutually prime. ::testing::AssertionResult AssertMutuallyPrime(const char* m_expr, const char* n_expr, int m, int n) { if (MutuallyPrime(m, n)) return ::testing::AssertionSuccess(); return ::testing::AssertionFailure() << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor(m, n); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2(AssertMutuallyPrime, b, c); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Availability : Linux, Windows, Mac.","title":"Using a Predicate-Formatter"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#floating-point-comparison","text":"Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and Google Test provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see this article on float comparison .","title":"Floating-Point Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#floating-point-macros","text":"Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ( expected, actual ); EXPECT_FLOAT_EQ( expected, actual ); the two float values are almost equal ASSERT_DOUBLE_EQ( expected, actual ); EXPECT_DOUBLE_EQ( expected, actual ); the two double values are almost equal By \"almost equal\", we mean the two values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR( val1, val2, abs_error ); EXPECT_NEAR (val1, val2, abs_error ); the difference between val1 and val2 doesn't exceed the given absolute error Availability : Linux, Windows, Mac.","title":"Floating-Point Macros"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#floating-point-predicate-format-functions","text":"Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2(::testing::FloatLE, val1, val2); EXPECT_PRED_FORMAT2(::testing::DoubleLE, val1, val2); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Availability : Linux, Windows, Mac.","title":"Floating-Point Predicate-Format Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#windows-hresult-assertions","text":"These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED( expression ); EXPECT_HRESULT_SUCCEEDED( expression ); expression is a success HRESULT ASSERT_HRESULT_FAILED( expression ); EXPECT_HRESULT_FAILED( expression ); expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr shell; ASSERT_HRESULT_SUCCEEDED(shell.CoCreateInstance(L\"Shell.Application\")); CComVariant empty; ASSERT_HRESULT_SUCCEEDED(shell->ShellExecute(CComBSTR(url), empty, empty, empty, empty)); Availability : Windows.","title":"Windows HRESULT assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#type-assertions","text":"You can call the function ::testing::StaticAssertTypeEq<T1, T2>(); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, and the compiler error message will likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat: When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template <typename T> class Foo { public: void Bar() { ::testing::StaticAssertTypeEq<int, T>(); } }; the code: void Test1() { Foo<bool> foo; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2() { Foo<bool> foo; foo.Bar(); } to cause a compiler error. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Type Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#assertion-placement","text":"You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google Test not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" . If you need to use assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . Note : Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them. You'll get a compilation error if you try. A simple workaround is to transfer the entire body of the constructor or destructor to a private void-returning method. However, you should be aware that a fatal assertion failure in a constructor does not terminate the current test, as your intuition might suggest; it merely returns from the constructor early, possibly leaving your object in a partially-constructed state. Likewise, a fatal assertion failure in a destructor may leave your object in a partially-destructed state. Use assertions carefully in these situations!","title":"Assertion Placement"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#teaching-google-test-how-to-print-your-values","text":"When a test assertion such as EXPECT_EQ fails, Google Test prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <iostream> namespace foo { class Bar { ... }; // We want Google Test to be able to print instances of this. // It's important that the << operator is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. ::std::ostream& operator<<(::std::ostream& os, const Bar& bar) { return os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <iostream> namespace foo { class Bar { ... }; // It's important that PrintTo() is defined in the SAME // namespace that defines Bar. C++'s look-up rules rely on that. void PrintTo(const Bar& bar, ::std::ostream* os) { *os << bar.DebugString(); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when Google Test is concerned. This allows you to customize how the value appears in Google Test's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using Google Test's value printer yourself, just call ::testing::PrintToString( x ) , which returns an std::string : vector<pair<Bar, int> > bar_ints = GetBarIntVector(); EXPECT_TRUE(IsCorrectBarIntVector(bar_ints)) << \"bar_ints = \" << ::testing::PrintToString(bar_ints);","title":"Teaching Google Test How to Print Your Values"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#death-tests","text":"In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures .","title":"Death Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#how-to-write-a-death-test","text":"Google Test has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH( statement, regex ); | EXPECT_DEATH( _statement, regex_ ); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED( statement, regex ); | EXPECT_DEATH_IF_SUPPORTED( _statement, regex_ ); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT( statement, predicate, regex ); | EXPECT_EXIT( _statement, predicate, regex_ ); statement exits with the given error and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and regex is a regular expression that the stderr output of statement is expected to match. Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. Note: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if statement terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . Google Test defines a few predicates that handle the most common cases: ::testing::ExitedWithCode(exit_code) This expression is true if the program exited normally with the given exit code. ::testing::KilledBySignal(signal_number) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as Google Test assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST(MyDeathTest, Foo) { // This death test uses a compound statement. ASSERT_DEATH({ int n = 5; Foo(&n); }, \"Error on line .* of Foo()\"); } TEST(MyDeathTest, NormalExit) { EXPECT_EXIT(NormalExit(), ::testing::ExitedWithCode(0), \"Success\"); } TEST(MyDeathTest, KillMyself) { EXPECT_EXIT(KillMyself(), ::testing::KilledBySignal(SIGKILL), \"Sending myself unblockable signal\"); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Important: We strongly recommend you to follow the convention of naming your test case (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public ::testing::Test { ... }; typedef FooTest FooDeathTest; TEST_F(FooTest, DoesThis) { // normal test } TEST_F(FooDeathTest, DoesThat) { // death test } Availability: Linux, Windows (requires MSVC 8.0 or above), Cygwin, and Mac (the latter three are supported since v1.3.0). (ASSERT|EXPECT)_DEATH_IF_SUPPORTED are new in v1.4.0.","title":"How to Write a Death Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#regular-expression-syntax","text":"On POSIX systems (e.g. Linux, Cygwin, and Mac), Google Test uses the POSIX extended regular expression syntax in death tests. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, Google Test uses its own simple regular expression implementation. It lacks many features you can find in POSIX extended regular expressions. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support (Letter A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation \\\\. matches the . character . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, Google Test defines macro GTEST_USES_POSIX_RE=1 when it uses POSIX extended regular expressions, or GTEST_USES_SIMPLE_RE=1 when it uses the simple version. If you want your death tests to work in both cases, you can either #if on these macros or use the more limited syntax only.","title":"Regular Expression Syntax"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#how-it-works","text":"Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" . However, we reserve the right to change it in the future. Therefore, your tests should not depend on this. In either case, the parent process waits for the child process to complete, and checks that the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails.","title":"How It Works"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#death-tests-and-threads","text":"The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. Google Test has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test cases with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent.","title":"Death Tests And Threads"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#death-test-styles","text":"The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. We suggest using the faster, default \"fast\" style unless your test has specific problems with it. You can choose a particular style of death tests by setting the flag programmatically: ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: TEST(MyDeathTest, TestOne) { ::testing::FLAGS_gtest_death_test_style = \"threadsafe\"; // This test is run in the \"threadsafe\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } TEST(MyDeathTest, TestTwo) { // This test is run in the \"fast\" style: ASSERT_DEATH(ThisShouldDie(), \"\"); } int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); ::testing::FLAGS_gtest_death_test_style = \"fast\"; return RUN_ALL_TESTS(); }","title":"Death Test Styles"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#caveats","text":"The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some Google Test macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) .","title":"Caveats"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#using-assertions-in-sub-routines","text":"","title":"Using Assertions in Sub-routines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#adding-traces-to-assertions","text":"If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro: SCOPED_TRACE( message ); where message can be anything streamable to std::ostream . This macro will cause the current file name, line number, and the given message to be added in every failure message. The effect will be undone when the control leaves the current lexical scope. For example, 10: void Sub1(int n) { 11: EXPECT_EQ(1, Bar(n)); 12: EXPECT_EQ(2, Bar(n + 1)); 13: } 14: 15: TEST(FooTest, Bar) { 16: { 17: SCOPED_TRACE(\"A\"); // This trace point will be included in 18: // every failure in this scope. 19: Sub1(1); 20: } 21: // Now it won't. 22: Sub1(9); 23: } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs' compilation buffer - hit return on a line number and you'll be taken to that line in the source file! Availability: Linux, Windows, Mac.","title":"Adding Traces to Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#propagating-fatal-failures","text":"A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine() { // Generates a fatal failure and aborts the current function. ASSERT_EQ(1, 2); // The following won't be executed. ... } TEST(FooTest, Bar) { Subroutine(); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int* p = NULL; *p = 3; // Segfault! } Since we don't use exceptions, it is technically impossible to implement the intended behavior here. To alleviate this, Google Test provides two solutions. You could use either the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections.","title":"Propagating Fatal Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#asserting-on-subroutines","text":"As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that Google Test offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE( statement ); EXPECT_NO_FATAL_FAILURE( statement ); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE(Foo()); int i; EXPECT_NO_FATAL_FAILURE({ i = Bar(); }); Availability: Linux, Windows, Mac. Assertions from multiple threads are currently not supported.","title":"Asserting on Subroutines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#checking-for-failures-in-the-current-test","text":"HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public: ... static bool HasFatalFailure(); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST(FooTest, Bar) { Subroutine(); // Aborts if Subroutine() had a fatal failure. if (HasFatalFailure()) return; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if (::testing::Test::HasFatalFailure()) return; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Availability: Linux, Windows, Mac. HasNonfatalFailure() and HasFailure() are available since version 1.4.0.","title":"Checking for Failures in the Current Test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#logging-additional-information","text":"In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a string or an int . The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F(WidgetUsageTest, MinAndMaxWidgets) { RecordProperty(\"MaximumWidgets\", ComputeMaxUsage()); RecordProperty(\"MinimumWidgets\", ComputeMinUsage()); } will output XML like this: ... <testcase name=\"MinAndMaxWidgets\" status=\"run\" time=\"6\" classname=\"WidgetUsageTest\" MaximumWidgets=\"12\" MinimumWidgets=\"9\" /> ... Note : * RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. * key must be a valid XML attribute name, and cannot conflict with the ones already used by Google Test ( name , status , time , classname , type_param , and value_param ). * Calling RecordProperty() outside of the lifespan of a test is allowed. If it's called outside of a test but between a test case's SetUpTestCase() and TearDownTestCase() methods, it will be attributed to the XML element for the test case. If it's called outside of all test cases (e.g. in a test environment), it will be attributed to the top-level XML element. Availability : Linux, Windows, Mac.","title":"Logging Additional Information"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#sharing-resources-between-tests-in-the-same-test-case","text":"Google Test creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in them sharing a single resource copy. So, in addition to per-test set-up/tear-down, Google Test also supports per-test-case set-up/tear-down. To use it: In your test fixture class (say FooTest ), define as static some member variables to hold the shared resources. In the same test fixture class, define a static void SetUpTestCase() function (remember not to spell it as SetupTestCase with a small u !) to set up the shared resources and a static void TearDownTestCase() function to tear them down. That's it! Google Test automatically calls SetUpTestCase() before running the first test in the FooTest test case (i.e. before creating the first FooTest object), and calls TearDownTestCase() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-case set-up and tear-down: class FooTest : public ::testing::Test { protected: // Per-test-case set-up. // Called before the first test in this test case. // Can be omitted if not needed. static void SetUpTestCase() { shared_resource_ = new ...; } // Per-test-case tear-down. // Called after the last test in this test case. // Can be omitted if not needed. static void TearDownTestCase() { delete shared_resource_; shared_resource_ = NULL; } // You can define per-test set-up and tear-down logic as usual. virtual void SetUp() { ... } virtual void TearDown() { ... } // Some expensive resource shared by all tests. static T* shared_resource_; }; T* FooTest::shared_resource_ = NULL; TEST_F(FooTest, Test1) { ... you can refer to shared_resource here ... } TEST_F(FooTest, Test2) { ... you can refer to shared_resource here ... } Availability: Linux, Windows, Mac.","title":"Sharing Resources Between Tests in the Same Test Case"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#global-set-up-and-tear-down","text":"Just as you can do set-up and tear-down at the test level and the test case level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment { public: virtual ~Environment() {} // Override this to define how to set up the environment. virtual void SetUp() {} // Override this to define how to tear down the environment. virtual void TearDown() {} }; Then, you register an instance of your environment class with Google Test by calling the ::testing::AddGlobalTestEnvironment() function: Environment* AddGlobalTestEnvironment(Environment* env); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of the environment object, then runs the tests if there was no fatal failures, and finally calls TearDown() of the environment object. It's OK to register multiple environment objects. In this case, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that Google Test takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: ::testing::Environment* const foo_env = ::testing::AddGlobalTestEnvironment(new FooEnvironment); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Availability: Linux, Windows, Mac.","title":"Global Set-Up and Tear-Down"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#value-parameterized-tests","text":"Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. Suppose you write a test for your code and then realize that your code is affected by a presence of a Boolean command line flag. TEST(MyCodeTest, TestFoo) { // A code to test foo(). } Usually people factor their test code into a function with a Boolean parameter in such situations. The function sets the flag, then executes the testing code. void TestFooHelper(bool flag_value) { flag = flag_value; // A code to test foo(). } TEST(MyCodeTest, TestFoo) { TestFooHelper(false); TestFooHelper(true); } But this setup has serious drawbacks. First, when a test assertion fails in your tests, it becomes unclear what value of the parameter caused it to fail. You can stream a clarifying message into your EXPECT / ASSERT statements, but it you'll have to do it with all of them. Second, you have to add one such helper function per test. What if you have ten tests? Twenty? A hundred? Value-parameterized tests will let you write your test only once and then easily instantiate and run it with an arbitrary number of parameter values. Here are some other situations when value-parameterized tests come handy: You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it!","title":"Value Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#how-to-write-value-parameterized-tests","text":"To write value-parameterized tests, first you should define a fixture class. It must be derived from both ::testing::Test and ::testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from ::testing::TestWithParam<T> , which itself is derived from both ::testing::Test and ::testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. class FooTest : public ::testing::TestWithParam<const char*> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public ::testing::Test { ... }; class BarTest : public BaseTest, public ::testing::WithParamInterface<const char*> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P(FooTest, DoesBlah) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE(foo.Blah(GetParam())); ... } TEST_P(FooTest, HasBlahBlah) { ... } Finally, you can use INSTANTIATE_TEST_CASE_P to instantiate the test case with any set of parameters you want. Google Test defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Range(begin, end[, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin, end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) . container , begin , and end can be expressions whose values are determined at run time. Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (the Cartesian product for the math savvy) of the values generated by the N generators. This is only available if your system provides the <tr1/tuple> header. If you are sure your system does, and Google Test disagrees, you can override it by defining GTEST_HAS_TR1_TUPLE=1 . See comments in include/gtest/internal/gtest-port.h for more information. For more details, see the comments at the definitions of these functions in the source code . The following statement will instantiate tests from the FooTest test case each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_CASE_P(InstantiationName, FooTest, ::testing::Values(\"meeny\", \"miny\", \"moe\")); To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_CASE_P is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest_filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char* pets[] = {\"cat\", \"dog\"}; INSTANTIATE_TEST_CASE_P(AnotherInstantiationName, FooTest, ::testing::ValuesIn(pets)); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_CASE_P will instantiate all tests in the given test case, whether their definitions come before or after the INSTANTIATE_TEST_CASE_P statement. You can see these files for more examples. Availability : Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.2.0.","title":"How to Write Value-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#creating-value-parameterized-abstract-tests","text":"In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, he can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_CASE_P() , and linking with foo_param_test.cc . You can instantiate the same abstract test case multiple times, possibly in different source files.","title":"Creating Value-Parameterized Abstract Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#typed-tests","text":"Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template <typename T> class FooTest : public ::testing::Test { public: ... typedef std::list<T> List; static T shared_; T value_; }; Next, associate a list of types with the test case, which will be repeated for each type in the list: typedef ::testing::Types<char, int, unsigned int> MyTypes; TYPED_TEST_CASE(FooTest, MyTypes); The typedef is necessary for the TYPED_TEST_CASE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test case. You can repeat this as many times as you want: TYPED_TEST(FooTest, DoesBlah) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this->value_; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture::shared_; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture::List values; values.push_back(n); ... } TYPED_TEST(FooTest, HasPropertyA) { ... } You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Typed Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#type-parameterized-tests","text":"Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with his type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template <typename T> class FooTest : public ::testing::Test { ... }; Next, declare that you will define a type-parameterized test case: TYPED_TEST_CASE_P(FooTest); The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P(FooTest, DoesBlah) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0; ... } TYPED_TEST_P(FooTest, HasPropertyA) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_CASE_P macro before you can instantiate them. The first argument of the macro is the test case name; the rest are the names of the tests in this test case: REGISTER_TYPED_TEST_CASE_P(FooTest, DoesBlah, HasPropertyA); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef ::testing::Types<char, int, unsigned int> MyTypes; INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, MyTypes); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_CASE_P macro is a prefix that will be added to the actual test case name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_CASE_P(My, FooTest, int); You can see samples/sample6_unittest.cc for a complete example. Availability: Linux, Windows (requires MSVC 8.0 or above), Mac; since version 1.1.0.","title":"Type-Parameterized Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#testing-private-code","text":"If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle , most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design that wouldn't require you to do so. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members","title":"Testing Private Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#static-functions","text":"Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. ( #include ing .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients.","title":"Static Functions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#private-class-members","text":"Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST(TestCaseName, TestName); For example, // foo.h #include \"gtest/gtest_prod.h\" // Defines FRIEND_TEST. class Foo { ... private: FRIEND_TEST(FooTest, BarReturnsZeroOnNull); int Bar(void* x); }; // foo_test.cc ... TEST(FooTest, BarReturnsZeroOnNull) { Foo foo; EXPECT_EQ(0, foo.Bar(NULL)); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest; FRIEND_TEST(FooTest, Bar); FRIEND_TEST(FooTest, Baz); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public ::testing::Test { protected: ... }; TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } } // namespace my_namespace","title":"Private Class Members"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#catching-failures","text":"If you are building a testing utility on top of Google Test, you'll want to test your utility. What framework would you use to test it? Google Test, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But Google Test doesn't use exceptions, so how do we test that a piece of code generates an expected failure? \"gtest/gtest-spi.h\" contains some constructs to do this. After #include ing this header, you can use EXPECT_FATAL_FAILURE( statement, substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE( statement, substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE() cannot return a value. Note: Google Test is designed with threads in mind. Once the synchronization primitives in \"gtest/internal/gtest-port.h\" have been implemented, Google Test will become thread-safe, meaning that you can then use assertions in multiple threads concurrently. Before that, however, Google Test only supports single-threaded usage. Once thread-safe, EXPECT_FATAL_FAILURE() and EXPECT_NONFATAL_FAILURE() will capture failures in the current thread only. If statement creates new threads, failures in these threads will be ignored. If you want to capture failures from all threads instead, you should use the following macros: EXPECT_FATAL_FAILURE_ON_ALL_THREADS( statement, substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS( statement, substring );","title":"Catching Failures"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#getting-the-current-tests-name","text":"Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public: // Returns the test case name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char* test_case_name() const; const char* name() const; }; } // namespace testing To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const ::testing::TestInfo* const test_info = ::testing::UnitTest::GetInstance()->current_test_info(); printf(\"We are in test %s of test case %s.\\n\", test_info->name(), test_info->test_case_name()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test case name in TestCaseSetUp() , TestCaseTearDown() (where you know the test case name implicitly), or functions called from them. Availability: Linux, Windows, Mac.","title":"Getting the Current Test's Name"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#extending-google-test-by-handling-test-events","text":"Google Test provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test case, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Extending Google Test by Handling Test Events"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#defining-event-listeners","text":"To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener . The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: * UnitTest reflects the state of the entire test program, * TestCase has information about a test case, which can contain one or more tests, * TestInfo contains the state of a test, and * TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public ::testing::EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s starting.\\n\", test_info.test_case_name(), test_info.name()); } // Called after a failed assertion or a SUCCEED() invocation. virtual void OnTestPartResult( const ::testing::TestPartResult& test_part_result) { printf(\"%s in %s:%d\\n%s\\n\", test_part_result.failed() ? \"*** Failure\" : \"Success\", test_part_result.file_name(), test_part_result.line_number(), test_part_result.summary()); } // Called after a test ends. virtual void OnTestEnd(const ::testing::TestInfo& test_info) { printf(\"*** Test %s.%s ending.\\n\", test_info.test_case_name(), test_info.name()); } };","title":"Defining Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#using-event-listeners","text":"To use the event listener you have defined, add an instance of it to the Google Test event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main(int argc, char** argv) { ::testing::InitGoogleTest(&argc, argv); // Gets hold of the event listener list. ::testing::TestEventListeners& listeners = ::testing::UnitTest::GetInstance()->listeners(); // Adds a listener to the end. Google Test takes the ownership. listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners.Release(listeners.default_result_printer()); listeners.Append(new MinimalistPrinter); return RUN_ALL_TESTS(); Now, sit back and enjoy a completely different output from your tests. For more details, you can read this sample . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier.","title":"Using Event Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#generating-failures-in-listeners","text":"You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. We have a sample of failure-raising listener here .","title":"Generating Failures in Listeners"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#running-test-programs-advanced-options","text":"Google Test test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. This feature is added in version 1.3.0. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Most of the options can also be set/read in code: to access the value of command line flag --gtest_foo , write ::testing::GTEST_FLAG(foo) . A common pattern is to set the value of a flag before calling ::testing::InitGoogleTest() to change the default value of the flag: int main(int argc, char** argv) { // Disables elapsed time by default. ::testing::GTEST_FLAG(print_time) = false; // This allows the user to override the flag on the command line. ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); }","title":"Running Test Programs: Advanced Options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#selecting-tests","text":"This section shows various options for choosing which tests to run.","title":"Selecting Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#listing-test-names","text":"Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestCase1. TestName1 TestName2 TestCase2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Availability: Linux, Windows, Mac.","title":"Listing Test Names"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#running-a-subset-of-the-tests","text":"By default, a Google Test program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, Google Test will only run the tests whose full names (in the form of TestCaseName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test case FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test case FooTest except FooTest.Bar . Availability: Linux, Windows, Mac.","title":"Running a Subset of the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#temporarily-disabling-tests","text":"If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test case, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test case name. For example, the following tests won't be run by Google Test, even though they will still be compiled: // Tests that Foo does Abc. TEST(FooTest, DISABLED_DoesAbc) { ... } class DISABLED_BarTest : public ::testing::Test { ... }; // Tests that Bar does Xyz. TEST_F(DISABLED_BarTest, DoesXyz) { ... } Note: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, Google Test will print a banner warning you if a test program contains any disabled tests. Tip: You can easily count the number of disabled tests you have using grep . This number can be used as a metric for improving your test quality. Availability: Linux, Windows, Mac.","title":"Temporarily Disabling Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#temporarily-enabling-disabled-tests","text":"To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Temporarily Enabling Disabled Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#repeating-the-tests","text":"Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the testfails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code registered using AddGlobalTestEnvironment() , it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Availability: Linux, Windows, Mac.","title":"Repeating the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#shuffling-the-tests","text":"You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, Google Test uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer between 0 and 99999. The seed value 0 is special: it tells Google Test to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , Google Test will pick a different random seed and re-shuffle the tests in each iteration. Availability: Linux, Windows, Mac; since v1.4.0.","title":"Shuffling the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#controlling-test-output","text":"This section teaches how to tweak the way test results are reported.","title":"Controlling Test Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#colored-terminal-output","text":"Google Test can use colors in its terminal output to make it easier to spot the separation between tests, and whether tests passed. You can set the GTEST_COLOR environment variable or set the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let Google Test decide. When the value is auto , Google Test will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Availability: Linux, Windows, Mac.","title":"Colored Terminal Output"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#suppressing-the-elapsed-time","text":"By default, Google Test prints the time it takes to run each test. To suppress that, run the test program with the --gtest_print_time=0 command line flag. Setting the GTEST_PRINT_TIME environment variable to 0 has the same effect. Availability: Linux, Windows, Mac. (In Google Test 1.3.0 and lower, the default behavior is that the elapsed time is not printed.)","title":"Suppressing the Elapsed Time"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#generating-an-xml-report","text":"Google Test can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:_path_to_output_file_\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), Google Test will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), Google Test will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report uses the format described here. It is based on the junitreport Ant task and can be parsed by popular continuous build systems like Jenkins . Since that format was originally intended for Java, a little interpretation is required to make it apply to Google Test tests, as shown here: <testsuites name=\"AllTests\" ...> <testsuite name=\"test_case_name\" ...> <testcase name=\"test_name\" ...> <failure message=\"...\"/> <failure message=\"...\"/> <failure message=\"...\"/> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to Google Test test cases. <testcase> elements correspond to Google Test test functions. For instance, the following program TEST(MathTest, Addition) { ... } TEST(MathTest, Subtraction) { ... } TEST(LogicTest, NonContradiction) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests=\"3\" failures=\"1\" errors=\"0\" time=\"35\" name=\"AllTests\"> <testsuite name=\"MathTest\" tests=\"2\" failures=\"1\" errors=\"0\" time=\"15\"> <testcase name=\"Addition\" status=\"run\" time=\"7\" classname=\"\"> <failure message=\"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type=\"\"/> <failure message=\"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type=\"\"/> </testcase> <testcase name=\"Subtraction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> <testsuite name=\"LogicTest\" tests=\"1\" failures=\"0\" errors=\"0\" time=\"5\"> <testcase name=\"NonContradiction\" status=\"run\" time=\"5\" classname=\"\"> </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the Google Test program or test case contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test case, or entire test program in milliseconds. Each <failure> element corresponds to a single failed Google Test assertion. Some JUnit concepts don't apply to Google Test, yet we have to conform to the DTD. Therefore you'll see some dummy elements and attributes in the report. You can safely ignore these parts. Availability: Linux, Windows, Mac.","title":"Generating an XML Report"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#controlling-how-failures-are-reported","text":"","title":"Controlling How Failures Are Reported"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#turning-assertion-failures-into-break-points","text":"When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. Google Test's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Availability: Linux, Windows, Mac.","title":"Turning Assertion Failures into Break-Points"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#disabling-catching-test-thrown-exceptions","text":"Google Test can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default Google Test catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests. Availability : Linux, Windows, Mac.","title":"Disabling Catching Test-Thrown Exceptions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#letting-another-testing-framework-drive","text":"If you work on a project that has already been using another testing framework and is not ready to completely switch to Google Test yet, you can get much of Google Test's benefit by using its assertions in your existing tests. Just change your main() function to look like: #include \"gtest/gtest.h\" int main(int argc, char** argv) { ::testing::GTEST_FLAG(throw_on_failure) = true; // Important: Google Test must be initialized. ::testing::InitGoogleTest(&argc, argv); ... whatever your existing testing framework requires ... } With that, you can use Google Test assertions in addition to the native assertions your testing framework provides, for example: void TestFooDoesBar() { Foo foo; EXPECT_LE(foo.Bar(1), 100); // A Google Test assertion. CPPUNIT_ASSERT(foo.IsEmpty()); // A native assertion. } If a Google Test assertion fails, it will print an error message and throw an exception, which will be treated as a failure by your host testing framework. If you compile your code with exceptions disabled, a failed Google Test assertion will instead exit your program with a non-zero code, which will also signal a test failure to your test runner. If you don't write ::testing::GTEST_FLAG(throw_on_failure) = true; in your main() , you can alternatively enable this feature by specifying the --gtest_throw_on_failure flag on the command-line or setting the GTEST_THROW_ON_FAILURE environment variable to a non-zero value. Death tests are not supported when other test framework is used to organize tests. Availability: Linux, Windows, Mac; since v1.3.0.","title":"Letting Another Testing Framework Drive"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#distributing-test-functions-to-multiple-machines","text":"If you have more than one machine you can use to run a test program, you might want to run the test functions in parallel and get the result faster. We call this technique sharding , where each machine is called a shard . Google Test is compatible with test sharding. To take advantage of this feature, your test runner (not part of Google Test) needs to do the following: Allocate a number of machines (shards) to run the tests. On each shard, set the GTEST_TOTAL_SHARDS environment variable to the total number of shards. It must be the same for all shards. On each shard, set the GTEST_SHARD_INDEX environment variable to the index of the shard. Different shards must be assigned different indices, which must be in the range [0, GTEST_TOTAL_SHARDS - 1] . Run the same test program on all shards. When Google Test sees the above two environment variables, it will select a subset of the test functions to run. Across all shards, each test function in the program will be run exactly once. Wait for all shards to finish, then collect and report the results. Your project may have tests that were written without Google Test and thus don't understand this protocol. In order for your test runner to figure out which test supports sharding, it can set the environment variable GTEST_SHARD_STATUS_FILE to a non-existent file path. If a test program supports sharding, it will create this file to acknowledge the fact (the actual contents of the file are not important at this time; although we may stick some useful information in it in the future.); otherwise it will not create it. Here's an example to make it clear. Suppose you have a test program foo_test that contains the following 5 test functions: TEST(A, V) TEST(A, W) TEST(B, X) TEST(B, Y) TEST(B, Z) and you have 3 machines at your disposal. To run the test functions in parallel, you would set GTEST_TOTAL_SHARDS to 3 on all machines, and set GTEST_SHARD_INDEX to 0, 1, and 2 on the machines respectively. Then you would run the same foo_test on each machine. Google Test reserves the right to change how the work is distributed across the shards, but here's one possible scenario: Machine #0 runs A.V and B.X . Machine #1 runs A.W and B.Y . Machine #2 runs B.Z . Availability: Linux, Windows, Mac; since version 1.3.0.","title":"Distributing Test Functions to Multiple Machines"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#fusing-google-test-source-files","text":"Google Test's implementation consists of ~30 files (excluding its own tests). Sometimes you may want them to be packaged up in two files (a .h and a .cc ) instead, such that you can easily copy them to a new machine and start hacking there. For this we provide an experimental Python script fuse_gtest_files.py in the scripts/ directory (since release 1.3.0). Assuming you have Python 2.4 or above installed on your machine, just go to that directory and run python fuse_gtest_files.py OUTPUT_DIR and you should see an OUTPUT_DIR directory being created with files gtest/gtest.h and gtest/gtest-all.cc in it. These files contain everything you need to use Google Test. Just copy them to anywhere you want and you are ready to write tests. You can use the scripts/test/Makefile file as an example on how to compile your tests against them.","title":"Fusing Google Test Source Files"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_AdvancedGuide/#where-to-go-from-here","text":"Congratulations! You've now learned more advanced Google Test tools and are ready to tackle more complex testing tasks. If you want to dive even deeper, you can read the Frequently-Asked Questions .","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Documentation/","text":"This page lists all documentation wiki pages for Google Test (the SVN trunk version) -- if you use a released version of Google Test, please read the documentation for that specific version instead. Primer -- start here if you are new to Google Test. Samples -- learn from examples. AdvancedGuide -- learn more about Google Test. XcodeGuide -- how to use Google Test in Xcode on Mac. Frequently-Asked Questions -- check here before asking a question on the mailing list. To contribute code to Google Test, read: DevGuide -- read this before writing your first patch. PumpManual -- how we generate some of Google Test's source files.","title":"V1 7 Documentation"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/","text":"If you cannot find the answer to your question here, and you have read Primer and AdvancedGuide , send it to googletestframework@googlegroups.com . Why should I use Google Test instead of my favorite C++ testing framework? \u00b6 First, let us say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable: it doesn't require exceptions or RTTI; it works around various bugs in various compilers and environments; etc. As a result, it works on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. Google Test can generate XML test result reports that can be parsed by popular continuous build system like Hudson. Simple things are easy in Google Test, while hard things are possible: in addition to advanced features like global test environments and tests parameterized by values or types , Google Test supports various ways for the user to extend the framework -- if Google Test doesn't do something out of the box, chances are that a user can implement the feature using Google Test's public API, without changing Google Test itself. In particular, you can: expand your testing vocabulary by defining custom predicates , teach Google Test how to print your types , define your own testing macros or utilities and verify them using Google Test's Service Provider Interface , and reflect on the test cases or change the test output format by intercepting the test events . I'm getting warnings when compiling Google Test. Would you fix them? \u00b6 We strive to minimize compiler warnings Google Test generates. Before releasing a new version, we test to make sure that it doesn't generate warnings when compiled using its CMake script on Windows, Linux, and Mac OS. Unfortunately, this doesn't mean you are guaranteed to see no warnings when compiling Google Test in your environment: You may be using a different compiler as we use, or a different version of the same compiler. We cannot possibly test for all compilers. You may be compiling on a different platform as we do. Your project may be using different compiler flags as we do. It is not always possible to make Google Test warning-free for everyone. Or, it may not be desirable if the warning is rarely enabled and fixing the violations makes the code more complex. If you see warnings when compiling Google Test, we suggest that you use the -isystem flag (assuming your are using GCC) to mark Google Test headers as system headers. That'll suppress warnings from Google Test headers. Why should not test case names and test names contain underscore? \u00b6 Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that containers two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestCaseName, TestName) generates a class named TestCaseName_TestName_Test . What happens if TestCaseName or TestName contains _ ? If TestCaseName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestCaseName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestCaseName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestCaseName_Bar__Test , which is invalid. So clearly TestCaseName and TestName cannot start or end with _ (Actually, TestCaseName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestCaseName and TestName to contain _ in the middle. However, consider this: TEST(Time, Flies_Like_An_Arrow) { ... } TEST(Time_Flies, Like_An_Arrow) { ... } Now, the two TEST s will both generate the same class ( Time_Files_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestCaseName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives Google Test some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediately consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of Google Test. Therefore it's best to follow the rule. Why is it not recommended to install a pre-compiled copy of Google Test (for example, into /usr/local)? \u00b6 In the early days, we said that you could install compiled Google Test libraries on * nix systems using make install . Then every user of your machine can write tests without recompiling Google Test. This seemed like a good idea, but it has a got-cha: every user needs to compile his tests using the same compiler flags used to compile the installed Google Test libraries; otherwise he may run into undefined behaviors (i.e. the tests can behave strangely and may even crash for no obvious reasons). Why? Because C++ has this thing called the One-Definition Rule: if two C++ source files contain different definitions of the same class/function/variable, and you link them together, you violate the rule. The linker may or may not catch the error (in many cases it's not required by the C++ standard to catch the violation). If it doesn't, you get strange run-time behaviors that are unexpected and hard to debug. If you compile Google Test and your test code using different compiler flags, they may see different definitions of the same class/function/variable (e.g. due to the use of #if in Google Test). Therefore, for your sanity, we recommend to avoid installing pre-compiled Google Test libraries. Instead, each project should compile Google Test itself such that it can be sure that the same flags are used for both Google Test and the tests. How do I generate 64-bit binaries on Windows (using Visual Studio 2008)? \u00b6 (Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory. Can I use Google Test on MinGW? \u00b6 We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW. Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)? \u00b6 Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr != NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros. Does Google Test support running tests in parallel? \u00b6 Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel. Why don't Google Test run the tests in different threads to speed things up? \u00b6 It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes. Why aren't Google Test assertions implemented using exceptions? \u00b6 Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE(...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST . Why do we use two different macros for tests with and without fixtures? \u00b6 Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public ::testing::Test {}; TEST_F(FooTest, DoesThis) { ... } or typedef ::testing::Test FooTest; TEST_F(FooTest, DoesThat) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way. Why don't we use structs as test fixtures? \u00b6 We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes. Why are death tests implemented as assertions instead of using a test runner? \u00b6 Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if (FooCondition()) { ASSERT_DEATH(Bar(), \"blah\"); } else { ASSERT_EQ(5, Bar()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount(); // Only known at run time. for (int i = 1; i <= count; i++) { ASSERT_DEATH({ double* buffer = new double[i]; ... initializes buffer ... Foo(buffer, i) }, \"blah blah\"); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically. My death test modifies some state, but the change seems lost after the death test finishes. Why? \u00b6 Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less. The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong? \u00b6 If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100; }; You also need to define it outside of the class body in foo.cc : const int Foo::kBar; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error. I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations? \u00b6 Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon. Can I derive a test fixture from another? \u00b6 Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public ::testing::Test { protected: ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected: virtual void SetUp() { BaseTest::SetUp(); // Sets up the base fixture first. ... additional set-up work ... } virtual void TearDown() { ... clean-up work for FooTest ... BaseTest::TearDown(); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5 . My compiler complains \"void value not ignored as it ought to be.\" What does this mean? \u00b6 You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions. My death test hangs (or seg-faults). How do I fix it? \u00b6 In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry! Should I use the constructor/destructor of the test fixture or the set-up/tear-down function? \u00b6 The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test, call TearDown() , and then immediately delete the test fixture object. Therefore, there is no need to write a SetUp() or TearDown() function if the constructor or destructor already does the job. You may still want to use SetUp()/TearDown() in the following cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The assertion macros throw an exception when flag --gtest_throw_on_failure is specified. Therefore, you shouldn't use Google Test assertions in a destructor if you plan to run your tests with this flag. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() . The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it? \u00b6 If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive(int n) { return n > 0; } bool IsPositive(double x) { return x > 0; } you will get a compiler error if you write EXPECT_PRED1(IsPositive, 5); However, this will work: EXPECT_PRED1(*static_cast<bool (*)(int)>*(IsPositive), 5); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template <typename T> bool IsNegative(T x) { return x < 0; } you can use it in a predicate assertion like this: ASSERT_PRED1(IsNegative*<int>*, -5); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2(*GreaterThan<int, int>*, 5, 0); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2(*(GreaterThan<int, int>)*, 5, 0); My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why? \u00b6 Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS(); they write RUN_ALL_TESTS(); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() . My compiler complains that a constructor (or destructor) cannot return a value. What's going on? \u00b6 Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ(1, Foo()) << \"blah blah\" << foo; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it. My set-up function is not called. Why? \u00b6 C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called. How do I jump to the line of a failure in Emacs directly? \u00b6 Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure. I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious. \u00b6 You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } class BarTest : public BaseTest {}; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef BaseTest BarTest; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } The Google Test output is buried in a whole bunch of log messages. What do I do? \u00b6 The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt Why should I prefer test fixtures over global variables? \u00b6 There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common. How do I test private class members without writing FRIEND_TEST()s? \u00b6 You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... void Test1() {...} // This accesses private members of class Foo. void Test2() {...} // So does this one. }; TEST_F(FooTest, Test1) { Test1(); } TEST_F(FooTest, Test2) { Test2(); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... T1 get_private_member1(Foo* obj) { return obj->private_member1_; } }; TEST_F(FooTest, Test1) { ... get_private_member1(x) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected: // protected access for testability. int DoSomethingReturningInt(); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public: using YourClass::DoSomethingReturningInt; // changes access rights ... }; TEST_F(YourClassTest, DoSomethingTest) { TestableYourClass obj; assertEquals(expected_value, obj.DoSomethingReturningInt()); } How do I test private class static members without writing FRIEND_TEST()s? \u00b6 We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private: static bool Func(int n); }; // foo.cc bool Foo::Func(int n) { ... } // foo_test.cc EXPECT_TRUE(Foo::Func(12345)); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func(int n) { ... } } // foo_test.cc namespace internal { bool Func(int n); } EXPECT_TRUE(internal::Func(12345)); I would like to run a test several times with different parameters. Do I need to write several similar copies of it? \u00b6 No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once. How do I test a file that defines main()? \u00b6 To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort. What can the statement argument in ASSERT_DEATH() be? \u00b6 ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST(MyDeathTest, FunctionCall) { ASSERT_DEATH(Xyz(5), \"Xyz failed\"); } // Or a complex expression that references variables and functions. TEST(MyDeathTest, ComplexExpression) { const bool c = Condition(); ASSERT_DEATH((c ? Func1(0) : object2.Method(\"test\")), \"(Func1|Method) failed\"); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST(MyDeathTest, InsideLoop) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for (int i = 0; i < 5; i++) { EXPECT_DEATH_M(Foo(i), \"Foo has \\\\d+ errors\", ::testing::Message() << \"where i is \" << i); } } // A death assertion can contain a compound statement. TEST(MyDeathTest, CompoundStatement) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH({ for (int i = 0; i < 5; i++) { Bar(i); } }, \"Bar has \\\\d+ errors\");} googletest_unittest.cc contains more examples if you are interested. What syntax does the regular expression in ASSERT_DEATH use? \u00b6 On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax . I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why? \u00b6 Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .) Why does ASSERT_DEATH complain about previous threads that were already joined? \u00b6 With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this. Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH? \u00b6 Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F(FooTest, AbcDeathTest) { ... } TEST_F(FooTest, Uvw) { ... } TEST_F(BarTest, DefDeathTest) { ... } TEST_F(BarTest, Xyz) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw . But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do? \u00b6 You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public ::testing::Test { ... }; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef FooTest FooDeathTest; TEST_F(FooDeathTest, Uvw) { ... EXPECT_DEATH(...) ... } TEST_F(FooDeathTest, Xyz) { ... ASSERT_DEATH(...) ... } The compiler complains about \"no match for 'operator<<'\" when I use an assertion. What gives? \u00b6 If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space. How do I suppress the memory leak messages on Windows? \u00b6 Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines. I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help! \u00b6 You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj. I put my tests in a library and Google Test doesn't run them. What's happening? \u00b6 Have you read a warning on the Google Test Primer page? I want to use Google Test with Visual Studio but don't know where to start. \u00b6 Many people are in your position and one of the posted his solution to our mailing list. Here is his link: http://hassanjamilahmad.blogspot.com/2009/07/gtest-starters-help.html . I am seeing compile errors mentioning std::type_traits when I try to use Google Test on Solaris. \u00b6 Google Test uses parts of the standard C++ library that SunStudio does not support. Our users reported success using alternative implementations. Try running the build after runing this commad: export CC=cc CXX=CC CXXFLAGS='-library=stlport4' How can my code detect if it is running in a test? \u00b6 If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and Google Test doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is dependency injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all, there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test. Google Test defines a macro that clashes with one defined by another library. How do I deal with that? \u00b6 In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO , you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST(SomeTest, DoesThis) { ... } instead of TEST(SomeTest, DoesThis) { ... } in order to define a test. Currently, the following TEST , FAIL , SUCCEED , and the basic comparison assertion macros can have alternative names. You can see the full list of covered macros here . More information can be found in the \"Avoiding Macro Name Clashes\" section of the README file. Is it OK if I have two separate TEST(Foo, Bar) test methods defined in different namespaces? \u00b6 Yes. The rule is all test methods in the same test case must use the same fixture class . This means that the following is allowed because both tests use the same fixture class ( ::testing::Test ). namespace foo { TEST(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo namespace bar { TEST(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo However, the following code is not allowed and will produce a runtime error from Google Test because the test methods are using different test fixture classes with the same test case name. namespace foo { class CoolTest : public ::testing::Test {}; // Fixture foo::CoolTest TEST_F(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo namespace bar { class CoolTest : public ::testing::Test {}; // Fixture: bar::CoolTest TEST_F(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo How do I build Google Testing Framework with Xcode 4? \u00b6 If you try to build Google Test's Xcode project with Xcode 4.0 or later, you may encounter an error message that looks like \"Missing SDK in target gtest_framework: /Developer/SDKs/MacOSX10.4u.sdk\". That means that Xcode does not support the SDK the project is targeting. See the Xcode section in the README file on how to resolve this. My question is not covered in your FAQ! \u00b6 If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the revision number if you check out from SVN directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.","title":"V1 7 FAQ"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-should-i-use-google-test-instead-of-my-favorite-c-testing-framework","text":"First, let us say clearly that we don't want to get into the debate of which C++ testing framework is the best . There exist many fine frameworks for writing C++ tests, and we have tremendous respect for the developers and users of them. We don't think there is (or will be) a single best framework - you have to pick the right tool for the particular task you are tackling. We created Google Test because we couldn't find the right combination of features and conveniences in an existing framework to satisfy our needs. The following is a list of things that we like about Google Test. We don't claim them to be unique to Google Test - rather, the combination of them makes Google Test the choice for us. We hope this list can help you decide whether it is for you too. Google Test is designed to be portable: it doesn't require exceptions or RTTI; it works around various bugs in various compilers and environments; etc. As a result, it works on Linux, Mac OS X, Windows and several embedded operating systems. Nonfatal assertions ( EXPECT_* ) have proven to be great time savers, as they allow a test to report multiple failures in a single edit-compile-test cycle. It's easy to write assertions that generate informative messages: you just use the stream syntax to append any additional information, e.g. ASSERT_EQ(5, Foo(i)) << \" where i = \" << i; . It doesn't require a new set of macros or special functions. Google Test automatically detects your tests and doesn't require you to enumerate them in order to run them. Death tests are pretty handy for ensuring that your asserts in production code are triggered by the right conditions. SCOPED_TRACE helps you understand the context of an assertion failure when it comes from inside a sub-routine or loop. You can decide which tests to run using name patterns. This saves time when you want to quickly reproduce a test failure. Google Test can generate XML test result reports that can be parsed by popular continuous build system like Hudson. Simple things are easy in Google Test, while hard things are possible: in addition to advanced features like global test environments and tests parameterized by values or types , Google Test supports various ways for the user to extend the framework -- if Google Test doesn't do something out of the box, chances are that a user can implement the feature using Google Test's public API, without changing Google Test itself. In particular, you can: expand your testing vocabulary by defining custom predicates , teach Google Test how to print your types , define your own testing macros or utilities and verify them using Google Test's Service Provider Interface , and reflect on the test cases or change the test output format by intercepting the test events .","title":"Why should I use Google Test instead of my favorite C++ testing framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#im-getting-warnings-when-compiling-google-test-would-you-fix-them","text":"We strive to minimize compiler warnings Google Test generates. Before releasing a new version, we test to make sure that it doesn't generate warnings when compiled using its CMake script on Windows, Linux, and Mac OS. Unfortunately, this doesn't mean you are guaranteed to see no warnings when compiling Google Test in your environment: You may be using a different compiler as we use, or a different version of the same compiler. We cannot possibly test for all compilers. You may be compiling on a different platform as we do. Your project may be using different compiler flags as we do. It is not always possible to make Google Test warning-free for everyone. Or, it may not be desirable if the warning is rarely enabled and fixing the violations makes the code more complex. If you see warnings when compiling Google Test, we suggest that you use the -isystem flag (assuming your are using GCC) to mark Google Test headers as system headers. That'll suppress warnings from Google Test headers.","title":"I'm getting warnings when compiling Google Test.  Would you fix them?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-should-not-test-case-names-and-test-names-contain-underscore","text":"Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that containers two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestCaseName, TestName) generates a class named TestCaseName_TestName_Test . What happens if TestCaseName or TestName contains _ ? If TestCaseName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestCaseName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestCaseName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestCaseName_Bar__Test , which is invalid. So clearly TestCaseName and TestName cannot start or end with _ (Actually, TestCaseName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestCaseName and TestName to contain _ in the middle. However, consider this: TEST(Time, Flies_Like_An_Arrow) { ... } TEST(Time_Flies, Like_An_Arrow) { ... } Now, the two TEST s will both generate the same class ( Time_Files_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestCaseName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives Google Test some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediately consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of Google Test. Therefore it's best to follow the rule.","title":"Why should not test case names and test names contain underscore?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-is-it-not-recommended-to-install-a-pre-compiled-copy-of-google-test-for-example-into-usrlocal","text":"In the early days, we said that you could install compiled Google Test libraries on * nix systems using make install . Then every user of your machine can write tests without recompiling Google Test. This seemed like a good idea, but it has a got-cha: every user needs to compile his tests using the same compiler flags used to compile the installed Google Test libraries; otherwise he may run into undefined behaviors (i.e. the tests can behave strangely and may even crash for no obvious reasons). Why? Because C++ has this thing called the One-Definition Rule: if two C++ source files contain different definitions of the same class/function/variable, and you link them together, you violate the rule. The linker may or may not catch the error (in many cases it's not required by the C++ standard to catch the violation). If it doesn't, you get strange run-time behaviors that are unexpected and hard to debug. If you compile Google Test and your test code using different compiler flags, they may see different definitions of the same class/function/variable (e.g. due to the use of #if in Google Test). Therefore, for your sanity, we recommend to avoid installing pre-compiled Google Test libraries. Instead, each project should compile Google Test itself such that it can be sure that the same flags are used for both Google Test and the tests.","title":"Why is it not recommended to install a pre-compiled copy of Google Test (for example, into /usr/local)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-generate-64-bit-binaries-on-windows-using-visual-studio-2008","text":"(Answered by Trevor Robinson) Load the supplied Visual Studio solution file, either msvc\\gtest-md.sln or msvc\\gtest.sln . Go through the migration wizard to migrate the solution and project files to Visual Studio 2008. Select Configuration Manager... from the Build menu. Select <New...> from the Active solution platform dropdown. Select x64 from the new platform dropdown, leave Copy settings from set to Win32 and Create new project platforms checked, then click OK . You now have Win32 and x64 platform configurations, selectable from the Standard toolbar, which allow you to toggle between building 32-bit or 64-bit binaries (or both at once using Batch Build). In order to prevent build output files from overwriting one another, you'll need to change the Intermediate Directory settings for the newly created platform configuration across all the projects. To do this, multi-select (e.g. using shift-click) all projects (but not the solution) in the Solution Explorer . Right-click one of them and select Properties . In the left pane, select Configuration Properties , and from the Configuration dropdown, select All Configurations . Make sure the selected platform is x64 . For the Intermediate Directory setting, change the value from $(PlatformName)\\$(ConfigurationName) to $(OutDir)\\$(ProjectName) . Click OK and then build the solution. When the build is complete, the 64-bit binaries will be in the msvc\\x64\\Debug directory.","title":"How do I generate 64-bit binaries on Windows (using Visual Studio 2008)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#can-i-use-google-test-on-mingw","text":"We haven't tested this ourselves, but Per Abrahamsen reported that he was able to compile and install Google Test successfully when using MinGW from Cygwin. You'll need to configure it with: PATH/TO/configure CC=\"gcc -mno-cygwin\" CXX=\"g++ -mno-cygwin\" You should be able to replace the -mno-cygwin option with direct links to the real MinGW binaries, but we haven't tried that. Caveats: There are many warnings when compiling. make check will produce some errors as not all tests for Google Test itself are compatible with MinGW. We also have reports on successful cross compilation of Google Test MinGW binaries on Linux using these instructions on the WxWidgets site. Please contact googletestframework@googlegroups.com if you are interested in improving the support for MinGW.","title":"Can I use Google Test on MinGW?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-does-google-test-support-expect_eqnull-ptr-and-assert_eqnull-ptr-but-not-expect_nenull-ptr-and-assert_nenull-ptr","text":"Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of Google Test harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr != NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of Google Mock's matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros.","title":"Why does Google Test support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#does-google-test-support-running-tests-in-parallel","text":"Test runners tend to be tightly coupled with the build/test environment, and Google Test doesn't try to solve the problem of running tests in parallel. Instead, we tried to make Google Test work nicely with test runners. For example, Google Test's XML report contains the time spent on each test, and its gtest_list_tests and gtest_filter flags can be used for splitting the execution of test methods into multiple processes. These functionalities can help the test runner run the tests in parallel.","title":"Does Google Test support running tests in parallel?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-dont-google-test-run-the-tests-in-different-threads-to-speed-things-up","text":"It's difficult to write thread-safe code. Most tests are not written with thread-safety in mind, and thus may not work correctly in a multi-threaded setting. If you think about it, it's already hard to make your code work when you know what other threads are doing. It's much harder, and sometimes even impossible, to make your code work when you don't know what other threads are doing (remember that test methods can be added, deleted, or modified after your test was written). If you want to run the tests in parallel, you'd better run them in different processes.","title":"Why don't Google Test run the tests in different threads to speed things up?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-arent-google-test-assertions-implemented-using-exceptions","text":"Our original motivation was to be able to use Google Test in projects that disable exceptions. Later we realized some additional benefits of this approach: Throwing in a destructor is undefined behavior in C++. Not using exceptions means Google Test's assertions are safe to use in destructors. The EXPECT_* family of macros will continue even after a failure, allowing multiple failures in a TEST to be reported in a single run. This is a popular feature, as in C++ the edit-compile-test cycle is usually quite long and being able to fixing more than one thing at a time is a blessing. If assertions are implemented using exceptions, a test may falsely ignore a failure if it's caught by user code: try { ... ASSERT_TRUE(...) ... } catch (...) { ... } The above code will pass even if the ASSERT_TRUE throws. While it's unlikely for someone to write this in a test, it's possible to run into this pattern when you write assertions in callbacks that are called by the code under test. The downside of not using exceptions is that ASSERT_* (implemented using return ) will only abort the current function, not the current TEST .","title":"Why aren't Google Test assertions implemented using exceptions?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-do-we-use-two-different-macros-for-tests-with-and-without-fixtures","text":"Unfortunately, C++'s macro system doesn't allow us to use the same macro for both cases. One possibility is to provide only one macro for tests with fixtures, and require the user to define an empty fixture sometimes: class FooTest : public ::testing::Test {}; TEST_F(FooTest, DoesThis) { ... } or typedef ::testing::Test FooTest; TEST_F(FooTest, DoesThat) { ... } Yet, many people think this is one line too many. :-) Our goal was to make it really easy to write tests, so we tried to make simple tests trivial to create. That means using a separate macro for such tests. We think neither approach is ideal, yet either of them is reasonable. In the end, it probably doesn't matter much either way.","title":"Why do we use two different macros for tests with and without fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-dont-we-use-structs-as-test-fixtures","text":"We like to use structs only when representing passive data. This distinction between structs and classes is good for documenting the intent of the code's author. Since test fixtures have logic like SetUp() and TearDown() , they are better defined as classes.","title":"Why don't we use structs as test fixtures?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-are-death-tests-implemented-as-assertions-instead-of-using-a-test-runner","text":"Our goal was to make death tests as convenient for a user as C++ possibly allows. In particular: The runner-style requires to split the information into two pieces: the definition of the death test itself, and the specification for the runner on how to run the death test and what to expect. The death test would be written in C++, while the runner spec may or may not be. A user needs to carefully keep the two in sync. ASSERT_DEATH(statement, expected_message) specifies all necessary information in one place, in one language, without boilerplate code. It is very declarative. ASSERT_DEATH has a similar syntax and error-reporting semantics as other Google Test assertions, and thus is easy to learn. ASSERT_DEATH can be mixed with other assertions and other logic at your will. You are not limited to one death test per test method. For example, you can write something like: if (FooCondition()) { ASSERT_DEATH(Bar(), \"blah\"); } else { ASSERT_EQ(5, Bar()); } If you prefer one death test per test method, you can write your tests in that style too, but we don't want to impose that on the users. The fewer artificial limitations the better. ASSERT_DEATH can reference local variables in the current function, and you can decide how many death tests you want based on run-time information. For example, const int count = GetCount(); // Only known at run time. for (int i = 1; i <= count; i++) { ASSERT_DEATH({ double* buffer = new double[i]; ... initializes buffer ... Foo(buffer, i) }, \"blah blah\"); } The runner-based approach tends to be more static and less flexible, or requires more user effort to get this kind of flexibility. Another interesting thing about ASSERT_DEATH is that it calls fork() to create a child process to run the death test. This is lightening fast, as fork() uses copy-on-write pages and incurs almost zero overhead, and the child process starts from the user-supplied statement directly, skipping all global and local initialization and any code leading to the given statement. If you launch the child process from scratch, it can take seconds just to load everything and start running if the test links to many libraries dynamically.","title":"Why are death tests implemented as assertions instead of using a test runner?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-death-test-modifies-some-state-but-the-change-seems-lost-after-the-death-test-finishes-why","text":"Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less.","title":"My death test modifies some state, but the change seems lost after the death test finishes. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#the-compiler-complains-about-undefined-references-to-some-static-const-member-variables-but-i-did-define-them-in-the-class-body-whats-wrong","text":"If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100; }; You also need to define it outside of the class body in foo.cc : const int Foo::kBar; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in Google Test comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error.","title":"The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-have-an-interface-that-has-several-implementations-can-i-write-a-set-of-tests-once-and-repeat-them-over-all-the-implementations","text":"Google Test doesn't yet have good support for this kind of tests, or data-driven tests in general. We hope to be able to make improvements in this area soon.","title":"I have an interface that has several implementations. Can I write a set of tests once and repeat them over all the implementations?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#can-i-derive-a-test-fixture-from-another","text":"Yes. Each test fixture has a corresponding and same named test case. This means only one test case can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test cases don't leak important system resources like fonts and brushes. In Google Test, you share a fixture among test cases by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test case that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public ::testing::Test { protected: ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected: virtual void SetUp() { BaseTest::SetUp(); // Sets up the base fixture first. ... additional set-up work ... } virtual void TearDown() { ... clean-up work for FooTest ... BaseTest::TearDown(); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F(FooTest, Bar) { ... } TEST_F(FooTest, Baz) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. Google Test has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5 .","title":"Can I derive a test fixture from another?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-compiler-complains-void-value-not-ignored-as-it-ought-to-be-what-does-this-mean","text":"You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions.","title":"My compiler complains \"void value not ignored as it ought to be.\" What does this mean?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-death-test-hangs-or-seg-faults-how-do-i-fix-it","text":"In Google Test, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this. In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry!","title":"My death test hangs (or seg-faults). How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#should-i-use-the-constructordestructor-of-the-test-fixture-or-the-set-uptear-down-function","text":"The first thing to remember is that Google Test does not reuse the same test fixture object across multiple tests. For each TEST_F , Google Test will create a fresh test fixture object, immediately call SetUp() , run the test, call TearDown() , and then immediately delete the test fixture object. Therefore, there is no need to write a SetUp() or TearDown() function if the constructor or destructor already does the job. You may still want to use SetUp()/TearDown() in the following cases: * If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. * The assertion macros throw an exception when flag --gtest_throw_on_failure is specified. Therefore, you shouldn't use Google Test assertions in a destructor if you plan to run your tests with this flag. * In a constructor or destructor, you cannot make a virtual function call on this object. (You can call a method declared as virtual, but it will be statically bound.) Therefore, if you need to call a method that will be overriden in a derived class, you have to use SetUp()/TearDown() .","title":"Should I use the constructor/destructor of the test fixture or the set-up/tear-down function?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#the-compiler-complains-no-matching-function-to-call-when-i-use-assert_predn-how-do-i-fix-it","text":"If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive(int n) { return n > 0; } bool IsPositive(double x) { return x > 0; } you will get a compiler error if you write EXPECT_PRED1(IsPositive, 5); However, this will work: EXPECT_PRED1(*static_cast<bool (*)(int)>*(IsPositive), 5); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template <typename T> bool IsNegative(T x) { return x < 0; } you can use it in a predicate assertion like this: ASSERT_PRED1(IsNegative*<int>*, -5); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2(*GreaterThan<int, int>*, 5, 0); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2(*(GreaterThan<int, int>)*, 5, 0);","title":"The compiler complains \"no matching function to call\" when I use ASSERT_PREDn. How do I fix it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-compiler-complains-about-ignoring-return-value-when-i-call-run_all_tests-why","text":"Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS(); they write RUN_ALL_TESTS(); This is wrong and dangerous. A test runner needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a Google Test assertion failure. Very bad. To help the users avoid this dangerous bug, the implementation of RUN_ALL_TESTS() causes gcc to raise this warning, when the return value is ignored. If you see this warning, the fix is simple: just make sure its value is used as the return value of main() .","title":"My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-compiler-complains-that-a-constructor-or-destructor-cannot-return-a-value-whats-going-on","text":"Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ(1, Foo()) << \"blah blah\" << foo; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it.","title":"My compiler complains that a constructor (or destructor) cannot return a value. What's going on?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-set-up-function-is-not-called-why","text":"C++ is case-sensitive. It should be spelled as SetUp() . Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestCase() as SetupTestCase() and wonder why it's never called.","title":"My set-up function is not called. Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-jump-to-the-line-of-a-failure-in-emacs-directly","text":"Google Test's failure message format is understood by Emacs and many other IDEs, like acme and XCode. If a Google Test message is in a compilation buffer in Emacs, then it's clickable. You can now hit enter on a message to jump to the corresponding source code, or use `C-x `` to jump to the next failure.","title":"How do I jump to the line of a failure in Emacs directly?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-have-several-test-cases-which-share-the-same-test-fixture-logic-do-i-have-to-define-a-new-test-fixture-class-for-each-of-them-this-seems-pretty-tedious","text":"You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } class BarTest : public BaseTest {}; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef BaseTest BarTest; TEST_F(BarTest, Abc) { ... } TEST_F(BarTest, Def) { ... }","title":"I have several test cases which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#the-google-test-output-is-buried-in-a-whole-bunch-of-log-messages-what-do-i-do","text":"The Google Test output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the Google Test output, making it hard to read. However, there is an easy solution to this problem. Since most log messages go to stderr, we decided to let Google Test output go to stdout. This way, you can easily separate the two using redirection. For example: ./my_test > googletest_output.txt","title":"The Google Test output is buried in a whole bunch of log messages. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-should-i-prefer-test-fixtures-over-global-variables","text":"There are several good reasons: 1. It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. 1. Global variables pollute the global namespace. 1. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test cases have something in common.","title":"Why should I prefer test fixtures over global variables?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-test-private-class-members-without-writing-friend_tests","text":"You should try to write testable code, which means classes should be easily tested from their public interface. One way to achieve this is the Pimpl idiom: you move all private members of a class into a helper class, and make all members of the helper class public. You have several other options that don't require using FRIEND_TEST : * Write the tests as members of the fixture class: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... void Test1() {...} // This accesses private members of class Foo. void Test2() {...} // So does this one. }; TEST_F(FooTest, Test1) { Test1(); } TEST_F(FooTest, Test2) { Test2(); } * In the fixture class, write accessors for the tested class' private members, then use the accessors in your tests: class Foo { friend class FooTest; ... }; class FooTest : public ::testing::Test { protected: ... T1 get_private_member1(Foo* obj) { return obj->private_member1_; } }; TEST_F(FooTest, Test1) { ... get_private_member1(x) ... } * If the methods are declared protected , you can change their access level in a test-only subclass: class YourClass { ... protected: // protected access for testability. int DoSomethingReturningInt(); ... }; // in the your_class_test.cc file: class TestableYourClass : public YourClass { ... public: using YourClass::DoSomethingReturningInt; // changes access rights ... }; TEST_F(YourClassTest, DoSomethingTest) { TestableYourClass obj; assertEquals(expected_value, obj.DoSomethingReturningInt()); }","title":"How do I test private class members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-test-private-class-static-members-without-writing-friend_tests","text":"We find private static methods clutter the header file. They are implementation details and ideally should be kept out of a .h. So often I make them free functions instead. Instead of: // foo.h class Foo { ... private: static bool Func(int n); }; // foo.cc bool Foo::Func(int n) { ... } // foo_test.cc EXPECT_TRUE(Foo::Func(12345)); You probably should better write: // foo.h class Foo { ... }; // foo.cc namespace internal { bool Func(int n) { ... } } // foo_test.cc namespace internal { bool Func(int n); } EXPECT_TRUE(internal::Func(12345));","title":"How do I test private class static members without writing FRIEND_TEST()s?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-would-like-to-run-a-test-several-times-with-different-parameters-do-i-need-to-write-several-similar-copies-of-it","text":"No. You can use a feature called value-parameterized tests which lets you repeat your tests with different parameters, without defining it more than once.","title":"I would like to run a test several times with different parameters. Do I need to write several similar copies of it?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-test-a-file-that-defines-main","text":"To test a foo.cc file, you need to compile and link it into your unit test program. However, when the file contains a definition for the main() function, it will clash with the main() of your unit test, and will result in a build error. The right solution is to split it into three files: 1. foo.h which contains the declarations, 1. foo.cc which contains the definitions except main() , and 1. foo_main.cc which contains nothing but the definition of main() . Then foo.cc can be easily tested. If you are adding tests to an existing file and don't want an intrusive change like this, there is a hack: just include the entire foo.cc file in your unit test. For example: // File foo_unittest.cc // The headers section ... // Renames main() in foo.cc to make room for the unit test main() #define main FooMain #include \"a/b/foo.cc\" // The tests start here. ... However, please remember this is a hack and should only be used as the last resort.","title":"How do I test a file that defines main()?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#what-can-the-statement-argument-in-assert_death-be","text":"ASSERT_DEATH(_statement_, _regex_) (or any death assertion macro) can be used wherever _statement_ is valid. So basically _statement_ can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: * a simple function call (often the case), * a complex expression, or * a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST(MyDeathTest, FunctionCall) { ASSERT_DEATH(Xyz(5), \"Xyz failed\"); } // Or a complex expression that references variables and functions. TEST(MyDeathTest, ComplexExpression) { const bool c = Condition(); ASSERT_DEATH((c ? Func1(0) : object2.Method(\"test\")), \"(Func1|Method) failed\"); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST(MyDeathTest, InsideLoop) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for (int i = 0; i < 5; i++) { EXPECT_DEATH_M(Foo(i), \"Foo has \\\\d+ errors\", ::testing::Message() << \"where i is \" << i); } } // A death assertion can contain a compound statement. TEST(MyDeathTest, CompoundStatement) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH({ for (int i = 0; i < 5; i++) { Bar(i); } }, \"Bar has \\\\d+ errors\");} googletest_unittest.cc contains more examples if you are interested.","title":"What can the statement argument in ASSERT_DEATH() be?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#what-syntax-does-the-regular-expression-in-assert_death-use","text":"On POSIX systems, Google Test uses the POSIX Extended regular expression syntax ( http://en.wikipedia.org/wiki/Regular_expression#POSIX_Extended_Regular_Expressions ). On Windows, it uses a limited variant of regular expression syntax. For more details, see the regular expression syntax .","title":"What syntax does the regular expression in ASSERT_DEATH use?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-have-a-fixture-class-foo-but-test_ffoo-bar-gives-me-error-no-matching-function-for-call-to-foofoo-why","text":"Google Test needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: * If you explicitly declare a non-default constructor for class Foo , then you need to define a default constructor, even if it would be empty. * If Foo has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .)","title":"I have a fixture class Foo, but TEST_F(Foo, Bar) gives me error \"no matching function for call to Foo::Foo()\". Why?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-does-assert_death-complain-about-previous-threads-that-were-already-joined","text":"With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this.","title":"Why does ASSERT_DEATH complain about previous threads that were already joined?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#why-does-google-test-require-the-entire-test-case-instead-of-individual-tests-to-be-named-foodeathtest-when-it-uses-assert_death","text":"Google Test does not interleave tests from different test cases. That is, it runs all tests in one test case first, and then runs all tests in the next test case, and so on. Google Test does this because it needs to set up a test case before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F(FooTest, AbcDeathTest) { ... } TEST_F(FooTest, Uvw) { ... } TEST_F(BarTest, DefDeathTest) { ... } TEST_F(BarTest, Xyz) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test cases, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw .","title":"Why does Google Test require the entire test case, instead of individual tests, to be named FOODeathTest when it uses ASSERT_DEATH?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#but-i-dont-like-calling-my-entire-test-case-foodeathtest-when-it-contains-both-death-tests-and-non-death-tests-what-do-i-do","text":"You don't have to, but if you like, you may split up the test case into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public ::testing::Test { ... }; TEST_F(FooTest, Abc) { ... } TEST_F(FooTest, Def) { ... } typedef FooTest FooDeathTest; TEST_F(FooDeathTest, Uvw) { ... EXPECT_DEATH(...) ... } TEST_F(FooDeathTest, Xyz) { ... ASSERT_DEATH(...) ... }","title":"But I don't like calling my entire test case FOODeathTest when it contains both death tests and non-death tests. What do I do?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#the-compiler-complains-about-no-match-for-operator-when-i-use-an-assertion-what-gives","text":"If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space.","title":"The compiler complains about \"no match for 'operator&lt;&lt;'\" when I use an assertion. What gives?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-suppress-the-memory-leak-messages-on-windows","text":"Since the statically initialized Google Test singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines.","title":"How do I suppress the memory leak messages on Windows?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-am-building-my-project-with-google-test-in-visual-studio-and-all-im-getting-is-a-bunch-of-linker-errors-or-warnings-help","text":"You may get a number of the following linker error or warnings if you attempt to link your test project with the Google Test library when your project and the are not built using the same compiler settings. LNK2005: symbol already defined in object LNK4217: locally defined symbol 'symbol' imported in function 'function' LNK4049: locally defined symbol 'symbol' imported The Google Test project (gtest.vcproj) has the Runtime Library option set to /MT (use multi-threaded static libraries, /MTd for debug). If your project uses something else, for example /MD (use multi-threaded DLLs, /MDd for debug), you need to change the setting in the Google Test project to match your project's. To update this setting open the project properties in the Visual Studio IDE then select the branch Configuration Properties | C/C++ | Code Generation and change the option \"Runtime Library\". You may also try using gtest-md.vcproj instead of gtest.vcproj.","title":"I am building my project with Google Test in Visual Studio and all I'm getting is a bunch of linker errors (or warnings). Help!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-put-my-tests-in-a-library-and-google-test-doesnt-run-them-whats-happening","text":"Have you read a warning on the Google Test Primer page?","title":"I put my tests in a library and Google Test doesn't run them. What's happening?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-want-to-use-google-test-with-visual-studio-but-dont-know-where-to-start","text":"Many people are in your position and one of the posted his solution to our mailing list. Here is his link: http://hassanjamilahmad.blogspot.com/2009/07/gtest-starters-help.html .","title":"I want to use Google Test with Visual Studio but don't know where to start."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#i-am-seeing-compile-errors-mentioning-stdtype_traits-when-i-try-to-use-google-test-on-solaris","text":"Google Test uses parts of the standard C++ library that SunStudio does not support. Our users reported success using alternative implementations. Try running the build after runing this commad: export CC=cc CXX=CC CXXFLAGS='-library=stlport4'","title":"I am seeing compile errors mentioning std::type_traits when I try to use Google Test on Solaris."},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-can-my-code-detect-if-it-is-running-in-a-test","text":"If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and Google Test doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is dependency injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all, there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test.","title":"How can my code detect if it is running in a test?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#google-test-defines-a-macro-that-clashes-with-one-defined-by-another-library-how-do-i-deal-with-that","text":"In C++, macros don't obey namespaces. Therefore two libraries that both define a macro of the same name will clash if you #include both definitions. In case a Google Test macro clashes with another library, you can force Google Test to rename its macro to avoid the conflict. Specifically, if both Google Test and some other code define macro FOO , you can add -DGTEST_DONT_DEFINE_FOO=1 to the compiler flags to tell Google Test to change the macro's name from FOO to GTEST_FOO . For example, with -DGTEST_DONT_DEFINE_TEST=1 , you'll need to write GTEST_TEST(SomeTest, DoesThis) { ... } instead of TEST(SomeTest, DoesThis) { ... } in order to define a test. Currently, the following TEST , FAIL , SUCCEED , and the basic comparison assertion macros can have alternative names. You can see the full list of covered macros here . More information can be found in the \"Avoiding Macro Name Clashes\" section of the README file.","title":"Google Test defines a macro that clashes with one defined by another library. How do I deal with that?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#is-it-ok-if-i-have-two-separate-testfoo-bar-test-methods-defined-in-different-namespaces","text":"Yes. The rule is all test methods in the same test case must use the same fixture class . This means that the following is allowed because both tests use the same fixture class ( ::testing::Test ). namespace foo { TEST(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo namespace bar { TEST(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo However, the following code is not allowed and will produce a runtime error from Google Test because the test methods are using different test fixture classes with the same test case name. namespace foo { class CoolTest : public ::testing::Test {}; // Fixture foo::CoolTest TEST_F(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo namespace bar { class CoolTest : public ::testing::Test {}; // Fixture: bar::CoolTest TEST_F(CoolTest, DoSomething) { SUCCEED(); } } // namespace foo","title":"Is it OK if I have two separate TEST(Foo, Bar) test methods defined in different namespaces?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#how-do-i-build-google-testing-framework-with-xcode-4","text":"If you try to build Google Test's Xcode project with Xcode 4.0 or later, you may encounter an error message that looks like \"Missing SDK in target gtest_framework: /Developer/SDKs/MacOSX10.4u.sdk\". That means that Xcode does not support the SDK the project is targeting. See the Xcode section in the README file on how to resolve this.","title":"How do I build Google Testing Framework with Xcode 4?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_FAQ/#my-question-is-not-covered-in-your-faq","text":"If you cannot find the answer to your question in this FAQ, there are some other resources you can use: read other wiki pages , search the mailing list archive , ask it on googletestframework@googlegroups.com and someone will answer it (to prevent spam, we require you to join the discussion group before you can post.). Please note that creating an issue in the issue tracker is not a good way to get your answer, as it is monitored infrequently by a very small number of people. When asking a question, it's helpful to provide as much of the following information as possible (people cannot help you if there's not enough information in your question): the version (or the revision number if you check out from SVN directly) of Google Test you use (Google Test is under active development, so it's possible that your problem has been solved in a later version), your operating system, the name and version of your compiler, the complete command line flags you give to your compiler, the complete compiler error messages (if the question is about compilation), the actual code (ideally, a minimal but complete program) that has the problem you encounter.","title":"My question is not covered in your FAQ!"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/","text":"Introduction: Why Google C++ Testing Framework? \u00b6 Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test . Setting up a New Test Project \u00b6 To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems: msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script (deprecated) and CMakeLists.txt for CMake (recommended) in the Google Test root directory. If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find \"gtest/gtest.h\" when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples. Basic Concepts \u00b6 When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases. Assertions \u00b6 Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed. Basic Assertions \u00b6 These assertions do basic true/false condition testing. | Fatal assertion | Nonfatal assertion | Verifies | |:--------------------|:-----------------------|:-------------| | ASSERT_TRUE( condition ) ; | EXPECT_TRUE( condition ) ; | condition is true | | ASSERT_FALSE( condition ) ; | EXPECT_FALSE( condition ) ; | condition is false | Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac. Binary Comparison \u00b6 This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( expected , actual ); EXPECT_EQ( expected , actual ); expected == actual ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll introduce later), you should put the expression you want to test in the position of actual , and put its expected value in expected , as Google Test's failure messages are optimized for this convention. Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but it's no longer necessary since v1.6.0 (if << is supported, it will be called to print the arguments when the assertion fails; otherwise Google Test will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see this Google Mock recipe .). These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. String Comparison \u00b6 The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( expected_str , actual_str ); EXPECT_STREQ( expected_str , actual_str ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( expected_str , actual_str ); EXPECT_STRCASEEQ( expected_str , actual_str ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the Advanced Google Test Guide . Simple Tests \u00b6 To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain underscore ( _ ). A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac. Test Fixtures: Using the Same Data Configuration for Multiple Tests \u00b6 If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed. Invoking the Tests \u00b6 TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac. Writing the main() Function \u00b6 You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go. Important note for Visual C++ users \u00b6 If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries! Where to Go from Here \u00b6 Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features. Known Limitations \u00b6 Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"V1 7 Primer"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#introduction-why-google-c-testing-framework","text":"Google C++ Testing Framework helps you write better C++ tests. No matter whether you work on Linux, Windows, or a Mac, if you write C++ code, Google Test can help you. So what makes a good test, and how does Google C++ Testing Framework fit in? We believe: 1. Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. Google C++ Testing Framework isolates the tests by running each of them on a different object. When a test fails, Google C++ Testing Framework allows you to run it in isolation for quick debugging. 1. Tests should be well organized and reflect the structure of the tested code. Google C++ Testing Framework groups related tests into test cases that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. 1. Tests should be portable and reusable . The open-source community has a lot of code that is platform-neutral, its tests should also be platform-neutral. Google C++ Testing Framework works on different OSes, with different compilers (gcc, MSVC, and others), with or without exceptions, so Google C++ Testing Framework tests can easily work with a variety of configurations. (Note that the current release only contains build scripts for Linux - we are actively working on scripts for other platforms.) 1. When tests fail, they should provide as much information about the problem as possible. Google C++ Testing Framework doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. 1. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . Google C++ Testing Framework automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. 1. Tests should be fast . With Google C++ Testing Framework, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since Google C++ Testing Framework is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Note: We sometimes refer to Google C++ Testing Framework informally as Google Test .","title":"Introduction: Why Google C++ Testing Framework?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#setting-up-a-new-test-project","text":"To write a test program using Google Test, you need to compile Google Test into a library and link your test with it. We provide build files for some popular build systems: msvc/ for Visual Studio, xcode/ for Mac Xcode, make/ for GNU make, codegear/ for Borland C++ Builder, and the autotools script (deprecated) and CMakeLists.txt for CMake (recommended) in the Google Test root directory. If your build system is not on this list, you can take a look at make/Makefile to learn how Google Test should be compiled (basically you want to compile src/gtest-all.cc with GTEST_ROOT and GTEST_ROOT/include in the header search path, where GTEST_ROOT is the Google Test root directory). Once you are able to compile the Google Test library, you should create a project or build target for your test program. Make sure you have GTEST_ROOT/include in the header search path so that the compiler can find \"gtest/gtest.h\" when compiling your test. Set up your test project to link with the Google Test library (for example, in Visual Studio, this is done by adding a dependency on gtest.vcproj ). If you still have questions, take a look at how Google Test's own tests are built and use them as examples.","title":"Setting up a New Test Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#basic-concepts","text":"When using Google Test, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test case contains one or many tests. You should group your tests into test cases that reflect the structure of the tested code. When multiple tests in a test case need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test cases. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test cases.","title":"Basic Concepts"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#assertions","text":"Google Test assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, Google Test prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to Google Test's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failures to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator, or a sequence of such operators. An example: ASSERT_EQ(x.size(), y.size()) << \"Vectors x and y are of unequal length\"; for (int i = 0; i < x.size(); ++i) { EXPECT_EQ(x[i], y[i]) << \"Vectors x and y differ at index \" << i; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed.","title":"Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#basic-assertions","text":"These assertions do basic true/false condition testing. | Fatal assertion | Nonfatal assertion | Verifies | |:--------------------|:-----------------------|:-------------| | ASSERT_TRUE( condition ) ; | EXPECT_TRUE( condition ) ; | condition is true | | ASSERT_FALSE( condition ) ; | EXPECT_FALSE( condition ) ; | condition is false | Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac.","title":"Basic Assertions"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#binary-comparison","text":"This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ( expected , actual ); EXPECT_EQ( expected , actual ); expected == actual ASSERT_NE( val1 , val2 ); EXPECT_NE( val1 , val2 ); val1 != val2 ASSERT_LT( val1 , val2 ); EXPECT_LT( val1 , val2 ); val1 < val2 ASSERT_LE( val1 , val2 ); EXPECT_LE( val1 , val2 ); val1 <= val2 ASSERT_GT( val1 , val2 ); EXPECT_GT( val1 , val2 ); val1 > val2 ASSERT_GE( val1 , val2 ); EXPECT_GE( val1 , val2 ); val1 >= val2 In the event of a failure, Google Test prints both val1 and val2 . In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll introduce later), you should put the expression you want to test in the position of actual , and put its expected value in expected , as Google Test's failure messages are optimized for this convention. Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but it's no longer necessary since v1.6.0 (if << is supported, it will be called to print the arguments when the assertion fails; otherwise Google Test will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see this Google Mock recipe .). These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g. == , < , etc). If the corresponding operator is defined, prefer using the ASSERT_*() macros because they will print out not only the result of the comparison, but the two operands as well. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e. the compiler is free to choose any order) and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(NULL, c_string) . However, to compare two string objects, you should use ASSERT_EQ . Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac.","title":"Binary Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#string-comparison","text":"The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ( expected_str , actual_str ); EXPECT_STREQ( expected_str , actual_str ); the two C strings have the same content ASSERT_STRNE( str1 , str2 ); EXPECT_STRNE( str1 , str2 ); the two C strings have different content ASSERT_STRCASEEQ( expected_str , actual_str ); EXPECT_STRCASEEQ( expected_str , actual_str ); the two C strings have the same content, ignoring case ASSERT_STRCASENE( str1 , str2 ); EXPECT_STRCASENE( str1 , str2 ); the two C strings have different content, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. A NULL pointer and an empty string are considered different . Availability : Linux, Windows, Mac. See also: For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see the Advanced Google Test Guide .","title":"String Comparison"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#simple-tests","text":"To create a test: 1. Use the TEST() macro to define and name a test function, These are ordinary C++ functions that don't return a value. 1. In this function, along with any valid C++ statements you want to include, use the various Google Test assertions to check values. 1. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST(test_case_name, test_name) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test case, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain underscore ( _ ). A test's full name consists of its containing test case and its individual name. Tests from different test cases can have the same individual name. For example, let's take a simple integer function: int Factorial(int n); // Returns the factorial of n A test case for this function might look like: // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(1, Factorial(0)); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); } Google Test groups the test results by test cases, so logically-related tests should be in the same test case; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test case FactorialTest . Availability : Linux, Windows, Mac.","title":"Simple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#test-fixtures-using-the-same-data-configuration-for-multiple-tests","text":"If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . It allows you to reuse the same configuration of objects for several different tests. To create a fixture, just: 1. Derive a class from ::testing::Test . Start its body with protected: or public: as we'll want to access fixture members from sub-classes. 1. Inside the class, declare any objects you plan to use. 1. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - don't let that happen to you. 1. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read this FAQ entry . 1. If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F(test_case_name, test_name) { ... test body ... } Like TEST() , the first argument is the test case name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , Google Test will: 1. Create a fresh test fixture at runtime 1. Immediately initialize it via SetUp() , 1. Run the test 1. Clean up by calling TearDown() 1. Delete the test fixture. Note that different tests in the same test case have different test fixture objects, and Google Test always deletes a test fixture before it creates the next one. Google Test does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template <typename E> // E is the element type. class Queue { public: Queue(); void Enqueue(const E& element); E* Dequeue(); // Returns NULL if the queue is empty. size_t size() const; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public ::testing::Test { protected: virtual void SetUp() { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // virtual void TearDown() {} Queue<int> q0_; Queue<int> q1_; Queue<int> q2_; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(0, q0_.size()); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(NULL, n); n = q1_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(1, *n); EXPECT_EQ(0, q1_.size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != NULL); EXPECT_EQ(2, *n); EXPECT_EQ(1, q2_.size()); delete n; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_TRUE(n != NULL) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: 1. Google Test constructs a QueueTest object (let's call it t1 ). 1. t1.SetUp() initializes t1 . 1. The first test ( IsEmptyInitially ) runs on t1 . 1. t1.TearDown() cleans up after the test finishes. 1. t1 is destructed. 1. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Note : Google Test automatically saves all Google Test flags when a test object is constructed, and restores them when it is destructed.","title":"Test Fixtures: Using the Same Data Configuration for Multiple Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#invoking-the-tests","text":"TEST() and TEST_F() implicitly register their tests with Google Test. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different test cases, or even different source files. When invoked, the RUN_ALL_TESTS() macro: 1. Saves the state of all Google Test flags. 1. Creates a test fixture object for the first test. 1. Initializes it via SetUp() . 1. Runs the test on the fixture object. 1. Cleans up the fixture via TearDown() . 1. Deletes the fixture. 1. Restores the state of all Google Test flags. 1. Repeats the above steps for the next test, until all tests have run. In addition, if the text fixture's constructor generates a fatal failure in step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly, if step 3 generates a fatal failure, step 4 will be skipped. Important : You must not ignore the return value of RUN_ALL_TESTS() , or gcc will give you a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced Google Test features (e.g. thread-safe death tests) and thus is not supported. Availability : Linux, Windows, Mac.","title":"Invoking the Tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#writing-the-main-function","text":"You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if its body // is empty. FooTest() { // You can do set-up work for each test here. } virtual ~FooTest() { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: virtual void SetUp() { // Code here will be called immediately after the constructor (right // before each test). } virtual void TearDown() { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test case for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const string input_filepath = \"this/package/testdata/myinputfile.dat\"; const string output_filepath = \"this/package/testdata/myoutputfile.dat\"; Foo f; EXPECT_EQ(0, f.Bar(input_filepath, output_filepath)); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace int main(int argc, char **argv) { ::testing::InitGoogleTest(&argc, argv); return RUN_ALL_TESTS(); } The ::testing::InitGoogleTest() function parses the command line for Google Test flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go.","title":"Writing the main() Function"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#important-note-for-visual-c-users","text":"If you put your tests into a library and your main() function is in a different library or in your .exe file, those tests will not run. The reason is a bug in Visual C++. When you define your tests, Google Test creates certain static objects to register them. These objects are not referenced from elsewhere but their constructors are still supposed to run. When Visual C++ linker sees that nothing in the library is referenced from other places it throws the library out. You have to reference your library with tests from your main program to keep the linker from discarding it. Here is how to do it. Somewhere in your library code declare a function: __declspec(dllexport) int PullInMyLibrary() { return 0; } If you put your tests in a static library (not DLL) then __declspec(dllexport) is not required. Now, in your main program, write a code that invokes that function: int PullInMyLibrary(); static int dummy = PullInMyLibrary(); This will keep your tests referenced and will make them register themselves at startup. In addition, if you define your tests in a static library, add /OPT:NOREF to your main program linker options. If you use MSVC++ IDE, go to your .exe project properties/Configuration Properties/Linker/Optimization and set References setting to Keep Unreferenced Data (/OPT:NOREF) . This will keep Visual C++ linker from discarding individual symbols generated by your tests from the final executable. There is one more pitfall, though. If you use Google Test as a static library (that's how it is defined in gtest.vcproj) your tests must also reside in a static library. If you have to have them in a DLL, you must change Google Test to build into a DLL as well. Otherwise your tests will not run correctly or will not run at all. The general conclusion here is: make your life easier - do not write your tests in libraries!","title":"Important note for Visual C++ users"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#where-to-go-from-here","text":"Congratulations! You've learned the Google Test basics. You can start writing and running Google Test tests, read some samples , or continue with AdvancedGuide , which describes many more useful Google Test features.","title":"Where to Go from Here"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Primer/#known-limitations","text":"Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Known Limitations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/","text":"P ump is U seful for M eta P rogramming. The Problem \u00b6 Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code. Our Solution \u00b6 Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain. Highlights \u00b6 The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode. Examples \u00b6 The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on... Constructs \u00b6 We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [[ code ]] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output. Grammar \u00b6 code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax Code \u00b6 You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump. Real Examples \u00b6 You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h . Tips \u00b6 If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"V1 7 Pump Manual"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#the-problem","text":"Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code.","title":"The Problem"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#our-solution","text":"Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain.","title":"Our Solution"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#highlights","text":"The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode.","title":"Highlights"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#examples","text":"The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template <size_t N> class Foo0 { blah a; }; // Foo1 does blah for 1-ary predicates. template <size_t N, typename A1> class Foo1 { blah b; }; // Foo2 does blah for 2-ary predicates. template <size_t N, typename A1, typename A2> class Foo2 { blah b; }; // Foo3 does blah for 3-ary predicates. template <size_t N, typename A1, typename A2, typename A3> class Foo3 { blah c; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func(); // If n is 0. Func(a1); // If n is 1. Func(a1 + a2); // If n is 2. Func(a1 + a2 + a3); // If n is 3. // And so on...","title":"Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#constructs","text":"We support the following meta programming constructs: $var id = exp Defines a named constant value. $id is valid util the end of the current meta lexical block. $range id exp..exp Sets the range of an iteration variable, which can be reused in multiple loops later. $for id sep [[ code ]] Iteration. The range of id must have been defined earlier. $id is valid in code . $($) Generates a single $ character. $id Value of the named constant or iteration variable. $(exp) Value of the expression. $if exp [[ code ]] else_branch Conditional. [[ code ]] Meta lexical block. cpp_code Raw C++ code. $$ comment Meta comment. Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output.","title":"Constructs"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#grammar","text":"code ::= atomic_code* atomic_code ::= $var id = exp | $var id = [[ code ]] | $range id exp..exp | $for id sep [[ code ]] | $($) | $id | $(exp) | $if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep ::= cpp_code | empty_string else_branch ::= $else [[ code ]] | $elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax","title":"Grammar"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#code","text":"You can find the source code of Pump in scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump.","title":"Code"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#real-examples","text":"You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h .","title":"Real Examples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_PumpManual/#tips","text":"If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Tips"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_Samples/","text":"If you're like us, you'd like to look at some Google Test sample code. The samples folder has a number of well-commented samples showing how to use a variety of Google Test features. Sample #1 shows the basic steps of using Google Test to test C++ functions. Sample #2 shows a more complex unit test for a class with multiple member functions. Sample #3 uses a test fixture. Sample #4 is another basic example of using Google Test. Sample #5 teaches how to reuse a test fixture in multiple test cases by deriving sub-fixtures from it. Sample #6 demonstrates type-parameterized tests. Sample #7 teaches the basics of value-parameterized tests. Sample #8 shows using Combine() in value-parameterized tests. Sample #9 shows use of the listener API to modify Google Test's console output and the use of its reflection API to inspect test results. Sample #10 shows use of the listener API to implement a primitive memory leak checker.","title":"V1 7 Samples"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/","text":"This guide will explain how to use the Google Testing Framework in your Xcode projects on Mac OS X. This tutorial begins by quickly explaining what to do for experienced users. After the quick start, the guide goes provides additional explanation about each step. Quick Start \u00b6 Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations. Get the Source \u00b6 Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk Add the Framework to Your Project \u00b6 The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below). Make a Test Target \u00b6 To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase. Set Up the Executable Run Environment \u00b6 Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH. Build and Go \u00b6 Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0. Summary \u00b6 Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"V1 7 Xcode Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#quick-start","text":"Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations.","title":"Quick Start"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#get-the-source","text":"Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk","title":"Get the Source"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#add-the-framework-to-your-project","text":"The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below).","title":"Add the Framework to Your Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#make-a-test-target","text":"To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase.","title":"Make a Test Target"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#set-up-the-executable-run-environment","text":"Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH.","title":"Set Up the Executable Run Environment"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#build-and-go","text":"Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0.","title":"Build and Go"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/V1_7_XcodeGuide/#summary","text":"Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"Summary"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/","text":"This guide will explain how to use the Google Testing Framework in your Xcode projects on Mac OS X. This tutorial begins by quickly explaining what to do for experienced users. After the quick start, the guide goes provides additional explanation about each step. Quick Start \u00b6 Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations. Get the Source \u00b6 Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk Add the Framework to Your Project \u00b6 The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below). Make a Test Target \u00b6 To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase. Set Up the Executable Run Environment \u00b6 Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH. Build and Go \u00b6 Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0. Summary \u00b6 Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"Xcode Guide"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#quick-start","text":"Here is the quick guide for using Google Test in your Xcode project. Download the source from the website using this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Open up the gtest.xcodeproj in the googletest-read-only/xcode/ directory and build the gtest.framework. Create a new \"Shell Tool\" target in your Xcode project called something like \"UnitTests\" Add the gtest.framework to your project and add it to the \"Link Binary with Libraries\" build phase of \"UnitTests\" Add your unit test source code to the \"Compile Sources\" build phase of \"UnitTests\" Edit the \"UnitTests\" executable and add an environment variable named \"DYLD_FRAMEWORK_PATH\" with a value equal to the path to the framework containing the gtest.framework relative to the compiled executable. Build and Go The following sections further explain each of the steps listed above in depth, describing in more detail how to complete it including some variations.","title":"Quick Start"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#get-the-source","text":"Currently, the gtest.framework discussed here isn't available in a tagged release of Google Test, it is only available in the trunk. As explained at the Google Test site , you can get the code from anonymous SVN with this command: svn checkout http://googletest.googlecode.com/svn/trunk/ googletest-read-only Alternatively, if you are working with Subversion in your own code base, you can add Google Test as an external dependency to your own Subversion repository. By following this approach, everyone that checks out your svn repository will also receive a copy of Google Test (a specific version, if you wish) without having to check it out explicitly. This makes the set up of your project simpler and reduces the copied code in the repository. To use svn:externals , decide where you would like to have the external source reside. You might choose to put the external source inside the trunk, because you want it to be part of the branch when you make a release. However, keeping it outside the trunk in a version-tagged directory called something like third-party/googletest/1.0.1 , is another option. Once the location is established, use svn propedit svn:externals _directory_ to set the svn:externals property on a directory in your repository. This directory won't contain the code, but be its versioned parent directory. The command svn propedit will bring up your Subversion editor, making editing the long, (potentially multi-line) property simpler. This same method can be used to check out a tagged branch, by using the appropriate URL (e.g. http://googletest.googlecode.com/svn/tags/release-1.0.1 ). Additionally, the svn:externals property allows the specification of a particular revision of the trunk with the -r_##_ option (e.g. externals/src/googletest -r60 http://googletest.googlecode.com/svn/trunk ). Here is an example of using the svn:externals properties on a trunk (read via svn propget ) of a project. This value checks out a copy of Google Test into the trunk/externals/src/googletest/ directory. [Computer:svn] user$ svn propget svn:externals trunk externals/src/googletest http://googletest.googlecode.com/svn/trunk","title":"Get the Source"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#add-the-framework-to-your-project","text":"The next step is to build and add the gtest.framework to your own project. This guide describes two common ways below. Option 1 --- The simplest way to add Google Test to your own project, is to open gtest.xcodeproj (found in the xcode/ directory of the Google Test trunk) and build the framework manually. Then, add the built framework into your project using the \"Add->Existing Framework...\" from the context menu or \"Project->Add...\" from the main menu. The gtest.framework is relocatable and contains the headers and object code that you'll need to make tests. This method requires rebuilding every time you upgrade Google Test in your project. Option 2 --- If you are going to be living off the trunk of Google Test, incorporating its latest features into your unit tests (or are a Google Test developer yourself). You'll want to rebuild the framework every time the source updates. to do this, you'll need to add the gtest.xcodeproj file, not the framework itself, to your own Xcode project. Then, from the build products that are revealed by the project's disclosure triangle, you can find the gtest.framework, which can be added to your targets (discussed below).","title":"Add the Framework to Your Project"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#make-a-test-target","text":"To start writing tests, make a new \"Shell Tool\" target. This target template is available under BSD, Cocoa, or Carbon. Add your unit test source code to the \"Compile Sources\" build phase of the target. Next, you'll want to add gtest.framework in two different ways, depending upon which option you chose above. Option 1 --- During compilation, Xcode will need to know that you are linking against the gtest.framework. Add the gtest.framework to the \"Link Binary with Libraries\" build phase of your test target. This will include the Google Test headers in your header search path, and will tell the linker where to find the library. Option 2 --- If your working out of the trunk, you'll also want to add gtest.framework to your \"Link Binary with Libraries\" build phase of your test target. In addition, you'll want to add the gtest.framework as a dependency to your unit test target. This way, Xcode will make sure that gtest.framework is up to date, every time your build your target. Finally, if you don't share build directories with Google Test, you'll have to copy the gtest.framework into your own build products directory using a \"Run Script\" build phase.","title":"Make a Test Target"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#set-up-the-executable-run-environment","text":"Since the unit test executable is a shell tool, it doesn't have a bundle with a Contents/Frameworks directory, in which to place gtest.framework. Instead, the dynamic linker must be told at runtime to search for the framework in another location. This can be accomplished by setting the \"DYLD_FRAMEWORK_PATH\" environment variable in the \"Edit Active Executable ...\" Arguments tab, under \"Variables to be set in the environment:\". The path for this value is the path (relative or absolute) of the directory containing the gtest.framework. If you haven't set up the DYLD_FRAMEWORK_PATH, correctly, you might get a message like this: [Session started at 2008-08-15 06:23:57 -0600.] dyld: Library not loaded: @loader_path/../Frameworks/gtest.framework/Versions/A/gtest Referenced from: /Users/username/Documents/Sandbox/gtestSample/build/Debug/WidgetFrameworkTest Reason: image not found To correct this problem, got to the directory containing the executable named in \"Referenced from:\" value in the error message above. Then, with the terminal in this location, find the relative path to the directory containing the gtest.framework. That is the value you'll need to set as the DYLD_FRAMEWORK_PATH.","title":"Set Up the Executable Run Environment"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#build-and-go","text":"Now, when you click \"Build and Go\", the test will be executed. Dumping out something like this: [Session started at 2008-08-06 06:36:13 -0600.] [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from WidgetInitializerTest [ RUN ] WidgetInitializerTest.TestConstructor [ OK ] WidgetInitializerTest.TestConstructor [ RUN ] WidgetInitializerTest.TestConversion [ OK ] WidgetInitializerTest.TestConversion [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. [ PASSED ] 2 tests. The Debugger has exited with status 0.","title":"Build and Go"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/boringssl/third_party/googletest/docs/XcodeGuide/#summary","text":"Unit testing is a valuable way to ensure your data model stays valid even during rapid development or refactoring. The Google Testing Framework is a great unit testing framework for C and C++ which integrates well with an Xcode development environment.","title":"Summary"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/","text":"I Can Haz Fuzz? \u00b6 LibFuzzer \u00b6 Or, how to fuzz OpenSSL with libfuzzer . Starting from a vanilla+OpenSSH server Ubuntu install. Use Chrome's handy recent build of clang. Older versions may also work. $ sudo apt-get install git $ mkdir git-work $ git clone https://chromium.googlesource.com/chromium/src/tools/clang $ clang/scripts/update.py You may want to git pull and re-run the update from time to time. Update your path: $ PATH = ~/third_party/llvm-build/Release+Asserts/bin/: $PATH Get and build libFuzzer (there is a git mirror at https://github.com/llvm-mirror/llvm/tree/master/lib/Fuzzer if you prefer): $ cd $ sudo apt-get install subversion $ mkdir svn-work $ cd svn-work $ svn co https://llvm.org/svn/llvm-project/compiler-rt/trunk/lib/fuzzer Fuzzer $ cd Fuzzer $ clang++ -c -g -O2 -std = c++11 *.cpp $ ar r libFuzzer.a *.o $ ranlib libFuzzer.a Configure for fuzzing: $ CC = clang ./config enable-fuzz-libfuzzer \\ --with-fuzzer-include = ../../svn-work/Fuzzer \\ --with-fuzzer-lib = ../../svn-work/Fuzzer/libFuzzer.a \\ -DPEDANTIC enable-asan enable-ubsan no-shared \\ -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION \\ -fsanitize-coverage = trace-pc-guard,indirect-calls,trace-cmp \\ enable-ec_nistp_64_gcc_128 -fno-sanitize = alignment enable-tls1_3 \\ enable-weak-ssl-ciphers enable-rc5 enable-md2 \\ enable-ssl3 enable-ssl3-method enable-nextprotoneg \\ --debug $ sudo apt-get install make $ LDCMD = clang++ make -j $ fuzz/helper.py $FUZZER Where $FUZZER is one of the executables in fuzz/ . If you get a crash, you should find a corresponding input file in fuzz/corpora/$FUZZER-crash/ . AFL \u00b6 Configure for fuzzing: $ sudo apt-get install afl-clang $ CC = afl-clang-fast ./config enable-fuzz-afl no-shared -DPEDANTIC \\ enable-tls1_3 enable-weak-ssl-ciphers enable-rc5 enable-md2 \\ enable-ssl3 enable-ssl3-method enable-nextprotoneg \\ enable-ec_nistp_64_gcc_128 -fno-sanitize = alignment \\ --debug $ make The following options can also be enabled: enable-asan, enable-ubsan, enable-msan Run one of the fuzzers: $ afl-fuzz -i fuzz/corpora/ $FUZZER -o fuzz/corpora/ $FUZZER /out fuzz/ $FUZZER Where $FUZZER is one of the executables in fuzz/ . Reproducing issues \u00b6 If a fuzzer generates a reproducible error, you can reproduce the problem using the fuzz/ -test binaries and the file generated by the fuzzer. They binaries don't need to be build for fuzzing, there is no need to set CC or the call config with enable-fuzz- or -fsanitize-coverage, but some of the other options above might be needed. For instance the enable-asan or enable-ubsan option might be useful to show you when the problem happens. For the client and server fuzzer it might be needed to use -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION to reproduce the generated random numbers. To reproduce the crash you can run: $ fuzz/ $FUZZER -test $file Random numbers \u00b6 The client and server fuzzer normally generate random numbers as part of the TLS connection setup. This results in the coverage of the fuzzing corpus changing depending on the random numbers. This also has an effect for coverage of the rest of the test suite and you see the coverage change for each commit even when no code has been modified. Since we want to maximize the coverage of the fuzzing corpus, the client and server fuzzer will use predictable numbers instead of the random numbers. This is controlled by the FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION define. The coverage depends on the way the numbers are generated. We don't disable any check of hashes, but the corpus has the correct hash in it for the random numbers that were generated. For instance the client fuzzer will always generate the same client hello with the same random number in it, and so the server, as emulated by the file, can be generated for that client hello. Coverage changes \u00b6 Since the corpus depends on the default behaviour of the client and the server, changes in what they send by default will have an impact on the coverage. The corpus will need to be updated in that case. Updating the corpus \u00b6 The client and server corpus is generated with multiple config options: - The options as documented above - Without enable-ec_nistp_64_gcc_128 and without --debug - With no-asm - Using 32 bit - A default config, plus options needed to generate the fuzzer. The libfuzzer merge option is used to add the additional coverage from each config to the minimal set.","title":"fuzz"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#i-can-haz-fuzz","text":"","title":"I Can Haz Fuzz?"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#libfuzzer","text":"Or, how to fuzz OpenSSL with libfuzzer . Starting from a vanilla+OpenSSH server Ubuntu install. Use Chrome's handy recent build of clang. Older versions may also work. $ sudo apt-get install git $ mkdir git-work $ git clone https://chromium.googlesource.com/chromium/src/tools/clang $ clang/scripts/update.py You may want to git pull and re-run the update from time to time. Update your path: $ PATH = ~/third_party/llvm-build/Release+Asserts/bin/: $PATH Get and build libFuzzer (there is a git mirror at https://github.com/llvm-mirror/llvm/tree/master/lib/Fuzzer if you prefer): $ cd $ sudo apt-get install subversion $ mkdir svn-work $ cd svn-work $ svn co https://llvm.org/svn/llvm-project/compiler-rt/trunk/lib/fuzzer Fuzzer $ cd Fuzzer $ clang++ -c -g -O2 -std = c++11 *.cpp $ ar r libFuzzer.a *.o $ ranlib libFuzzer.a Configure for fuzzing: $ CC = clang ./config enable-fuzz-libfuzzer \\ --with-fuzzer-include = ../../svn-work/Fuzzer \\ --with-fuzzer-lib = ../../svn-work/Fuzzer/libFuzzer.a \\ -DPEDANTIC enable-asan enable-ubsan no-shared \\ -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION \\ -fsanitize-coverage = trace-pc-guard,indirect-calls,trace-cmp \\ enable-ec_nistp_64_gcc_128 -fno-sanitize = alignment enable-tls1_3 \\ enable-weak-ssl-ciphers enable-rc5 enable-md2 \\ enable-ssl3 enable-ssl3-method enable-nextprotoneg \\ --debug $ sudo apt-get install make $ LDCMD = clang++ make -j $ fuzz/helper.py $FUZZER Where $FUZZER is one of the executables in fuzz/ . If you get a crash, you should find a corresponding input file in fuzz/corpora/$FUZZER-crash/ .","title":"LibFuzzer"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#afl","text":"Configure for fuzzing: $ sudo apt-get install afl-clang $ CC = afl-clang-fast ./config enable-fuzz-afl no-shared -DPEDANTIC \\ enable-tls1_3 enable-weak-ssl-ciphers enable-rc5 enable-md2 \\ enable-ssl3 enable-ssl3-method enable-nextprotoneg \\ enable-ec_nistp_64_gcc_128 -fno-sanitize = alignment \\ --debug $ make The following options can also be enabled: enable-asan, enable-ubsan, enable-msan Run one of the fuzzers: $ afl-fuzz -i fuzz/corpora/ $FUZZER -o fuzz/corpora/ $FUZZER /out fuzz/ $FUZZER Where $FUZZER is one of the executables in fuzz/ .","title":"AFL"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#reproducing-issues","text":"If a fuzzer generates a reproducible error, you can reproduce the problem using the fuzz/ -test binaries and the file generated by the fuzzer. They binaries don't need to be build for fuzzing, there is no need to set CC or the call config with enable-fuzz- or -fsanitize-coverage, but some of the other options above might be needed. For instance the enable-asan or enable-ubsan option might be useful to show you when the problem happens. For the client and server fuzzer it might be needed to use -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION to reproduce the generated random numbers. To reproduce the crash you can run: $ fuzz/ $FUZZER -test $file","title":"Reproducing issues"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#random-numbers","text":"The client and server fuzzer normally generate random numbers as part of the TLS connection setup. This results in the coverage of the fuzzing corpus changing depending on the random numbers. This also has an effect for coverage of the rest of the test suite and you see the coverage change for each commit even when no code has been modified. Since we want to maximize the coverage of the fuzzing corpus, the client and server fuzzer will use predictable numbers instead of the random numbers. This is controlled by the FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION define. The coverage depends on the way the numbers are generated. We don't disable any check of hashes, but the corpus has the correct hash in it for the random numbers that were generated. For instance the client fuzzer will always generate the same client hello with the same random number in it, and so the server, as emulated by the file, can be generated for that client hello.","title":"Random numbers"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#coverage-changes","text":"Since the corpus depends on the default behaviour of the client and the server, changes in what they send by default will have an impact on the coverage. The corpus will need to be updated in that case.","title":"Coverage changes"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/fuzz/#updating-the-corpus","text":"The client and server corpus is generated with multiple config options: - The options as documented above - Without enable-ec_nistp_64_gcc_128 and without --debug - With no-asm - Using 32 bit - A default config, plus options needed to generate the fuzzer. The libfuzzer merge option is used to add the additional coverage from each config to the minimal set.","title":"Updating the corpus"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/github/PULL_REQUEST_TEMPLATE/","text":"Checklist \u00b6 - [ ] documentation is added or updated - [ ] tests are added or updated","title":"github"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/github/PULL_REQUEST_TEMPLATE/#checklist","text":"- [ ] documentation is added or updated - [ ] tests are added or updated","title":"Checklist"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/pyca-cryptography/github/ISSUE_TEMPLATE/openssl-release/","text":"Windows Run the openssl-release-1.1 Jenkins job Copy the resulting artifacts to the Windows builders and unzip them in the root of the file system macOS Send a pull request to homebrew upgrading the openssl@1.1 formula Wait for it to be merged Run the update-brew-openssl Jenkins job manylinux1 Send a pull request to pyca/infra updating the version and hash","title":"ISSUE TEMPLATE"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/","text":"SSL tests \u00b6 SSL testcases are configured in the ssl-tests directory. Each ssl_*.conf.in file contains a number of test configurations. These files are used to generate testcases in the OpenSSL CONF format. The precise test output can be dependent on the library configuration. The test harness generates the output files on the fly. However, for verification, we also include checked-in configuration outputs corresponding to the default configuration. These testcases live in test/ssl-tests/*.conf files. For more details, see ssl-tests/01-simple.conf.in for an example. Configuring the test \u00b6 First, give your test a name. The names do not have to be unique. An example test input looks like this: { name => \"test-default\", server => { \"CipherString\" => \"DEFAULT\" }, client => { \"CipherString\" => \"DEFAULT\" }, test => { \"ExpectedResult\" => \"Success\" }, } The test section supports the following options Test mode \u00b6 Method - the method to test. One of DTLS or TLS. HandshakeMode - which handshake flavour to test: Simple - plain handshake (default) Resume - test resumption RenegotiateServer - test server initiated renegotiation RenegotiateClient - test client initiated renegotiation When HandshakeMode is Resume or Renegotiate, the original handshake is expected to succeed. All configured test expectations are verified against the second handshake. ApplicationData - amount of application data bytes to send (integer, defaults to 256 bytes). Applies to both client and server. Application data is sent in 64kB chunks (but limited by MaxFragmentSize and available parallelization, see below). MaxFragmentSize - maximum send fragment size (integer, defaults to 512 in tests - see SSL_CTX_set_max_send_fragment for documentation). Applies to both client and server. Lowering the fragment size will split handshake and application data up between more SSL_write calls, thus allowing to exercise different code paths. In particular, if the buffer size (64kB) is at least four times as large as the maximum fragment, interleaved multi-buffer crypto implementations may be used on some platforms. Test expectations \u00b6 ExpectedResult - expected handshake outcome. One of Success - handshake success ServerFail - serverside handshake failure ClientFail - clientside handshake failure InternalError - some other error ExpectedClientAlert, ExpectedServerAlert - expected alert. See ssl_test_ctx.c for known values. Note: the expected alert is currently matched against the last received alert (i.e., a fatal alert or a close_notify ). Warning alert expectations are not yet supported. (A warning alert will not be correctly matched, if followed by a close_notify or another alert.) ExpectedProtocol - expected negotiated protocol. One of SSLv3, TLSv1, TLSv1.1, TLSv1.2. SessionTicketExpected - whether or not a session ticket is expected Ignore - do not check for a session ticket (default) Yes - a session ticket is expected No - a session ticket is not expected SessionIdExpected - whether or not a session id is expected Ignore - do not check for a session id (default) Yes - a session id is expected No - a session id is not expected ResumptionExpected - whether or not resumption is expected (Resume mode only) Yes - resumed handshake No - full handshake (default) ExpectedNPNProtocol, ExpectedALPNProtocol - NPN and ALPN expectations. ExpectedTmpKeyType - the expected algorithm or curve of server temp key ExpectedServerCertType, ExpectedClientCertType - the expected algorithm or curve of server or client certificate ExpectedServerSignHash, ExpectedClientSignHash - the expected signing hash used by server or client certificate ExpectedServerSignType, ExpectedClientSignType - the expected signature type used by server or client when signing messages ExpectedClientCANames - for client auth list of CA names the server must send. If this is \"empty\" the list is expected to be empty otherwise it is a file of certificates whose subject names form the list. ExpectedServerCANames - list of CA names the client must send, TLS 1.3 only. If this is \"empty\" the list is expected to be empty otherwise it is a file of certificates whose subject names form the list. Configuring the client and server \u00b6 The client and server configurations can be any valid SSL_CTX configurations. For details, see the manpages for SSL_CONF_cmd . Give your configurations as a dictionary of CONF commands, e.g. server => { \"CipherString\" => \"DEFAULT\", \"MinProtocol\" => \"TLSv1\", } The following sections may optionally be defined: server2 - this section configures a secondary context that is selected via the ServerName test option. This context is used whenever a ServerNameCallback is specified. If the server2 section is not present, then the configuration matches server. resume_server - this section configures the client to resume its session against a different server. This context is used whenever HandshakeMode is Resume. If the resume_server section is not present, then the configuration matches server. resume_client - this section configures the client to resume its session with a different configuration. In practice this may occur when, for example, upgraded clients reuse sessions persisted on disk. This context is used whenever HandshakeMode is Resume. If the resume_client section is not present, then the configuration matches client. Configuring callbacks and additional options \u00b6 Additional handshake settings can be configured in the extra section of each client and server: client => { \"CipherString\" => \"DEFAULT\", extra => { \"ServerName\" => \"server2\", } } Supported client-side options \u00b6 ClientVerifyCallback - the client's custom certificate verify callback. Used to test callback behaviour. One of None - no custom callback (default) AcceptAll - accepts all certificates. RejectAll - rejects all certificates. ServerName - the server the client should attempt to connect to. One of None - do not use SNI (default) server1 - the initial context server2 - the secondary context invalid - an unknown context CTValidation - Certificate Transparency validation strategy. One of None - no validation (default) Permissive - SSL_CT_VALIDATION_PERMISSIVE Strict - SSL_CT_VALIDATION_STRICT Supported server-side options \u00b6 ServerNameCallback - the SNI switching callback to use None - no callback (default) IgnoreMismatch - continue the handshake on SNI mismatch RejectMismatch - abort the handshake on SNI mismatch BrokenSessionTicket - a special test case where the session ticket callback does not initialize crypto. No (default) Yes Mutually supported options \u00b6 NPNProtocols, ALPNProtocols - NPN and ALPN settings. Server and client protocols can be specified as a comma-separated list, and a callback with the recommended behaviour will be installed automatically. SRPUser, SRPPassword - SRP settings. For client, this is the SRP user to connect as; for server, this is a known SRP user. Default server and client configurations \u00b6 The default server certificate and CA files are added to the configurations automatically. Server certificate verification is requested by default. You can override these options by redefining them: client => { \"VerifyCAFile\" => \"/path/to/custom/file\" } or by deleting them client => { \"VerifyCAFile\" => undef } Adding a test to the test harness \u00b6 Add a new test configuration to test/ssl-tests , following the examples of existing *.conf.in files (for example, 01-simple.conf.in ). Generate the generated *.conf test input file. You can do so by running generate_ssl_tests.pl : $ ./config $ cd test $ TOP=.. perl -I ../util/perl/ generate_ssl_tests.pl ssl-tests/my.conf.in \\ > ssl-tests/my.conf where my.conf.in is your test input file. For example, to generate the test cases in ssl-tests/01-simple.conf.in , do $ TOP=.. perl -I ../util/perl/ generate_ssl_tests.pl ssl-tests/01-simple.conf.in > ssl-tests/01-simple.conf Alternatively (hackish but simple), you can comment out unlink glob $tmp_file; in test/recipes/80-test_ssl_new.t and run $ make TESTS=test_ssl_new test This will save the generated output in a *.tmp file in the build directory. Update the number of tests planned in test/recipes/80-test_ssl_new.t . If the test suite has any skip conditions, update those too (see test/recipes/80-test_ssl_new.t for details). Running the tests with the test harness \u00b6 HARNESS_VERBOSE=yes make TESTS=test_ssl_new test Running a test manually \u00b6 These steps are only needed during development. End users should run make test or follow the instructions above to run the SSL test suite. To run an SSL test manually from the command line, the TEST_CERTS_DIR environment variable to point to the location of the certs. E.g., from the root OpenSSL directory, do $ CTLOG_FILE=test/ct/log_list.conf TEST_CERTS_DIR=test/certs test/ssl_test \\ test/ssl-tests/01-simple.conf or for shared builds $ CTLOG_FILE=test/ct/log_list.conf TEST_CERTS_DIR=test/certs \\ util/shlib_wrap.sh test/ssl_test test/ssl-tests/01-simple.conf Note that the test expectations sometimes depend on the Configure settings. For example, the negotiated protocol depends on the set of available (enabled) protocols: a build with enable-ssl3 has different test expectations than a build with no-ssl3 . The Perl test harness automatically generates expected outputs, so users who just run make test do not need any extra steps. However, when running a test manually, keep in mind that the repository version of the generated test/ssl-tests/*.conf correspond to expected outputs in with the default Configure options. To run ssl_test manually from the command line in a build with a different configuration, you may need to generate the right *.conf file from the *.conf.in input first.","title":"test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#ssl-tests","text":"SSL testcases are configured in the ssl-tests directory. Each ssl_*.conf.in file contains a number of test configurations. These files are used to generate testcases in the OpenSSL CONF format. The precise test output can be dependent on the library configuration. The test harness generates the output files on the fly. However, for verification, we also include checked-in configuration outputs corresponding to the default configuration. These testcases live in test/ssl-tests/*.conf files. For more details, see ssl-tests/01-simple.conf.in for an example.","title":"SSL tests"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#configuring-the-test","text":"First, give your test a name. The names do not have to be unique. An example test input looks like this: { name => \"test-default\", server => { \"CipherString\" => \"DEFAULT\" }, client => { \"CipherString\" => \"DEFAULT\" }, test => { \"ExpectedResult\" => \"Success\" }, } The test section supports the following options","title":"Configuring the test"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#test-mode","text":"Method - the method to test. One of DTLS or TLS. HandshakeMode - which handshake flavour to test: Simple - plain handshake (default) Resume - test resumption RenegotiateServer - test server initiated renegotiation RenegotiateClient - test client initiated renegotiation When HandshakeMode is Resume or Renegotiate, the original handshake is expected to succeed. All configured test expectations are verified against the second handshake. ApplicationData - amount of application data bytes to send (integer, defaults to 256 bytes). Applies to both client and server. Application data is sent in 64kB chunks (but limited by MaxFragmentSize and available parallelization, see below). MaxFragmentSize - maximum send fragment size (integer, defaults to 512 in tests - see SSL_CTX_set_max_send_fragment for documentation). Applies to both client and server. Lowering the fragment size will split handshake and application data up between more SSL_write calls, thus allowing to exercise different code paths. In particular, if the buffer size (64kB) is at least four times as large as the maximum fragment, interleaved multi-buffer crypto implementations may be used on some platforms.","title":"Test mode"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#test-expectations","text":"ExpectedResult - expected handshake outcome. One of Success - handshake success ServerFail - serverside handshake failure ClientFail - clientside handshake failure InternalError - some other error ExpectedClientAlert, ExpectedServerAlert - expected alert. See ssl_test_ctx.c for known values. Note: the expected alert is currently matched against the last received alert (i.e., a fatal alert or a close_notify ). Warning alert expectations are not yet supported. (A warning alert will not be correctly matched, if followed by a close_notify or another alert.) ExpectedProtocol - expected negotiated protocol. One of SSLv3, TLSv1, TLSv1.1, TLSv1.2. SessionTicketExpected - whether or not a session ticket is expected Ignore - do not check for a session ticket (default) Yes - a session ticket is expected No - a session ticket is not expected SessionIdExpected - whether or not a session id is expected Ignore - do not check for a session id (default) Yes - a session id is expected No - a session id is not expected ResumptionExpected - whether or not resumption is expected (Resume mode only) Yes - resumed handshake No - full handshake (default) ExpectedNPNProtocol, ExpectedALPNProtocol - NPN and ALPN expectations. ExpectedTmpKeyType - the expected algorithm or curve of server temp key ExpectedServerCertType, ExpectedClientCertType - the expected algorithm or curve of server or client certificate ExpectedServerSignHash, ExpectedClientSignHash - the expected signing hash used by server or client certificate ExpectedServerSignType, ExpectedClientSignType - the expected signature type used by server or client when signing messages ExpectedClientCANames - for client auth list of CA names the server must send. If this is \"empty\" the list is expected to be empty otherwise it is a file of certificates whose subject names form the list. ExpectedServerCANames - list of CA names the client must send, TLS 1.3 only. If this is \"empty\" the list is expected to be empty otherwise it is a file of certificates whose subject names form the list.","title":"Test expectations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#configuring-the-client-and-server","text":"The client and server configurations can be any valid SSL_CTX configurations. For details, see the manpages for SSL_CONF_cmd . Give your configurations as a dictionary of CONF commands, e.g. server => { \"CipherString\" => \"DEFAULT\", \"MinProtocol\" => \"TLSv1\", } The following sections may optionally be defined: server2 - this section configures a secondary context that is selected via the ServerName test option. This context is used whenever a ServerNameCallback is specified. If the server2 section is not present, then the configuration matches server. resume_server - this section configures the client to resume its session against a different server. This context is used whenever HandshakeMode is Resume. If the resume_server section is not present, then the configuration matches server. resume_client - this section configures the client to resume its session with a different configuration. In practice this may occur when, for example, upgraded clients reuse sessions persisted on disk. This context is used whenever HandshakeMode is Resume. If the resume_client section is not present, then the configuration matches client.","title":"Configuring the client and server"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#configuring-callbacks-and-additional-options","text":"Additional handshake settings can be configured in the extra section of each client and server: client => { \"CipherString\" => \"DEFAULT\", extra => { \"ServerName\" => \"server2\", } }","title":"Configuring callbacks and additional options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#supported-client-side-options","text":"ClientVerifyCallback - the client's custom certificate verify callback. Used to test callback behaviour. One of None - no custom callback (default) AcceptAll - accepts all certificates. RejectAll - rejects all certificates. ServerName - the server the client should attempt to connect to. One of None - do not use SNI (default) server1 - the initial context server2 - the secondary context invalid - an unknown context CTValidation - Certificate Transparency validation strategy. One of None - no validation (default) Permissive - SSL_CT_VALIDATION_PERMISSIVE Strict - SSL_CT_VALIDATION_STRICT","title":"Supported client-side options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#supported-server-side-options","text":"ServerNameCallback - the SNI switching callback to use None - no callback (default) IgnoreMismatch - continue the handshake on SNI mismatch RejectMismatch - abort the handshake on SNI mismatch BrokenSessionTicket - a special test case where the session ticket callback does not initialize crypto. No (default) Yes","title":"Supported server-side options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#mutually-supported-options","text":"NPNProtocols, ALPNProtocols - NPN and ALPN settings. Server and client protocols can be specified as a comma-separated list, and a callback with the recommended behaviour will be installed automatically. SRPUser, SRPPassword - SRP settings. For client, this is the SRP user to connect as; for server, this is a known SRP user.","title":"Mutually supported options"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#default-server-and-client-configurations","text":"The default server certificate and CA files are added to the configurations automatically. Server certificate verification is requested by default. You can override these options by redefining them: client => { \"VerifyCAFile\" => \"/path/to/custom/file\" } or by deleting them client => { \"VerifyCAFile\" => undef }","title":"Default server and client configurations"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#adding-a-test-to-the-test-harness","text":"Add a new test configuration to test/ssl-tests , following the examples of existing *.conf.in files (for example, 01-simple.conf.in ). Generate the generated *.conf test input file. You can do so by running generate_ssl_tests.pl : $ ./config $ cd test $ TOP=.. perl -I ../util/perl/ generate_ssl_tests.pl ssl-tests/my.conf.in \\ > ssl-tests/my.conf where my.conf.in is your test input file. For example, to generate the test cases in ssl-tests/01-simple.conf.in , do $ TOP=.. perl -I ../util/perl/ generate_ssl_tests.pl ssl-tests/01-simple.conf.in > ssl-tests/01-simple.conf Alternatively (hackish but simple), you can comment out unlink glob $tmp_file; in test/recipes/80-test_ssl_new.t and run $ make TESTS=test_ssl_new test This will save the generated output in a *.tmp file in the build directory. Update the number of tests planned in test/recipes/80-test_ssl_new.t . If the test suite has any skip conditions, update those too (see test/recipes/80-test_ssl_new.t for details).","title":"Adding a test to the test harness"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#running-the-tests-with-the-test-harness","text":"HARNESS_VERBOSE=yes make TESTS=test_ssl_new test","title":"Running the tests with the test harness"},{"location":"dyn/mu_tiano_plus/CryptoPkg/Library/OpensslLib/openssl/test/READMEssltest/#running-a-test-manually","text":"These steps are only needed during development. End users should run make test or follow the instructions above to run the SSL test suite. To run an SSL test manually from the command line, the TEST_CERTS_DIR environment variable to point to the location of the certs. E.g., from the root OpenSSL directory, do $ CTLOG_FILE=test/ct/log_list.conf TEST_CERTS_DIR=test/certs test/ssl_test \\ test/ssl-tests/01-simple.conf or for shared builds $ CTLOG_FILE=test/ct/log_list.conf TEST_CERTS_DIR=test/certs \\ util/shlib_wrap.sh test/ssl_test test/ssl-tests/01-simple.conf Note that the test expectations sometimes depend on the Configure settings. For example, the negotiated protocol depends on the set of available (enabled) protocols: a build with enable-ssl3 has different test expectations than a build with no-ssl3 . The Perl test harness automatically generates expected outputs, so users who just run make test do not need any extra steps. However, when running a test manually, keep in mind that the repository version of the generated test/ssl-tests/*.conf correspond to expected outputs in with the default Configure options. To run ssl_test manually from the command line in a build with a different configuration, you may need to generate the right *.conf file from the *.conf.in input first.","title":"Running a test manually"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/","text":"Verify Pkcs7 Enhanced Key Usage Unit Test App \u00b6 Testing the verification of Enhanced Key Usages in PKCS7 formatted signatures. About \u00b6 This application tests the VerifyEKUsInPkcs7Signature() function in BaseCryptLib. VerifyPkcs7EkuUnitTestApp \u00b6 This application consumes the UnitTestLib and implements various test cases for the verification of Enhanced Key Usages (EKUs) contained in the end-entity (leaf) signing certificate. TestEKUCerts folder \u00b6 This folder contains information on how the test certificate chain and various leaf certificates were created. It also contains all the files required, and the certificates used in the unit-tests. ChainCreationInstructions.txt \u00b6 This file contains the instructions for creating your own chain, and how to use certreq.exe to create chains. INF files These INF files specify the properties of the certificate to be created. PFX files These Personal Information Exchange (PFX) files contain the certificate, and private key associated with the certificate. The passwords for these files are specified in ChainCreationInstructions.txt. P7B files These are the detached PKCS7 formatted test signatures used in the unit-tests. CER files These are the certificates that were created. SignFirmwareWithEKUs.cmd This script calls signtool.exe to sign a file with various leaf certificates. Note signtool.exe must be in your PATH. Use the Visual Studio command line with the Windows Software Development kit installed. To run SignFirmwareWithEKUs.cmd, install the PFX files. The passwords are in the ChainCreationInstructions.txt file. Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Verify Pkcs7Eku Unit Test App"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#verify-pkcs7-enhanced-key-usage-unit-test-app","text":"Testing the verification of Enhanced Key Usages in PKCS7 formatted signatures.","title":"Verify Pkcs7 Enhanced Key Usage Unit Test App"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#about","text":"This application tests the VerifyEKUsInPkcs7Signature() function in BaseCryptLib.","title":"About"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#verifypkcs7ekuunittestapp","text":"This application consumes the UnitTestLib and implements various test cases for the verification of Enhanced Key Usages (EKUs) contained in the end-entity (leaf) signing certificate.","title":"VerifyPkcs7EkuUnitTestApp"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#testekucerts-folder","text":"This folder contains information on how the test certificate chain and various leaf certificates were created. It also contains all the files required, and the certificates used in the unit-tests.","title":"TestEKUCerts folder"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#chaincreationinstructionstxt","text":"This file contains the instructions for creating your own chain, and how to use certreq.exe to create chains. INF files These INF files specify the properties of the certificate to be created. PFX files These Personal Information Exchange (PFX) files contain the certificate, and private key associated with the certificate. The passwords for these files are specified in ChainCreationInstructions.txt. P7B files These are the detached PKCS7 formatted test signatures used in the unit-tests. CER files These are the certificates that were created. SignFirmwareWithEKUs.cmd This script calls signtool.exe to sign a file with various leaf certificates. Note signtool.exe must be in your PATH. Use the Visual Studio command line with the Windows Software Development kit installed. To run SignFirmwareWithEKUs.cmd, install the PFX files. The passwords are in the ChainCreationInstructions.txt file.","title":"ChainCreationInstructions.txt"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#copyright","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_tiano_plus/EmbeddedPkg/Scripts/LauterbachT32/Readme/","text":"DXE Phase Debug \u00b6 Update the memsize variable in EfiLoadDxe.cmm for the actual amount of memory available in your system. Allow your system to boot to the point that the DXE core is initialized (so that the System Table and Debug Information table is present in memory) and execute this script (using the toolbar button or 'do EfiLoadDxe' from the command area). It will scan memory for the debug info table and load modules in it. SEC/PEI Phase Debug \u00b6 There is no way to autodetect where these images reside so you must pass an address for the memory-mapped Firmware Volume containing these images. To do this, enter 'do EfiLoadFv ' where is the base address for the firmware volume containing the SEC or PEI code. To be more efficient you may want to create a script that calls this, like MyBoardLoadSec.cmm which contains the call to EfiLoadFv. You can them map this script to a T32 menu or toolbar button for quick access.","title":"Lauterbach T32"},{"location":"dyn/mu_tiano_plus/EmbeddedPkg/Scripts/LauterbachT32/Readme/#dxe-phase-debug","text":"Update the memsize variable in EfiLoadDxe.cmm for the actual amount of memory available in your system. Allow your system to boot to the point that the DXE core is initialized (so that the System Table and Debug Information table is present in memory) and execute this script (using the toolbar button or 'do EfiLoadDxe' from the command area). It will scan memory for the debug info table and load modules in it.","title":"DXE Phase Debug"},{"location":"dyn/mu_tiano_plus/EmbeddedPkg/Scripts/LauterbachT32/Readme/#secpei-phase-debug","text":"There is no way to autodetect where these images reside so you must pass an address for the memory-mapped Firmware Volume containing these images. To do this, enter 'do EfiLoadFv ' where is the base address for the firmware volume containing the SEC or PEI code. To be more efficient you may want to create a script that calls this, like MyBoardLoadSec.cmm which contains the call to EfiLoadFv. You can them map this script to a T32 menu or toolbar button for quick access.","title":"SEC/PEI Phase Debug"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/","text":"Introduction \u00b6 TODO: Give a short introduction of FmpDxe driver. Getting Started \u00b6 TODO: Guide users through getting code up and running on their own system. Ideally including: 1. Installation process 2. Software dependencies 3. Latest releases 4. API references Build and Test \u00b6 TODO: Describe and show how to build code and run the tests. Design Changes \u00b6 Date: 06/15/2020 Description/Rationale: Extending on the more granular LastAttemptStatus support added in FmpDeviceSetImage (), FmpDeviceCheckImage () also has a LastAttemptStatus parameter added. An image check is always performed by a set image operation. A more granular status code from the check image path greatly improves overall error isolation when applying an image. Changes: This change allows the FmpDeviceLib implementation to return a last attempt status code in the range LAST_ATTEMPT_STATUS_LIBRARY_ERROR_MIN_ERROR_CODE to LAST_ATTEMPT_STATUS_LIBRARY_ERROR_MAX_ERROR_CODE. Furthermore, an internal wrapper for CheckTheImage () in FmpDxe was added called CheckTheImageInternal (). This function can return a last attempt status code for an error in the driver prior to invoking FmpDeviceCheckImage (). These driver error codes will be in the range of LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE_MIN to LAST_ATTEMPT_STATUS_DRIVER_ERROR_MAX_ERROR_CODE. Impact/Mitigation: The change break the build for all FmpDeviceLib instances due to the API change. Each FmpDeviceLib should change to the new API definition and implement support to return unique values for LastAttemptStatus when appropriate. Date: 10/07/2019 Description/Rationale: Capsule update is the process where each OEM has a lot of interest. Especially when there is capsule update failure, it is helpful to gather more information of the failure. With existed implementation, SetImage routine from FmpDxe driver, which is the most heavy lifting function call during capsule update, will only populate LastAttemptStatus with limited pre-defined error codes which could be consumed/inspected by the OS when it recovers and boots. Thus our proposal is to update the SetImage routine and leverage the LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range newly defined in UEFI Spec 2.8 Section 23.4, so that the error code will provide better granularity when viewing capsule update failure from OS device manager. Changes: A few error codes (128 total) are reserved from LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range for FmpDxe driver usage, which ranges from thermal and power API failure to capsule payload header check failure. Furthermore, an output pointer of the LastAttemptStatus is added as an input argument for FmpDeviceSetImage function in FmpDeviceLib to allow platform to provide their own platform specific error codes (SPI write failure, SVN checking failure, and more). Impact/Mitigation: The italic text above will cause a breaking change for all the FmpDeviceLib instances due to API change. This is to provide a better visibility for OEMs to decode the capsule update failure more efficiently. Each FmpDeviceLib should change to new API definition and populate proper LastAttemptStatus value when applicable.","title":"Fmp Dxe"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#introduction","text":"TODO: Give a short introduction of FmpDxe driver.","title":"Introduction"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#getting-started","text":"TODO: Guide users through getting code up and running on their own system. Ideally including: 1. Installation process 2. Software dependencies 3. Latest releases 4. API references","title":"Getting Started"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#build-and-test","text":"TODO: Describe and show how to build code and run the tests.","title":"Build and Test"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#design-changes","text":"Date: 06/15/2020 Description/Rationale: Extending on the more granular LastAttemptStatus support added in FmpDeviceSetImage (), FmpDeviceCheckImage () also has a LastAttemptStatus parameter added. An image check is always performed by a set image operation. A more granular status code from the check image path greatly improves overall error isolation when applying an image. Changes: This change allows the FmpDeviceLib implementation to return a last attempt status code in the range LAST_ATTEMPT_STATUS_LIBRARY_ERROR_MIN_ERROR_CODE to LAST_ATTEMPT_STATUS_LIBRARY_ERROR_MAX_ERROR_CODE. Furthermore, an internal wrapper for CheckTheImage () in FmpDxe was added called CheckTheImageInternal (). This function can return a last attempt status code for an error in the driver prior to invoking FmpDeviceCheckImage (). These driver error codes will be in the range of LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE_MIN to LAST_ATTEMPT_STATUS_DRIVER_ERROR_MAX_ERROR_CODE. Impact/Mitigation: The change break the build for all FmpDeviceLib instances due to the API change. Each FmpDeviceLib should change to the new API definition and implement support to return unique values for LastAttemptStatus when appropriate. Date: 10/07/2019 Description/Rationale: Capsule update is the process where each OEM has a lot of interest. Especially when there is capsule update failure, it is helpful to gather more information of the failure. With existed implementation, SetImage routine from FmpDxe driver, which is the most heavy lifting function call during capsule update, will only populate LastAttemptStatus with limited pre-defined error codes which could be consumed/inspected by the OS when it recovers and boots. Thus our proposal is to update the SetImage routine and leverage the LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range newly defined in UEFI Spec 2.8 Section 23.4, so that the error code will provide better granularity when viewing capsule update failure from OS device manager. Changes: A few error codes (128 total) are reserved from LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range for FmpDxe driver usage, which ranges from thermal and power API failure to capsule payload header check failure. Furthermore, an output pointer of the LastAttemptStatus is added as an input argument for FmpDeviceSetImage function in FmpDeviceLib to allow platform to provide their own platform specific error codes (SPI write failure, SVN checking failure, and more). Impact/Mitigation: The italic text above will cause a breaking change for all the FmpDeviceLib instances due to API change. This is to provide a better visibility for OEMs to decode the capsule update failure more efficiently. Each FmpDeviceLib should change to new API definition and populate proper LastAttemptStatus value when applicable.","title":"Design Changes"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/","text":"Azure DevOps Pipelines \u00b6 These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution. Core CI \u00b6 Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details Platform CI \u00b6 Focused on building a single target platform and confirming functionality on that platform. Conventions \u00b6 Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml Links \u00b6 Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library Lessons Learned \u00b6 Templates and parameters \u00b6 They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late. File matching patterns \u00b6 On Linux this can hang if there are too many files in the search list. Templates and file splitting \u00b6 Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs. Conditional steps \u00b6 If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Read Me"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#azure-devops-pipelines","text":"These yml files are used to provide CI builds using the Azure DevOps Pipeline Service. Most of the CI leverages edk2-pytools to support cross platform building and execution.","title":"Azure DevOps Pipelines"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#core-ci","text":"Focused on building and testing all packages in Edk2 without an actual target platform. See .pytools/ReadMe.py for more details","title":"Core CI"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#platform-ci","text":"Focused on building a single target platform and confirming functionality on that platform.","title":"Platform CI"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#conventions","text":"Files extension should be *.yml. *.yaml is also supported but in Edk2 we use those for our package configuration. Platform CI files should be in the <PlatformPkg>/.azurepipelines folder. Core CI files are in the root folder. Shared templates are in the templates folder. Top level CI files should be named <host os>-<tool_chain_tag>.yml","title":"Conventions"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#links","text":"Basic Azure Landing Site - https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops Pipeline jobs - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml Pipeline yml scheme - https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema Pipeline expression - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops PyTools - https://github.com/tianocore/edk2-pytool-extensions and https://github.com/tianocore/edk2-pytool-library","title":"Links"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#lessons-learned","text":"","title":"Lessons Learned"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#templates-and-parameters","text":"They are great but evil. If they are used as part of determining the steps of a build they must resolve before the build starts. They can not use variables set in a yml or determined as part of a matrix. If they are used in a step then they can be bound late.","title":"Templates and parameters"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#file-matching-patterns","text":"On Linux this can hang if there are too many files in the search list.","title":"File matching patterns"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#templates-and-file-splitting","text":"Suggestion is to do one big yaml file that does what you want for one of your targets. Then do the second one and find the deltas. From that you can start to figure out the right split of files, steps, jobs.","title":"Templates and file splitting"},{"location":"dyn/mu_tiano_plus/azurepipelines/ReadMe/#conditional-steps","text":"If you want the step to show up in the log but not run, use a step conditional. This is great when a platform doesn't currently support a feature but you want the builders to know that the features exists and maybe someday it will. If you want the step to not show up use a template step conditional wrapper. Beware this will be evaluated early (at build start). This can hide things not needed on a given OS for example.","title":"Conditional steps"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/","text":"CI Templates \u00b6 This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation. Common CI templates \u00b6 basetools-build-steps.yml \u00b6 This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds. Core CI templates \u00b6 pr-gate-build-job.yml \u00b6 This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI. pr-gate-steps.yml \u00b6 This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\" spell-check-prereq-steps.yml \u00b6 This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. Platform CI templates \u00b6 platform-build-run-steps.yml \u00b6 This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools Special Notes \u00b6 For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"templates"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#ci-templates","text":"This folder contains azure pipeline yml templates for \"Core\" and \"Platform\" Continuous Integration and PR validation.","title":"CI Templates"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#common-ci-templates","text":"","title":"Common CI templates"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#basetools-build-stepsyml","text":"This template compiles the Edk2 basetools from source. The steps in this template are conditional and will only run if variable pkg_count is greater than 0. It also has two conditional steps only used when the toolchain contains GCC. These two steps use apt to update the system packages and add those necessary for Edk2 builds.","title":"basetools-build-steps.yml"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#core-ci-templates","text":"","title":"Core CI templates"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#pr-gate-build-jobyml","text":"This templates contains the jobs and most importantly the matrix of which packages and targets to run for Core CI.","title":"pr-gate-build-job.yml"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#pr-gate-stepsyml","text":"This template is the main Core CI template. It controls all the steps run and is responsible for most functionality of the Core CI process. This template sets the pkg_count variable using the stuart_pr_eval tool when the build type is \"pull request\"","title":"pr-gate-steps.yml"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#spell-check-prereq-stepsyml","text":"This template installs the node based tools used by the spell checker plugin. The steps in this template are conditional and will only run if variable pkg_count is greater than 0.","title":"spell-check-prereq-steps.yml"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#platform-ci-templates","text":"","title":"Platform CI templates"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#platform-build-run-stepsyml","text":"This template makes heavy use of pytools to build and run a platform in the Edk2 repo Also uses basetools-build-steps.yml to compile basetools","title":"platform-build-run-steps.yml"},{"location":"dyn/mu_tiano_plus/azurepipelines/templates/ReadMe/#special-notes","text":"For a build type of pull request it will conditionally build if the patches change files that impact the platform. uses stuart_pr_eval to determine impact For manual builds or CI builds it will always build the platform It compiles basetools from source Will use stuart_build --FlashOnly to attempt to run the built image if the Run parameter is set. See the parameters block for expected configuration options Parameter extra_install_step allows the caller to insert extra steps. This is useful if additional dependencies, tools, or other things need to be installed. Here is an example of installing qemu on Windows. steps : - template : ../../.azurepipelines/templates/build-run-steps.yml parameters : extra_install_step : - powershell : choco install qemu; Write-Host \"##vso[task.prependpath]c:\\Program Files\\qemu\" displayName : Install QEMU and Set QEMU on path # friendly name displayed in the UI condition : and(gt(variables.pkg_count, 0), succeeded())","title":"Special Notes"},{"location":"dyn/mu_tiano_plus/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues CryptoPkg Spell checking in audit mode EmbeddedPkg FatPkg FmpDevicePkg ShellPkg Spell checking in audit mode, 3 modules are not being built by DSC SourceLevelDebugPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment Host Module Inclusion Test - HostUnitTestDscCompleteCheck \u00b6 This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin \u00b6 A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way... Including Host-Based Tests in the Package YAML \u00b6 For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided. Configuring the HostBased DSC \u00b6 The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"pytool"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues CryptoPkg Spell checking in audit mode EmbeddedPkg FatPkg FmpDevicePkg ShellPkg Spell checking in audit mode, 3 modules are not being built by DSC SourceLevelDebugPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines/Windows-VS2019.yml and .azurepipelines/Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 18.04 or Fedora GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool/Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. This test will ignore INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance only supports the HOST_APPLICATION environment","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#host-module-inclusion-test-hostunittestdsccompletecheck","text":"This test scans all INF files from a package for those related to host based unit tests and confirms they are listed in the unit test DSC file for the package. The test considers it an error if any INF meeting the requirements does not appear in the Components section of the unit test DSC. This is critical because much of the CI infrastructure assumes that modules will be listed in the DSC and compiled. This test will only require INFs in the following cases: When MODULE_TYPE = HOST_APPLICATION When a Library instance explicitly supports the HOST_APPLICATION environment","title":"Host Module Inclusion Test - HostUnitTestDscCompleteCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#host-unit-test-compilation-and-run-test-hostunittestcompilerplugin","text":"A test that compiles the dsc for host based unit test apps. On Windows this will also enable a build plugin to execute that will run the unit tests and verify the results. These tools will be invoked on any CI pass that includes the NOOPT target. In order for these tools to do their job, the package and tests must be configured in a particular way...","title":"Host Unit Test Compilation and Run Test - HostUnitTestCompilerPlugin"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#including-host-based-tests-in-the-package-yaml","text":"For example, looking at the MdeModulePkg.ci.yaml config file, there are two config options that control HostBased test behavior: ## options defined .pytool/Plugin/HostUnitTestCompilerPlugin \"HostUnitTestCompilerPlugin\" : { \"DscPath\" : \"Test/MdeModulePkgHostTest.dsc\" } , This option tell the test builder to run. The test builder needs to know which modules in this package are host-based tests, so that DSC path is provided.","title":"Including Host-Based Tests in the Package YAML"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#configuring-the-hostbased-dsc","text":"The HostBased DSC for MdeModulePkg is located at MdeModulePkg/Test/MdeModulePkgHostTest.dsc . To add automated host-based unit test building to a new package, create a similar DSC. The new DSC should make sure to have the NOOPT BUILD_TARGET and should include the line: !include UnitTestFrameworkPkg/UnitTestFrameworkPkgHost.dsc.inc All of the modules that are included in the Components section of this DSC should be of type HOST_APPLICATION.","title":"Configuring the HostBased DSC"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool/Plugin/SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler host-based-test set in .pytool/CISettings.py Turns on the host based tests and plugin host-test-win set in .pytool/CISettings.py Enables the host based test runner for Windows","title":"PyTool Scopes"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Extensible private/closed source platform reporting UEFI SCTs Other automation","title":"Future investments"}]}